{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84482766-9f65-4cb5-9309-e8bd125501f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from multiprocessing import get_context\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "from DNNFM_functions import *\n",
    "from comp_m_functions import static_factor_obs, cov_e_poet\n",
    "\n",
    "def round(x):\n",
    "    return int(Decimal(x).to_integral_value(rounding=ROUND_HALF_UP))\n",
    "\n",
    "#---------------- Main ----------------\n",
    "\n",
    "def DNN_FM_main(data, data_factor, architecture=1, const_err_cov=2.5, use_CV_err=False, eval_type='frob'):\n",
    "\n",
    "    data_dm = data\n",
    "    data_factor_dm = data_factor\n",
    "    \n",
    "    # Obtain optimal tuning parameter based on cross-validation or pre-specified values\n",
    "    opt = opt_hyper_parameters(data=data, data_F=data_factor, architecture=architecture, const_err_cov=const_err_cov, \n",
    "                               use_CV_err=use_CV_err, eval_type=eval_type)\n",
    "    # Compute DNN-FM based on optimal hyper-parameters\n",
    "    res_DNN_FM = DNN_FM_core(data=data_dm, data_factor=data_factor_dm, architecture=architecture, opt=opt)\n",
    "\n",
    "    # c_err_min = DNN_FM_cov_e_cmin(data=data_dm, data_factor=data_factor_dm, DNN_model=res_DNN_FM['neural_net'])\n",
    "\n",
    "    print(opt['const_err_cov'])\n",
    "    res_DNN_FM_cov = DNN_FM_cov(data=data_dm, data_factor=data_factor_dm, DNN_model=res_DNN_FM['neural_net'], \n",
    "                                c_err_cov=opt['const_err_cov'], check_eig=False)\n",
    "    \n",
    "    res_DNN_FM.update(res_DNN_FM_cov)\n",
    "\n",
    "    return res_DNN_FM\n",
    "\n",
    "\n",
    "#---------------- Functions ----------------\n",
    "\n",
    "# Core function for creating Neural Network\n",
    "\n",
    "def DNN_FM_core(data, data_factor, architecture, opt):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "    num_f = data_factor.shape[1]\n",
    "\n",
    "    # Create neural network specifications\n",
    "\n",
    "    if architecture == 1:\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 1\n",
    "        d_rate = 0.2\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 512\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 2:\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.1\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 256\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "        \n",
    "    elif architecture == 3:\n",
    "        \n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 1\n",
    "        d_rate = 0.0\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 512\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "    \n",
    "    elif architecture == 4:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.0\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "    \n",
    "    elif architecture == 5:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "\n",
    "    elif architecture == 6:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.0\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 7:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 8:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 9:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 10:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "\n",
    "    # Optimization options\n",
    "    optimizer = 'Adam'\n",
    "    max_iter = 2000                 # maximum number of iterations\n",
    "    max_iter_nc = 50                # maximum number of iterations early stopping\n",
    "    \n",
    "    split_ratio = 0.3               # split ratio for training and validation set\n",
    "    batch_s = 256                   # batch size\n",
    "    use_bias = False                # include bias in neural net, if true\n",
    "\n",
    "    # Define early stopping criterion\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=max_iter_nc, mode='min')\n",
    "\n",
    "    learning_rate = opt['learning_rate']\n",
    "    reg_par_w = opt['reg_par_w']          # regularization parameter for weights\n",
    "    reg_par_b = opt['reg_par_b']          # regularization parameter for bias\n",
    "    el_r_pro = opt['el_r_pro']            # split between elastic net and lasso: 1 - only l1 norm\n",
    "\n",
    "    \"\"\"\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=1,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sparse neural network and compile it\n",
    "    neural_net = sparse_nn(hidden_layer_s, activation_functions, dropout_rates_tr, num_s, num_f,\n",
    "                reg_par_w, reg_par_b, el_r_pro, max_iter_nc, max_iter, optimizer, learning_rate, use_bias, inter_layer)\n",
    "\n",
    "    neural_net.build_neural_network()\n",
    "    # Compile and fit model\n",
    "    neural_net.compile_nn()\n",
    "\n",
    "    fit_nn = neural_net.model.fit(data_factor, data, epochs=max_iter,\n",
    "                        validation_split=split_ratio, shuffle=True, batch_size=batch_s,\n",
    "                        callbacks=early_stopping, verbose=0)\n",
    "    \n",
    "    # Compute market sensitivity\n",
    "    with tf.GradientTape() as tape:\n",
    "        data_factor_ts = tf.convert_to_tensor(data_factor[-1,:].reshape(1,-1), dtype=tf.float32)\n",
    "        tape.watch(data_factor_ts)\n",
    "        y_pred = neural_net.model(data_factor_ts)\n",
    "\n",
    "    market_sens = tape.jacobian(y_pred, data_factor_ts)[-1,:,-1,:].numpy() \n",
    "\n",
    "    # Store for analysis\n",
    "    market_return_t = data_factor_ts.numpy()[-1, :].reshape(1,-1)\n",
    "    market_sensitivity = (market_sens, market_return_t)\n",
    "\n",
    "    res = {'neural_net': neural_net, 'opt': opt, 'market_sensitivity': market_sensitivity}\n",
    "    return res\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "def DNN_FM_cov(data, data_factor, DNN_model, c_err_cov, check_eig=False):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "\n",
    "    y_hat_nn = DNN_model.model(data_factor).numpy()\n",
    "    resd_nn = data - y_hat_nn\n",
    "\n",
    "    sig_hat_e = thres_resd_new(resd_nn, c_err_cov, num_s, num_n)\n",
    "    cov_f_nnet = np.cov(y_hat_nn.T)\n",
    "\n",
    "    if not check_eig:\n",
    "                \n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "    else:\n",
    "\n",
    "        cond = True\n",
    "        const_c = 0\n",
    "        while cond:\n",
    "\n",
    "            sig_hat_e = thres_resd_new(resd_nn, c_err_cov+const_c, num_s, num_n)\n",
    "\n",
    "            cond = (round(min(np.linalg.eig(sig_hat_e)[0]),2) < 0.01) or (np.linalg.cond(sig_hat_e) > num_s*10)\n",
    "            const_c+=0.01\n",
    "        \n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "\n",
    "    inv_sigma_y_nnet = sig_inv_f_nnet(cov_f_nnet, sig_hat_e)\n",
    "\n",
    "    res = {'sigma_hat': sigma_y_nnet, 'sigma_f_hat': cov_f_nnet, 'sigma_e_hat': sig_hat_e, \n",
    "           'inv_sigma_hat': inv_sigma_y_nnet}\n",
    "\n",
    "    return res\n",
    "\n",
    "def DNN_FM_cov_e_cmin(data, data_factor, DNN_model):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "\n",
    "    y_hat_nn = DNN_model.model(data_factor).numpy()\n",
    "    resd_nn = data - y_hat_nn\n",
    "\n",
    "    num_n, num_s = resd_nn.shape\n",
    "\n",
    "    f = lambda c: mineig_cov_e(resd=resd_nn, c_err_cov=c, num_s=num_s, num_n=num_n)\n",
    "\n",
    "    if (f(50) * f(-50) < 0):\n",
    "        r = brentq(f, -50, 50)\n",
    "        return max(0, r)\n",
    "    else:\n",
    "        c = 0\n",
    "        return c\n",
    "\n",
    "def mineig_cov_e(resd, c_err_cov, num_s, num_n):\n",
    "\n",
    "    return min(np.linalg.eig(thres_resd_new(resd, c_err_cov, num_s, num_n))[0])\n",
    "\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "# Function for determining optimal regularization and learning rate based on cross-validation or fixed\n",
    "def opt_hyper_parameters(data, data_F, architecture, const_err_cov, use_CV_err, eval_type):\n",
    "    \n",
    "    parallel = False\n",
    "\n",
    "    if architecture == 1 or architecture == 5 or architecture == 7:\n",
    "        reg_par_w = 0.0005           # regularization parameter for weights\n",
    "        reg_par_b = 0.0005           # regularization parameter for bias\n",
    "    elif architecture == 9 or architecture == 10:\n",
    "        reg_par_w = 0.005           # regularization parameter for weights\n",
    "        reg_par_b = 0.005           # regularization parameter for bias\n",
    "    else:\n",
    "        reg_par_w = 0.0             # regularization parameter for weights\n",
    "        reg_par_b = 0.0             # regularization parameter for bias\n",
    "    el_r_pro = 1                    # split between elastic net and lasso: 1 - only l1 norm\n",
    "    # Alternative specification for the learning rate\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    if use_CV_err:\n",
    "\n",
    "        range_cov_err = np.arange(0, const_err_cov+0.6, 0.1)\n",
    "\n",
    "        \"\"\"\n",
    "        opt = block_cv(data=data, data_F=data_F, architecture=architecture, \n",
    "                       reg_par=reg_par_w, lr=learning_rate, range_cov_err=range_cov_err, \n",
    "                       eval_type=eval_type, test_size=10, parallel=parallel)\n",
    "        \"\"\"\n",
    "        \n",
    "        opt = cv_split(data=data, data_F=data_F, architecture=architecture, \n",
    "                       reg_par=reg_par_w, lr=learning_rate, range_cov_err=range_cov_err, \n",
    "                       eval_type=eval_type)\n",
    "\n",
    "    else:\n",
    "\n",
    "        opt = {'learning_rate': learning_rate, 'reg_par_w': reg_par_w, \n",
    "            'reg_par_b': reg_par_b, 'el_r_pro': el_r_pro, 'const_err_cov': const_err_cov}\n",
    "    \n",
    "    return opt\n",
    "        \n",
    "#-------------------------------------------\n",
    "\n",
    "# Function that determines hyperparameters based on block cross-validation\n",
    "\n",
    "def block_cv(data, data_F, architecture, reg_par, lr, range_cov_err, eval_type, min_train_ratio=0.8, test_size=5, parallel=False):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    T, p = data.shape\n",
    "    train_size = int(np.floor(min_train_ratio * T)) # Get training size\n",
    "    numBlocks = int(np.floor((T - train_size) / test_size)) # Compute number of blocks\n",
    "    \n",
    "    # Show error message if test size is too large\n",
    "    assert numBlocks > 1, f\"Test size is too large\"\n",
    "\n",
    "    # Predefine dictionary for options\n",
    "    opt = {'learning_rate': lr, 'reg_par_w': reg_par, 'reg_par_b': reg_par, 'el_r_pro': 1}\n",
    "    \n",
    "    res_mat = np.empty((numBlocks,len(range_cov_err)))\n",
    "    res_mat[:] = np.nan\n",
    "\n",
    "    for bl in range(1, numBlocks+1):\n",
    "        idxTrainX = list(range((bl-1)*test_size,train_size+(bl-1)*test_size))\n",
    "\n",
    "        if bl == numBlocks:\n",
    "            idxTestX = list(range(train_size+(bl-1)*test_size,T))\n",
    "        else:\n",
    "            idxTestX = list(range(train_size+(bl-1)*test_size,train_size+bl*test_size))\n",
    "\n",
    "        \n",
    "        data_ntrain, data_mean, data_std = normalize_dat(data[idxTrainX,:])\n",
    "        data_F_ntrain, data_F_mean, data_F_std = normalize_dat(data_F[idxTrainX,:])    \n",
    "\n",
    "        test_res = np.empty((len(idxTestX),len(range_cov_err)))\n",
    "        test_res[:] = np.nan\n",
    "        \n",
    "        # if bl == 1:\n",
    "        res_DNN_FM = DNN_FM_core(data_ntrain, data_F_ntrain, architecture, opt)\n",
    "        neural_net = res_DNN_FM['neural_net']\n",
    "\n",
    "\n",
    "        for idx_t in range(1, len(idxTestX)+1):\n",
    "\n",
    "            ind_test_temp = idxTrainX[idx_t:]+idxTestX[:idx_t]\n",
    "\n",
    "            data_ntest, data_mean, data_std = normalize_dat(data[ind_test_temp,:])\n",
    "            data_F_ntest, data_F_mean, data_F_std = normalize_dat(data_F[ind_test_temp,:])\n",
    "\n",
    "            y_hat = neural_net.model(data_F_ntrain).numpy()\n",
    "            resd_nn = data_ntrain - y_hat\n",
    "\n",
    "            num_n, num_s = resd_nn.shape\n",
    "\n",
    "            sigma_test = cov_sfm(data_ntest - neural_net.model(data_F_ntest).numpy())\n",
    "\n",
    "            \"\"\"\n",
    "            # define the number of worker processes to use\n",
    "            num_processes = 2\n",
    "\n",
    "            combined_list = [(resd_nn, sigma_test, eval_type, const_err_item) for const_err_item in range_cov_err]\n",
    "\n",
    "            if parallel:\n",
    "                # create a pool of worker processes\n",
    "                with get_context(\"spawn\").Pool(processes=num_processes) as pool:\n",
    "                    ret = pool.imap(err_cv_core, combined_list)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "            else:\n",
    "                ret = map(err_cv_core, combined_list)\n",
    "\n",
    "            for indx, err_measure in enumerate(ret):\n",
    "                test_res[idx_t-1,indx] = err_measure\n",
    "            \"\"\"\n",
    "\n",
    "            # sigma_test = cov_sfm(data_ntest - neural_net.model(data_F_ntest).numpy())\n",
    "\n",
    "            rate_thres = np.sqrt((np.log(num_s))/num_n)\n",
    "            sig_e_samp = np.cov(resd_nn.T)\n",
    "            thet_par = np.empty((num_s, num_s))\n",
    "            thet_par[:] = np.nan\n",
    "\n",
    "            for ii in range(0, num_s):\n",
    "                for jj in range(0, num_s):\n",
    "                    thet_par[ii, jj] = np.mean(np.abs(resd_nn[:, ii] * resd_nn[:, jj] - sig_e_samp[ii, jj]))\n",
    "\n",
    "            sig_e_diag = np.diag(sig_e_samp)\n",
    "            \n",
    "            \"\"\"\n",
    "            sig_e_diag = np.diag(np.diag(sig_e_samp)**(0.5))\n",
    "            R = np.linalg.inv(sig_e_diag) @ sig_e_samp @ np.linalg.inv(sig_e_diag)\n",
    "            \"\"\"\n",
    "            \n",
    "            for c_idx in range(0,len(range_cov_err)):\n",
    "                lam = rate_thres * range_cov_err[c_idx] * thet_par\n",
    "                \n",
    "                \"\"\"\n",
    "                M = soft_t(R, lam)\n",
    "                M = M - np.diag(np.diag(M)) + np.eye(num_s)\n",
    "                sig_hat_e = sig_e_diag @ M @ sig_e_diag\n",
    "                \"\"\"\n",
    "                \n",
    "                sig_hat_e = soft_t(sig_e_samp, lam)\n",
    "                np.fill_diagonal(sig_hat_e, sig_e_diag)\n",
    "                \n",
    "                # sig_hat_e = thres_resd_new(resd_nn, range_cov_err[c_idx], num_s, num_n)\n",
    "\n",
    "                if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "                    test_res[idx_t-1,c_idx] = np.inf\n",
    "                else:\n",
    "                    if eval_type == 'frob':\n",
    "                        test_res[idx_t-1,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')**2\n",
    "                    elif eval_type == 'spec':\n",
    "                        test_res[idx_t-1,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord=2)**2\n",
    "\n",
    "            \n",
    "        res_mat[bl-1,:] = np.mean(test_res, axis=0)\n",
    "\n",
    "\n",
    "     \n",
    "    idx_opt = np.where(res_mat.mean(axis=0) == np.nanmin(res_mat.mean(axis=0)))\n",
    "\n",
    "    opt.update({'const_err_cov': range_cov_err[idx_opt][0]})\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "    return opt\n",
    "\n",
    "def err_cv_core(tuple_in):\n",
    "\n",
    "    resd_nn, sigma_test, eval_type, const_err_cov = tuple_in[0], tuple_in[1], tuple_in[2], tuple_in[3]\n",
    "\n",
    "    num_n, num_s = resd_nn.shape\n",
    "\n",
    "    st = time.time()\n",
    "    sig_hat_e = thres_resd_new(resd_nn, const_err_cov, num_s, num_n)\n",
    "    print(time.time()-st)\n",
    "\n",
    "    if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "        test_res = np.inf\n",
    "    else:\n",
    "        if eval_type == 'frob':\n",
    "            test_res = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')\n",
    "        elif eval_type == 'spec':\n",
    "            st1 = time.time()\n",
    "            test_res = np.linalg.norm(sig_hat_e - sigma_test, ord=2)\n",
    "            print(time.time()-st1)\n",
    "\n",
    "    return test_res\n",
    "\n",
    "def cv_split(data, data_F, architecture, reg_par, lr, range_cov_err, eval_type):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Predefine dictionary for options\n",
    "    opt = {'learning_rate': lr, 'reg_par_w': reg_par, 'reg_par_b': reg_par, 'el_r_pro': 1}\n",
    "\n",
    "    data_dm, _, _ = normalize_dat_sim(data)\n",
    "    data_F_dm, _, _ = normalize_dat_sim(data_F)    \n",
    "\n",
    "    res_DNN_FM = DNN_FM_core(data_dm, data_F_dm, architecture, opt)\n",
    "    neural_net = res_DNN_FM['neural_net']\n",
    "\n",
    "    y_hat = neural_net.model(data_F_dm).numpy()\n",
    "    resd_nn = data_dm - y_hat\n",
    "\n",
    "    n_folds = 10\n",
    "\n",
    "    res_mat = np.empty((n_folds,len(range_cov_err)))\n",
    "    res_mat[:] = np.nan\n",
    "\n",
    "    split_sample, _ = ts_train_test_split(resd_nn, n_folds, train_size=0.5)\n",
    "\n",
    "    for m_idx in range(0, n_folds):\n",
    "\n",
    "\n",
    "            resd_nn_s1 = split_sample[m_idx][0]\n",
    "            resd_nn_s2 = split_sample[m_idx][1]\n",
    "\n",
    "            num_n, num_s = resd_nn_s1.shape\n",
    "\n",
    "            sigma_test = cov_sfm(resd_nn_s2) # np.cov(resd_nn_s2.T) # \n",
    "\n",
    "            sig_e_samp, thet_par = thres_cov_resd_aux(resd_nn_s1, num_s)\n",
    "\n",
    "            for c_idx in range(0,len(range_cov_err)):\n",
    "                \n",
    "                sig_hat_e = thres_cov_resd(sig_e_samp, thet_par, range_cov_err[c_idx], num_s, num_n)\n",
    "\n",
    "                if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "                    res_mat[m_idx,c_idx] = np.inf\n",
    "                else:\n",
    "                    if eval_type == 'frob':\n",
    "                        res_mat[m_idx,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')**2\n",
    "                    elif eval_type == 'spec':\n",
    "                        res_mat[m_idx,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord=2)**2\n",
    "\n",
    "    idx_opt = np.where(res_mat.mean(axis=0) == np.nanmin(res_mat.mean(axis=0)))\n",
    "\n",
    "    opt.update({'const_err_cov': range_cov_err[idx_opt][0]})\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "\n",
    "    return opt\n",
    "\n",
    "def ts_train_test_split(X, n_folds = 5, train_size=0.5):\n",
    "\n",
    "    test_size = 1 - train_size\n",
    "    n_obs = X.shape[0]\n",
    "\n",
    "    size_split = n_obs - n_folds\n",
    "\n",
    "    n_train = round(size_split * train_size)\n",
    "    n_test = round(size_split * test_size)\n",
    "\n",
    "    split_t = (list(range(0, n_train)), list(range(n_train, n_train+n_test)))\n",
    "\n",
    "    split_sample = []\n",
    "    split_index = []\n",
    "\n",
    "    for jj in range(0, n_folds):\n",
    "        split_index.append(([el1 + jj for el1 in split_t[0]], [el2 + jj for el2 in split_t[1]]))\n",
    "        split_sample.append((X[split_index[jj][0],:], X[split_index[jj][1],:]))\n",
    "\n",
    "    return split_sample, split_index\n",
    "\n",
    "\n",
    "def gmv_weights(Theta_hat):\n",
    "    \"\"\"\n",
    "    Compute Global Minimum Variance (GMV) portfolio weights (Section 6.1).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # w* = (Θ 1_p) / (1_p' Θ 1_p)\n",
    "    numerator = Theta_hat @ ones_p\n",
    "    denominator = ones_p @ Theta_hat @ ones_p\n",
    "    \n",
    "    if np.abs(denominator) < 1e-10:\n",
    "        # Fallback to equal weights if precision matrix is near-singular\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = numerator / denominator\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "def mv_weights(Theta_hat, mu, target_return=0.01):\n",
    "    \"\"\"\n",
    "    Compute Mean-Variance portfolio weights with target return.\n",
    "    \n",
    "    Solves the constrained optimization:\n",
    "    min w' Sigma w  subject to  w' mu = target_return  and  w' 1 = 1\n",
    "    \n",
    "    Solution uses Lagrange multipliers with two constraints.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected returns\n",
    "    target_return : float\n",
    "        Target portfolio return (default: 0.01 = 1% monthly)\n",
    "    long_only : bool\n",
    "        If True, falls back to GMV if MV produces negative weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute key quantities\n",
    "    A = ones_p @ Theta_hat @ ones_p  # 1' Theta 1\n",
    "    B = ones_p @ Theta_hat @ mu       # 1' Theta mu  \n",
    "    C = mu @ Theta_hat @ mu           # mu' Theta mu\n",
    "    D = A * C - B * B                  # Determinant\n",
    "    \n",
    "    # Check for singularity\n",
    "    if np.abs(D) < 1e-10:\n",
    "        print('SINGULARITY')\n",
    "        # System is singular, use GMV instead\n",
    "        if np.abs(A) > 1e-10:\n",
    "            w_star = (Theta_hat @ ones_p) / A\n",
    "            return w_star\n",
    "        else:\n",
    "            return ones_p / p\n",
    "    \n",
    "    \n",
    "    # Compute Lagrange multipliers\n",
    "    lambda1 = (C - B * target_return) / D\n",
    "    lambda2 = (A * target_return - B) / D\n",
    "    \n",
    "    # Compute weights: w = lambda1 * Theta^{-1} 1 + lambda2 * Theta^{-1} mu\n",
    "    w_star = lambda1 * (Theta_hat @ ones_p) + lambda2 * (Theta_hat @ mu)\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "def msr_weights(Theta_hat, mu):\n",
    "    \"\"\"\n",
    "    Compute Maximum Sharpe Ratio portfolio weights.\n",
    "    \n",
    "    The maximum Sharpe ratio portfolio solves:\n",
    "    max (w' mu) / sqrt(w' Sigma w)\n",
    "    \n",
    "    Solution (when mu represents excess returns):\n",
    "    w ∝ Sigma^{-1} mu = Theta mu\n",
    "    \n",
    "    Then normalize so that sum(w) = 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected excess returns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights (sum to 1)\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute unnormalized weights: w ∝ Theta mu\n",
    "    w_unnorm = Theta_hat @ mu\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    weight_sum = np.sum(w_unnorm)\n",
    "    \n",
    "    if np.abs(weight_sum) < 1e-10:\n",
    "        print('WARNING: Weight sum near zero, returning equal weights')\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = w_unnorm / weight_sum\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "\n",
    "def load_yearly_signals(year, buys_path_template='buys_{}.csv', sells_path_template='sells_{}.csv'):\n",
    "    \"\"\"\n",
    "    Load buy and sell signals for a specific year.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    year : int\n",
    "        Year to load signals for\n",
    "    buys_path_template : str\n",
    "        Template for buys file path (use {} for year placeholder)\n",
    "    sells_path_template : str\n",
    "        Template for sells file path (use {} for year placeholder)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    permno_set : set\n",
    "        Set of permnos in the buy and sell signals for this year\n",
    "    \"\"\"\n",
    "    try:\n",
    "        buys = pd.read_csv(buys_path_template.format(year), index_col=1)\n",
    "        sells = pd.read_csv(sells_path_template.format(year), index_col=1)\n",
    "        \n",
    "        buys.index.name = 'permno'\n",
    "        sells.index.name = 'permno'\n",
    "        \n",
    "        buys_index = buys.index.astype(int)\n",
    "        sells_index = sells.index.astype(int)\n",
    "        \n",
    "        return set(buys_index.union(sells_index))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ⚠ Warning: Could not load signals for year {year}: {e}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def backtest_dnn_yearly(df, \n",
    "                                  test_start_date='2020-01-31', \n",
    "                                  test_end_date='2024-11-30',\n",
    "                                  lookback_window=180,\n",
    "                                  transaction_cost=0.001,\n",
    "                                  buys_path_template='buys_{}.csv',\n",
    "                                  sells_path_template='sells_{}.csv',\n",
    "                                  data_factor=None,\n",
    "                                  verbose=True):\n",
    "    \"\"\"\n",
    "    Backtest Nodewise + GMV strategy with year-specific buy/sell signals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: permno, datadate, ret_fwd_1\n",
    "    test_start_date : str\n",
    "        First date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    test_end_date : str\n",
    "        Last date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    lookback_window : int\n",
    "        Number of months in rolling training window (default: 180)\n",
    "    transaction_cost : float\n",
    "        Proportional transaction cost (default: 0.001 = 10 bps)\n",
    "    buys_path_template : str\n",
    "        Template for buys file path (use {} for year placeholder)\n",
    "    sells_path_template : str\n",
    "        Template for sells file path (use {} for year placeholder)\n",
    "    verbose : bool\n",
    "        If True, prints detailed log at each time step.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with columns: date, portfolio_return, cumulative_return\n",
    "    metrics : dict\n",
    "        Overall performance metrics\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    df = df.copy()\n",
    "    if 'datadate' not in df.columns or 'permno' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'datadate' and 'permno' columns\")\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'])\n",
    "    \n",
    "    # Get unique dates\n",
    "    all_dates = sorted(df['datadate'].unique())\n",
    "    \n",
    "    # Convert test dates to datetime\n",
    "    test_start_dt = pd.to_datetime(test_start_date)\n",
    "    test_end_dt = pd.to_datetime(test_end_date)\n",
    "    \n",
    "    # Find date indices\n",
    "    try:\n",
    "        test_start_idx = all_dates.index(test_start_dt)\n",
    "        test_end_idx = all_dates.index(test_end_dt)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Date not found in DataFrame: {e}\")\n",
    "    \n",
    "    if test_start_idx < lookback_window:\n",
    "        raise ValueError(f\"Not enough data for lookback. Test start date {test_start_date} \"\n",
    "                         f\"requires data back to {all_dates[test_start_idx - lookback_window]}, \"\n",
    "                         f\"but only {test_start_idx} periods are available.\")\n",
    "    \n",
    "    # Storage for results\n",
    "    portfolio_returns = []\n",
    "    portfolio_dates = []\n",
    "    portfolio_weights_list = []\n",
    "    portfolio_turnover_list = []\n",
    "    portfolio_gross_returns = []\n",
    "    \n",
    "    portfolio_returns_2 = []\n",
    "    portfolio_dates_2 = []\n",
    "    portfolio_weights_list_2 = []\n",
    "    portfolio_turnover_list_2 = []\n",
    "    portfolio_gross_returns_2 = []\n",
    "    \n",
    "    portfolio_returns_3 = []\n",
    "    portfolio_dates_3 = []\n",
    "    portfolio_weights_list_3 = []\n",
    "    portfolio_turnover_list_3 = []\n",
    "    portfolio_gross_returns_3 = []\n",
    "    \n",
    "    # Track weights by permno\n",
    "    prev_weights_dict = {}\n",
    "    prev_oos_returns_dict = {}\n",
    "    prev_gross_return = 0.0\n",
    "    \n",
    "    prev_weights_dict_2 = {}\n",
    "    prev_oos_returns_dict_2 = {}\n",
    "    prev_gross_return_2 = 0.0\n",
    "    \n",
    "    prev_weights_dict_3 = {}\n",
    "    prev_oos_returns_dict_3 = {}\n",
    "    prev_gross_return_3 = 0.0\n",
    "    \n",
    "    # Cache for yearly signals\n",
    "    yearly_signals_cache = {}\n",
    "    \n",
    "    # --- 2. Rolling Window Backtest ---\n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING BACKTEST WITH YEARLY SIGNALS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    for t in range(test_start_idx, test_end_idx + 1):\n",
    "        current_date = all_dates[t]\n",
    "        current_year = current_date.year\n",
    "        \n",
    "        # Define the lookback window\n",
    "        window_start_date = all_dates[t - lookback_window]\n",
    "        window_end_date = all_dates[t - 1]\n",
    "        \n",
    "        # Get training data for this window, filtered by current year's permnos\n",
    "        train_data = df[(df['datadate'] >= window_start_date) & \n",
    "                        (df['datadate'] <= window_end_date) ]\n",
    "        train_factor = data_factor.loc[window_start_date : window_end_date]\n",
    "        \n",
    "        # Pivot to get returns matrix (time x assets)\n",
    "        returns_pivot = train_data.pivot(index='datadate', columns='permno', values='ret_fwd_1')\n",
    "        \n",
    "        # Reindex to ensure all dates are present\n",
    "        window_dates = all_dates[t - lookback_window : t]\n",
    "        returns_pivot = returns_pivot.reindex(index=window_dates)\n",
    "        \n",
    "        # Filter assets with any NaNs in this window\n",
    "        nan_assets = returns_pivot.columns[returns_pivot.isna().any()]\n",
    "        filtered_pivot = returns_pivot.drop(columns=nan_assets)\n",
    "        \n",
    "        current_assets = filtered_pivot.columns.tolist()\n",
    "        Y = filtered_pivot.values\n",
    "        n_train, p_current = Y.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[{t - test_start_idx + 1}/{test_end_idx - test_start_idx + 1}] \"\n",
    "                  f\"Date: {current_date.strftime('%Y-%m-%d')} | Year: {current_year}\")\n",
    "            print(f\"  Window: {window_start_date.strftime('%Y-%m-%d')} to \"\n",
    "                  f\"{window_end_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Assets with data: {p_current}\")\n",
    "\n",
    "        # Check for valid data\n",
    "        if n_train < lookback_window or p_current < 2:\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Insufficient data (n={n_train}, p={p_current}), using prev weights\")\n",
    "            # Filter previous weights to only allowed permnos\n",
    "            new_weights_dict = prev_weights_dict.copy()\n",
    "            new_weights_dict_2 = prev_weights_dict_2.copy()\n",
    "            new_weights_dict_3 = prev_weights_dict_3.copy()\n",
    "        else:\n",
    "            try:\n",
    "                # Demean the returns\n",
    "                Y_bar = Y.mean(axis=0)\n",
    "                Y_star = Y - Y_bar\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Running Deep Learning Regression...\")\n",
    "                F = train_factor.values.astype(float)\n",
    "                res_nnet_fm=DNN_FM_main(Y_star, F, architecture=5, const_err_cov=2.5, use_CV_err=False, eval_type='frob')\n",
    "                Theta_hat = res_nnet_fm['inv_sigma_hat']\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing GMV weights...\")\n",
    "                w_star = gmv_weights(Theta_hat)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing MV weights...\")\n",
    "                w_star_2 = mv_weights(Theta_hat,Y_bar,target_return=0.01)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing MSR weights...\")\n",
    "                w_star_3 = msr_weights(Theta_hat,Y_bar)\n",
    "                \n",
    "                # Create weights dictionary\n",
    "                new_weights_dict = {asset: w_star[i] for i, asset in enumerate(current_assets)}\n",
    "                new_weights_dict_2 = {asset: w_star_2[i] for i, asset in enumerate(current_assets)}\n",
    "                new_weights_dict_3 = {asset: w_star_3[i] for i, asset in enumerate(current_assets)}\n",
    "                \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"  ✗ Error: {e}\")\n",
    "                    print(f\"  Using previous weights\")\n",
    "                # Filter previous weights to only allowed permnos\n",
    "                new_weights_dict = {k: v for k, v in prev_weights_dict.items() if k in allowed_permnos}\n",
    "                new_weights_dict_2 = {k: v for k, v in prev_weights_dict_2.items() if k in allowed_permnos}\n",
    "                new_weights_dict_3 = {k: v for k, v in prev_weights_dict_3.items() if k in allowed_permnos}\n",
    "\n",
    "        # Normalize weights to sum to 1\n",
    "        weight_sum = sum(new_weights_dict.values())\n",
    "        if weight_sum > 1e-10:\n",
    "            new_weights_dict = {k: v/weight_sum for k, v in new_weights_dict.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum, using previous weights\")\n",
    "            new_weights_dict = {k: v for k, v in prev_weights_dict.items() if k in allowed_permnos}\n",
    "            weight_sum = sum(new_weights_dict.values())\n",
    "            if weight_sum > 1e-10:\n",
    "                new_weights_dict = {k: v/weight_sum for k, v in new_weights_dict.items()}\n",
    "        \n",
    "        weight_sum_2 = sum(new_weights_dict_2.values())\n",
    "        if weight_sum_2 > 1e-10:\n",
    "            new_weights_dict_2 = {k: v/weight_sum_2 for k, v in new_weights_dict_2.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum, using previous weights\")\n",
    "            new_weights_dict_2 = {k: v for k, v in prev_weights_dict_2.items() if k in allowed_permnos}\n",
    "            weight_sum_2 = sum(new_weights_dict_2.values())\n",
    "            if weight_sum_2 > 1e-10:\n",
    "                new_weights_dict_2 = {k: v/weight_sum_2 for k, v in new_weights_dict_2.items()}\n",
    "        \n",
    "        weight_sum_3 = sum(new_weights_dict_3.values())\n",
    "        if weight_sum_3 > 1e-10:\n",
    "            new_weights_dict_3 = {k: v/weight_sum_3 for k, v in new_weights_dict_3.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum, using previous weights\")\n",
    "            new_weights_dict_3 = {k: v for k, v in prev_weights_dict_3.items() if k in allowed_permnos}\n",
    "            weight_sum_3 = sum(new_weights_dict_3.values())\n",
    "            if weight_sum_3 > 1e-10:\n",
    "                new_weights_dict_3 = {k: v/weight_sum_3 for k, v in new_weights_dict_3.items()}\n",
    "        \n",
    "        # --- 3. OOS Returns & Transaction Costs ---\n",
    "        \n",
    "        # Get out-of-sample returns for current month (only for allowed permnos)\n",
    "        oos_data = df[df['datadate'] == current_date]\n",
    "        oos_returns_series = oos_data.set_index('permno')['ret_fwd_1']\n",
    "        \n",
    "        # Filter out NaN returns\n",
    "        oos_returns_series = oos_returns_series.dropna()\n",
    "        oos_returns_dict = oos_returns_series.to_dict()\n",
    "        \n",
    "        # Find common assets between weights and returns\n",
    "        common_assets = set(new_weights_dict.keys()) & set(oos_returns_dict.keys())\n",
    "        common_assets_2 = set(new_weights_dict_2.keys()) & set(oos_returns_dict.keys())\n",
    "        common_assets_3 = set(new_weights_dict_3.keys()) & set(oos_returns_dict.keys())\n",
    "        \n",
    "        if len(common_assets) == 0 or len(common_assets_2)==0 or  len(common_assets_3)==0:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ No common assets with valid returns, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to common assets and renormalize\n",
    "        common_weights = {a: new_weights_dict[a] for a in common_assets}\n",
    "        common_weight_sum = sum(common_weights.values())\n",
    "        if common_weight_sum > 1e-10:\n",
    "            common_weights = {k: v/common_weight_sum for k, v in common_weights.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        common_weights_2 = {a: new_weights_dict_2[a] for a in common_assets_2}\n",
    "        common_weight_sum_2 = sum(common_weights_2.values())\n",
    "        if common_weight_sum_2 > 1e-10:\n",
    "            common_weights_2 = {k: v/common_weight_sum_2 for k, v in common_weights_2.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        common_weights_3 = {a: new_weights_dict_3[a] for a in common_assets_3}\n",
    "        common_weight_sum_3 = sum(common_weights_3.values())\n",
    "        if common_weight_sum_3 > 1e-10:\n",
    "            common_weights_3 = {k: v/common_weight_sum_3 for k, v in common_weights_3.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Compute gross portfolio return\n",
    "        gross_return = sum(common_weights[a] * oos_returns_dict[a] for a in common_assets)\n",
    "        gross_return_2 = sum(common_weights_2[a] * oos_returns_dict[a] for a in common_assets_2)\n",
    "        gross_return_3 = sum(common_weights_3[a] * oos_returns_dict[a] for a in common_assets_3)\n",
    "        \n",
    "        # Sanity check\n",
    "        if np.isnan(gross_return) or np.isinf(gross_return):\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Invalid gross return: {gross_return}, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate transaction costs\n",
    "        if len(prev_weights_dict) > 0:\n",
    "            all_traded_assets = set(common_weights.keys()) | set(prev_weights_dict.keys())\n",
    "            \n",
    "            # Adjust previous weights\n",
    "            adjusted_prev = {}\n",
    "            for asset in all_traded_assets:\n",
    "                prev_w = prev_weights_dict.get(asset, 0.0)\n",
    "                \n",
    "                if asset in prev_oos_returns_dict:\n",
    "                    prev_r = prev_oos_returns_dict[asset]\n",
    "                    if prev_gross_return > -0.99:\n",
    "                        adjusted_prev[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return)\n",
    "                    else:\n",
    "                        adjusted_prev[asset] = 0.0\n",
    "                else:\n",
    "                    adjusted_prev[asset] = 0.0\n",
    "            \n",
    "            # Renormalize adjusted weights\n",
    "            adj_sum = sum(adjusted_prev.get(a, 0.0) for a in common_weights.keys())\n",
    "            if adj_sum > 1e-10:\n",
    "                adjusted_prev_normalized = {k: adjusted_prev.get(k, 0.0)/adj_sum \n",
    "                                           for k in common_weights.keys()}\n",
    "            else:\n",
    "                adjusted_prev_normalized = {k: 0.0 for k in common_weights.keys()}\n",
    "            \n",
    "            # Turnover\n",
    "            turnover = sum(abs(common_weights.get(a, 0.0) - adjusted_prev_normalized.get(a, 0.0)) \n",
    "                          for a in all_traded_assets)\n",
    "            \n",
    "            # Transaction cost\n",
    "            tc = transaction_cost * (1 + gross_return) * turnover\n",
    "        else:\n",
    "            # First period\n",
    "            turnover = sum(abs(w) for w in common_weights.values())\n",
    "            tc = transaction_cost * turnover\n",
    "        \n",
    "        if len(prev_weights_dict_2) > 0:\n",
    "            all_traded_assets_2 = set(common_weights_2.keys()) | set(prev_weights_dict_2.keys())\n",
    "            \n",
    "            # Adjust previous weights\n",
    "            adjusted_prev_2 = {}\n",
    "            for asset in all_traded_assets_2:\n",
    "                prev_w = prev_weights_dict_2.get(asset, 0.0)\n",
    "                \n",
    "                if asset in prev_oos_returns_dict_2:\n",
    "                    prev_r = prev_oos_returns_dict_2[asset]\n",
    "                    if prev_gross_return_2 > -0.99:\n",
    "                        adjusted_prev_2[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return_2)\n",
    "                    else:\n",
    "                        adjusted_prev_2[asset] = 0.0\n",
    "                else:\n",
    "                    adjusted_prev_2[asset] = 0.0\n",
    "            \n",
    "            # Renormalize adjusted weights\n",
    "            adj_sum = sum(adjusted_prev_2.get(a, 0.0) for a in common_weights_2.keys())\n",
    "            if adj_sum > 1e-10:\n",
    "                adjusted_prev_normalized = {k: adjusted_prev_2.get(k, 0.0)/adj_sum \n",
    "                                           for k in common_weights_2.keys()}\n",
    "            else:\n",
    "                adjusted_prev_normalized = {k: 0.0 for k in common_weights_2.keys()}\n",
    "            \n",
    "            # Turnover\n",
    "            turnover_2 = sum(abs(common_weights_2.get(a, 0.0) - adjusted_prev_normalized.get(a, 0.0)) \n",
    "                          for a in all_traded_assets_2)\n",
    "            \n",
    "            # Transaction cost\n",
    "            tc_2 = transaction_cost * (1 + gross_return_2) * turnover_2\n",
    "        else:\n",
    "            # First period\n",
    "            turnover_2 = sum(abs(w) for w in common_weights_2.values())\n",
    "            tc_2 = transaction_cost * turnover_2\n",
    "        \n",
    "        if len(prev_weights_dict_3) > 0:\n",
    "            all_traded_assets_3 = set(common_weights_3.keys()) | set(prev_weights_dict_3.keys())\n",
    "            \n",
    "            # Adjust previous weights\n",
    "            adjusted_prev_3 = {}\n",
    "            for asset in all_traded_assets_3:\n",
    "                prev_w = prev_weights_dict_3.get(asset, 0.0)\n",
    "                \n",
    "                if asset in prev_oos_returns_dict_3:\n",
    "                    prev_r = prev_oos_returns_dict_3[asset]\n",
    "                    if prev_gross_return_3 > -0.99:\n",
    "                        adjusted_prev_3[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return_3)\n",
    "                    else:\n",
    "                        adjusted_prev_3[asset] = 0.0\n",
    "                else:\n",
    "                    adjusted_prev_3[asset] = 0.0\n",
    "            \n",
    "            # Renormalize adjusted weights\n",
    "            adj_sum = sum(adjusted_prev_3.get(a, 0.0) for a in common_weights_3.keys())\n",
    "            if adj_sum > 1e-10:\n",
    "                adjusted_prev_normalized = {k: adjusted_prev_3.get(k, 0.0)/adj_sum \n",
    "                                           for k in common_weights_3.keys()}\n",
    "            else:\n",
    "                adjusted_prev_normalized = {k: 0.0 for k in common_weights_3.keys()}\n",
    "            \n",
    "            # Turnover\n",
    "            turnover_3 = sum(abs(common_weights_3.get(a, 0.0) - adjusted_prev_normalized.get(a, 0.0)) \n",
    "                          for a in all_traded_assets_3)\n",
    "            \n",
    "            # Transaction cost\n",
    "            tc_3 = transaction_cost * (1 + gross_return_3) * turnover_3\n",
    "        else:\n",
    "            # First period\n",
    "            turnover_3 = sum(abs(w) for w in common_weights_3.values())\n",
    "            tc_3 = transaction_cost * turnover_3\n",
    "        \n",
    "        # Net return\n",
    "        net_return = gross_return - tc\n",
    "        net_return_2 = gross_return_2 - tc_2\n",
    "        net_return_3 = gross_return_3 - tc_3\n",
    "        \n",
    "        # Store results\n",
    "        portfolio_returns.append(net_return)\n",
    "        portfolio_dates.append(current_date)\n",
    "        portfolio_weights_list.append(common_weights.copy())\n",
    "        portfolio_turnover_list.append(turnover)\n",
    "        portfolio_gross_returns.append(gross_return)\n",
    "        \n",
    "        portfolio_returns_2.append(net_return_2)\n",
    "        portfolio_dates_2.append(current_date)\n",
    "        portfolio_weights_list_2.append(common_weights_2.copy())\n",
    "        portfolio_turnover_list_2.append(turnover_2)\n",
    "        portfolio_gross_returns_2.append(gross_return_2)\n",
    "        \n",
    "        portfolio_returns_3.append(net_return_3)\n",
    "        portfolio_dates_3.append(current_date)\n",
    "        portfolio_weights_list_3.append(common_weights_3.copy())\n",
    "        portfolio_turnover_list_3.append(turnover_3)\n",
    "        portfolio_gross_returns_3.append(gross_return_3)\n",
    "        \n",
    "        # Update previous values for next iteration\n",
    "        prev_weights_dict = common_weights.copy()\n",
    "        prev_oos_returns_dict = {a: oos_returns_dict[a] for a in common_assets}\n",
    "        prev_gross_return = gross_return\n",
    "        \n",
    "        prev_weights_dict_2 = common_weights_2.copy()\n",
    "        prev_oos_returns_dict_2 = {a: oos_returns_dict[a] for a in common_assets_2}\n",
    "        prev_gross_return_2 = gross_return_2\n",
    "        \n",
    "        prev_weights_dict_3 = common_weights_3.copy()\n",
    "        prev_oos_returns_dict_3 = {a: oos_returns_dict[a] for a in common_assets_3}\n",
    "        prev_gross_return_3 = gross_return_3\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  GMV\", f\"  Gross: {gross_return:>8.5f} | Turnover: {turnover:>6.4f} | \"\n",
    "                  f\"TC: {tc:>8.6f} | Net: {net_return:>8.5f}\")\n",
    "            print(f\"  MV\", f\"  Gross: {gross_return_2:>8.5f} | Turnover: {turnover_2:>6.4f} | \"\n",
    "                  f\"TC: {tc_2:>8.6f} | Net: {net_return_2:>8.5f}\")\n",
    "            print(f\"  MSR\", f\"  Gross: {gross_return_3:>8.5f} | Turnover: {turnover_3:>6.4f} | \"\n",
    "                  f\"TC: {tc_3:>8.6f} | Net: {net_return_3:>8.5f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BACKTEST COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # --- 4. Compile Results ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'date': portfolio_dates,\n",
    "        'portfolio_return': portfolio_returns,\n",
    "        'portfolio_gross_return': portfolio_gross_returns,\n",
    "        'portfolio_weights': portfolio_weights_list,\n",
    "        'portfolio_turnover': portfolio_turnover_list\n",
    "    })\n",
    "    results_df['cumulative_return'] = (1 + results_df['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    results_df_2 = pd.DataFrame({\n",
    "        'date': portfolio_dates,\n",
    "        'portfolio_return': portfolio_returns_2,\n",
    "        'portfolio_gross_return': portfolio_gross_returns_2,\n",
    "        'portfolio_weights': portfolio_weights_list_2,\n",
    "        'portfolio_turnover': portfolio_turnover_list_2\n",
    "    })\n",
    "    results_df_2['cumulative_return'] = (1 + results_df_2['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    results_df_3 = pd.DataFrame({\n",
    "        'date': portfolio_dates,\n",
    "        'portfolio_return': portfolio_returns_3,\n",
    "        'portfolio_gross_return': portfolio_gross_returns_3,\n",
    "        'portfolio_weights': portfolio_weights_list_3,\n",
    "        'portfolio_turnover': portfolio_turnover_list_3\n",
    "    })\n",
    "    results_df_3['cumulative_return'] = (1 + results_df_3['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    if len(portfolio_returns) > 0:\n",
    "        mean_return = np.mean(portfolio_returns)\n",
    "        variance = np.var(portfolio_returns, ddof=1)\n",
    "        sharpe_ratio = mean_return / np.sqrt(variance) if variance > 0 else 0\n",
    "        \n",
    "        # Annualized metrics (monthly data)\n",
    "        annual_return = mean_return * 12\n",
    "        annual_volatility = np.sqrt(variance * 12)\n",
    "        annual_sharpe = annual_return / annual_volatility if annual_volatility > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'mean_return': mean_return,\n",
    "            'variance': variance,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'annual_return': annual_return,\n",
    "            'annual_volatility': annual_volatility,\n",
    "            'annual_sharpe_ratio': annual_sharpe,\n",
    "            'total_return': results_df['cumulative_return'].iloc[-1],\n",
    "            'avg_turnover': np.mean(portfolio_turnover_list),\n",
    "            'n_periods': len(portfolio_returns)\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'mean_return': 0,\n",
    "            'variance': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'annual_return': 0,\n",
    "            'annual_volatility': 0,\n",
    "            'annual_sharpe_ratio': 0,\n",
    "            'total_return': 0,\n",
    "            'avg_turnover': 0,\n",
    "            'n_periods': 0\n",
    "        }\n",
    "        \n",
    "    if len(portfolio_returns_2) > 0:\n",
    "        mean_return_2 = np.mean(portfolio_returns_2)\n",
    "        variance_2 = np.var(portfolio_returns_2, ddof=1)\n",
    "        sharpe_ratio_2 = mean_return_2 / np.sqrt(variance_2) if variance_2 > 0 else 0\n",
    "        \n",
    "        # Annualized metrics (monthly data)\n",
    "        annual_return_2 = mean_return_2 * 12\n",
    "        annual_volatility_2 = np.sqrt(variance_2 * 12)\n",
    "        annual_sharpe_2 = annual_return_2 / annual_volatility_2 if annual_volatility_2 > 0 else 0\n",
    "        \n",
    "        metrics_2 = {\n",
    "            'mean_return': mean_return_2,\n",
    "            'variance': variance_2,\n",
    "            'sharpe_ratio': sharpe_ratio_2,\n",
    "            'annual_return': annual_return_2,\n",
    "            'annual_volatility': annual_volatility_2,\n",
    "            'annual_sharpe_ratio': annual_sharpe_2,\n",
    "            'total_return': results_df_2['cumulative_return'].iloc[-1],\n",
    "            'avg_turnover': np.mean(portfolio_turnover_list_2),\n",
    "            'n_periods': len(portfolio_returns_2)\n",
    "        }\n",
    "    else:\n",
    "        metrics_2 = {\n",
    "            'mean_return': 0,\n",
    "            'variance': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'annual_return': 0,\n",
    "            'annual_volatility': 0,\n",
    "            'annual_sharpe_ratio': 0,\n",
    "            'total_return': 0,\n",
    "            'avg_turnover': 0,\n",
    "            'n_periods': 0\n",
    "        }\n",
    "    \n",
    "    if len(portfolio_returns_3) > 0:\n",
    "        mean_return_3 = np.mean(portfolio_returns_3)\n",
    "        variance_3 = np.var(portfolio_returns_3, ddof=1)\n",
    "        sharpe_ratio_3 = mean_return_3 / np.sqrt(variance_3) if variance_3 > 0 else 0\n",
    "        \n",
    "        # Annualized metrics (monthly data)\n",
    "        annual_return_3 = mean_return_3 * 12\n",
    "        annual_volatility_3 = np.sqrt(variance_3 * 12)\n",
    "        annual_sharpe_3 = annual_return_3 / annual_volatility_3 if annual_volatility_3 > 0 else 0\n",
    "        \n",
    "        metrics_3 = {\n",
    "            'mean_return': mean_return_3,\n",
    "            'variance': variance_3,\n",
    "            'sharpe_ratio': sharpe_ratio_3,\n",
    "            'annual_return': annual_return_3,\n",
    "            'annual_volatility': annual_volatility_3,\n",
    "            'annual_sharpe_ratio': annual_sharpe_3,\n",
    "            'total_return': results_df_3['cumulative_return'].iloc[-1],\n",
    "            'avg_turnover': np.mean(portfolio_turnover_list_3),\n",
    "            'n_periods': len(portfolio_returns_3)\n",
    "        }\n",
    "    else:\n",
    "        metrics_3 = {\n",
    "            'mean_return': 0,\n",
    "            'variance': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'annual_return': 0,\n",
    "            'annual_volatility': 0,\n",
    "            'annual_sharpe_ratio': 0,\n",
    "            'total_return': 0,\n",
    "            'avg_turnover': 0,\n",
    "            'n_periods': 0\n",
    "        }\n",
    "    \n",
    "    return results_df, metrics, results_df_2, metrics_2, results_df_3, metrics_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f77cbae-5233-4cbe-8d95-7d8e0a1a7e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING BACKTEST WITH YEARLY SIGNALS\n",
      "============================================================\n",
      "\n",
      "[1/59] Date: 2020-01-31 | Year: 2020\n",
      "  Window: 2005-01-31 to 2019-12-31\n",
      "  Assets with data: 252\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.50814 | Turnover: 96.8305 | TC: 0.096830 | Net:  0.41131\n",
      "  MV   Gross:  0.64916 | Turnover: 112.6244 | TC: 0.112624 | Net:  0.53654\n",
      "  MSR   Gross: -2.73139 | Turnover: 281.7154 | TC: 0.281715 | Net: -3.01311\n",
      "\n",
      "[2/59] Date: 2020-02-29 | Year: 2020\n",
      "  Window: 2005-02-28 to 2020-01-31\n",
      "  Assets with data: 252\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.29923 | Turnover: 29.9309 | TC: 0.038887 | Net:  0.26034\n",
      "  MV   Gross:  0.31871 | Turnover: 33.4574 | TC: 0.044121 | Net:  0.27459\n",
      "  MSR   Gross:  0.72627 | Turnover: 35.9642 | TC: 0.062084 | Net:  0.66419\n",
      "\n",
      "[3/59] Date: 2020-03-31 | Year: 2020\n",
      "  Window: 2005-03-31 to 2020-02-29\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x0000023D937BFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00676 | Turnover: 4.5080 | TC: 0.004478 | Net: -0.01124\n",
      "  MV   Gross: -0.00750 | Turnover: 4.4777 | TC: 0.004444 | Net: -0.01194\n",
      "  MSR   Gross: -0.04356 | Turnover: 4.5932 | TC: 0.004393 | Net: -0.04795\n",
      "\n",
      "[4/59] Date: 2020-04-30 | Year: 2020\n",
      "  Window: 2005-04-30 to 2020-03-31\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x0000023D94150F40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00923 | Turnover: 6.4605 | TC: 0.006401 | Net: -0.01563\n",
      "  MV   Gross: -0.00645 | Turnover: 6.4388 | TC: 0.006397 | Net: -0.01285\n",
      "  MSR   Gross:  0.09633 | Turnover: 6.0226 | TC: 0.006603 | Net:  0.08973\n",
      "\n",
      "[5/59] Date: 2020-05-31 | Year: 2020\n",
      "  Window: 2005-05-31 to 2020-04-30\n",
      "  Assets with data: 250\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.11175 | Turnover: 1.3467 | TC: 0.001196 | Net: -0.11294\n",
      "  MV   Gross: -0.11178 | Turnover: 1.3617 | TC: 0.001210 | Net: -0.11299\n",
      "  MSR   Gross: -0.12364 | Turnover: 1.4484 | TC: 0.001269 | Net: -0.12491\n",
      "\n",
      "[6/59] Date: 2020-06-30 | Year: 2020\n",
      "  Window: 2005-06-30 to 2020-05-31\n",
      "  Assets with data: 248\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.04738 | Turnover: 4.3470 | TC: 0.004553 | Net:  0.04283\n",
      "  MV   Gross:  0.04851 | Turnover: 4.3606 | TC: 0.004572 | Net:  0.04394\n",
      "  MSR   Gross:  0.12885 | Turnover: 4.2030 | TC: 0.004745 | Net:  0.12410\n",
      "\n",
      "[7/59] Date: 2020-07-31 | Year: 2020\n",
      "  Window: 2005-07-31 to 2020-06-30\n",
      "  Assets with data: 250\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05790 | Turnover: 2.1008 | TC: 0.001979 | Net: -0.05988\n",
      "  MV   Gross: -0.05878 | Turnover: 2.1460 | TC: 0.002020 | Net: -0.06080\n",
      "  MSR   Gross:  0.01499 | Turnover: 1.9725 | TC: 0.002002 | Net:  0.01298\n",
      "\n",
      "[8/59] Date: 2020-08-31 | Year: 2020\n",
      "  Window: 2005-08-31 to 2020-07-31\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.00250 | Turnover: 5.6609 | TC: 0.005675 | Net: -0.00317\n",
      "  MV   Gross:  0.00236 | Turnover: 5.7160 | TC: 0.005729 | Net: -0.00337\n",
      "  MSR   Gross:  0.03679 | Turnover: 3.2886 | TC: 0.003410 | Net:  0.03338\n",
      "\n",
      "[9/59] Date: 2020-09-30 | Year: 2020\n",
      "  Window: 2005-09-30 to 2020-08-31\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.09338 | Turnover: 1.0704 | TC: 0.000970 | Net: -0.09435\n",
      "  MV   Gross: -0.09284 | Turnover: 1.0894 | TC: 0.000988 | Net: -0.09383\n",
      "  MSR   Gross: -0.12218 | Turnover: 1.6082 | TC: 0.001412 | Net: -0.12359\n",
      "\n",
      "[10/59] Date: 2020-10-31 | Year: 2020\n",
      "  Window: 2005-10-31 to 2020-09-30\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06886 | Turnover: 2.8667 | TC: 0.002669 | Net: -0.07153\n",
      "  MV   Gross: -0.06784 | Turnover: 2.8723 | TC: 0.002677 | Net: -0.07052\n",
      "  MSR   Gross: -0.13155 | Turnover: 2.9535 | TC: 0.002565 | Net: -0.13411\n",
      "\n",
      "[11/59] Date: 2020-11-30 | Year: 2020\n",
      "  Window: 2005-11-30 to 2020-10-31\n",
      "  Assets with data: 252\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05441 | Turnover: 3.0077 | TC: 0.002844 | Net: -0.05726\n",
      "  MV   Gross: -0.05422 | Turnover: 3.0112 | TC: 0.002848 | Net: -0.05707\n",
      "  MSR   Gross: -0.08138 | Turnover: 3.7102 | TC: 0.003408 | Net: -0.08479\n",
      "\n",
      "[12/59] Date: 2020-12-31 | Year: 2020\n",
      "  Window: 2005-12-31 to 2020-11-30\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06215 | Turnover: 0.7308 | TC: 0.000685 | Net: -0.06284\n",
      "  MV   Gross: -0.06291 | Turnover: 0.7242 | TC: 0.000679 | Net: -0.06359\n",
      "  MSR   Gross: -0.09466 | Turnover: 1.3347 | TC: 0.001208 | Net: -0.09586\n",
      "\n",
      "[13/59] Date: 2021-01-31 | Year: 2021\n",
      "  Window: 2006-01-31 to 2020-12-31\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.25221 | Turnover: 1.5194 | TC: 0.001136 | Net: -0.25335\n",
      "  MV   Gross: -0.25848 | Turnover: 1.5597 | TC: 0.001157 | Net: -0.25963\n",
      "  MSR   Gross: -0.33399 | Turnover: 3.0495 | TC: 0.002031 | Net: -0.33602\n",
      "\n",
      "[14/59] Date: 2021-02-28 | Year: 2021\n",
      "  Window: 2006-02-28 to 2021-01-31\n",
      "  Assets with data: 252\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.07670 | Turnover: 4.3119 | TC: 0.004643 | Net:  0.07206\n",
      "  MV   Gross:  0.05507 | Turnover: 3.7910 | TC: 0.004000 | Net:  0.05107\n",
      "  MSR   Gross: -0.09506 | Turnover: 3.4339 | TC: 0.003107 | Net: -0.09817\n",
      "\n",
      "[15/59] Date: 2021-03-31 | Year: 2021\n",
      "  Window: 2006-03-31 to 2021-02-28\n",
      "  Assets with data: 251\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.04549 | Turnover: 1.0665 | TC: 0.001115 | Net:  0.04437\n",
      "  MV   Gross:  0.04693 | Turnover: 1.4394 | TC: 0.001507 | Net:  0.04542\n",
      "  MSR   Gross:  0.05906 | Turnover: 7.6799 | TC: 0.008133 | Net:  0.05092\n",
      "\n",
      "[16/59] Date: 2021-04-30 | Year: 2021\n",
      "  Window: 2006-04-30 to 2021-03-31\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.02386 | Turnover: 2.6609 | TC: 0.002597 | Net: -0.02645\n",
      "  MV   Gross: -0.03492 | Turnover: 2.6512 | TC: 0.002559 | Net: -0.03748\n",
      "  MSR   Gross: -0.11466 | Turnover: 3.3997 | TC: 0.003010 | Net: -0.11767\n",
      "\n",
      "[17/59] Date: 2021-05-31 | Year: 2021\n",
      "  Window: 2006-05-31 to 2021-04-30\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05116 | Turnover: 1.1205 | TC: 0.001063 | Net: -0.05222\n",
      "  MV   Gross: -0.04695 | Turnover: 1.2394 | TC: 0.001181 | Net: -0.04813\n",
      "  MSR   Gross: -0.01973 | Turnover: 3.1203 | TC: 0.003059 | Net: -0.02279\n",
      "\n",
      "[18/59] Date: 2021-06-30 | Year: 2021\n",
      "  Window: 2006-06-30 to 2021-05-31\n",
      "  Assets with data: 254\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.02216 | Turnover: 1.4816 | TC: 0.001514 | Net:  0.02065\n",
      "  MV   Gross:  0.02813 | Turnover: 1.6963 | TC: 0.001744 | Net:  0.02639\n",
      "  MSR   Gross:  0.07422 | Turnover: 2.8225 | TC: 0.003032 | Net:  0.07119\n",
      "\n",
      "[19/59] Date: 2021-07-31 | Year: 2021\n",
      "  Window: 2006-07-31 to 2021-06-30\n",
      "  Assets with data: 254\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.03524 | Turnover: 0.7336 | TC: 0.000760 | Net:  0.03448\n",
      "  MV   Gross:  0.03853 | Turnover: 0.8209 | TC: 0.000853 | Net:  0.03767\n",
      "  MSR   Gross:  0.06423 | Turnover: 1.9142 | TC: 0.002037 | Net:  0.06219\n",
      "\n",
      "[20/59] Date: 2021-08-31 | Year: 2021\n",
      "  Window: 2006-08-31 to 2021-07-31\n",
      "  Assets with data: 255\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06122 | Turnover: 1.2325 | TC: 0.001157 | Net: -0.06238\n",
      "  MV   Gross: -0.06655 | Turnover: 1.3146 | TC: 0.001227 | Net: -0.06778\n",
      "  MSR   Gross: -0.10817 | Turnover: 2.2563 | TC: 0.002012 | Net: -0.11018\n",
      "\n",
      "[21/59] Date: 2021-09-30 | Year: 2021\n",
      "  Window: 2006-09-30 to 2021-08-31\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06125 | Turnover: 0.8057 | TC: 0.000756 | Net: -0.06200\n",
      "  MV   Gross: -0.05128 | Turnover: 0.8996 | TC: 0.000854 | Net: -0.05213\n",
      "  MSR   Gross:  0.01127 | Turnover: 1.5406 | TC: 0.001558 | Net:  0.00972\n",
      "\n",
      "[22/59] Date: 2021-10-31 | Year: 2021\n",
      "  Window: 2006-10-31 to 2021-09-30\n",
      "  Assets with data: 253\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.08791 | Turnover: 1.8364 | TC: 0.001675 | Net: -0.08959\n",
      "  MV   Gross: -0.08302 | Turnover: 1.8645 | TC: 0.001710 | Net: -0.08473\n",
      "  MSR   Gross: -0.05220 | Turnover: 1.9977 | TC: 0.001893 | Net: -0.05410\n",
      "\n",
      "[23/59] Date: 2021-11-30 | Year: 2021\n",
      "  Window: 2006-11-30 to 2021-10-31\n",
      "  Assets with data: 257\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.15648 | Turnover: 1.9587 | TC: 0.002265 | Net:  0.15422\n",
      "  MV   Gross:  0.15719 | Turnover: 2.2886 | TC: 0.002648 | Net:  0.15454\n",
      "  MSR   Gross:  0.16213 | Turnover: 4.3622 | TC: 0.005069 | Net:  0.15706\n",
      "\n",
      "[24/59] Date: 2021-12-31 | Year: 2021\n",
      "  Window: 2006-12-31 to 2021-11-30\n",
      "  Assets with data: 255\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00916 | Turnover: 1.6564 | TC: 0.001641 | Net: -0.01080\n",
      "  MV   Gross: -0.02145 | Turnover: 1.7242 | TC: 0.001687 | Net: -0.02314\n",
      "  MSR   Gross: -0.12913 | Turnover: 3.0622 | TC: 0.002667 | Net: -0.13179\n",
      "\n",
      "[25/59] Date: 2022-01-31 | Year: 2022\n",
      "  Window: 2007-01-31 to 2021-12-31\n",
      "  Assets with data: 256\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06200 | Turnover: 0.9201 | TC: 0.000863 | Net: -0.06286\n",
      "  MV   Gross: -0.06414 | Turnover: 0.9698 | TC: 0.000908 | Net: -0.06505\n",
      "  MSR   Gross: -0.07900 | Turnover: 2.1339 | TC: 0.001965 | Net: -0.08096\n",
      "\n",
      "[26/59] Date: 2022-02-28 | Year: 2022\n",
      "  Window: 2007-02-28 to 2022-01-31\n",
      "  Assets with data: 255\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.02180 | Turnover: 2.2636 | TC: 0.002313 | Net:  0.01949\n",
      "  MV   Gross:  0.02528 | Turnover: 2.6322 | TC: 0.002699 | Net:  0.02258\n",
      "  MSR   Gross:  0.04870 | Turnover: 5.3729 | TC: 0.005635 | Net:  0.04306\n",
      "\n",
      "[27/59] Date: 2022-03-31 | Year: 2022\n",
      "  Window: 2007-03-31 to 2022-02-28\n",
      "  Assets with data: 257\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.01652 | Turnover: 3.0396 | TC: 0.003090 | Net:  0.01343\n",
      "  MV   Gross:  0.01001 | Turnover: 3.3090 | TC: 0.003342 | Net:  0.00667\n",
      "  MSR   Gross: -0.03297 | Turnover: 5.4072 | TC: 0.005229 | Net: -0.03820\n",
      "\n",
      "[28/59] Date: 2022-04-30 | Year: 2022\n",
      "  Window: 2007-04-30 to 2022-03-31\n",
      "  Assets with data: 259\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.01286 | Turnover: 1.2662 | TC: 0.001282 | Net:  0.01158\n",
      "  MV   Gross:  0.00502 | Turnover: 1.3699 | TC: 0.001377 | Net:  0.00364\n",
      "  MSR   Gross: -0.03812 | Turnover: 2.5747 | TC: 0.002477 | Net: -0.04060\n",
      "\n",
      "[29/59] Date: 2022-05-31 | Year: 2022\n",
      "  Window: 2007-05-31 to 2022-04-30\n",
      "  Assets with data: 259\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.01962 | Turnover: 0.5529 | TC: 0.000542 | Net: -0.02017\n",
      "  MV   Gross: -0.01712 | Turnover: 0.5822 | TC: 0.000572 | Net: -0.01769\n",
      "  MSR   Gross: -0.00241 | Turnover: 1.6054 | TC: 0.001602 | Net: -0.00401\n",
      "\n",
      "[30/59] Date: 2022-06-30 | Year: 2022\n",
      "  Window: 2007-06-30 to 2022-05-31\n",
      "  Assets with data: 259\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.01992 | Turnover: 0.7960 | TC: 0.000780 | Net: -0.02070\n",
      "  MV   Gross: -0.01538 | Turnover: 0.8089 | TC: 0.000796 | Net: -0.01617\n",
      "  MSR   Gross:  0.01304 | Turnover: 1.1223 | TC: 0.001137 | Net:  0.01190\n",
      "\n",
      "[31/59] Date: 2022-07-31 | Year: 2022\n",
      "  Window: 2007-07-31 to 2022-06-30\n",
      "  Assets with data: 260\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.04757 | Turnover: 0.9326 | TC: 0.000888 | Net: -0.04846\n",
      "  MV   Gross: -0.04549 | Turnover: 0.9514 | TC: 0.000908 | Net: -0.04640\n",
      "  MSR   Gross: -0.03293 | Turnover: 1.6819 | TC: 0.001627 | Net: -0.03456\n",
      "\n",
      "[32/59] Date: 2022-08-31 | Year: 2022\n",
      "  Window: 2007-08-31 to 2022-07-31\n",
      "  Assets with data: 261\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05064 | Turnover: 0.7699 | TC: 0.000731 | Net: -0.05137\n",
      "  MV   Gross: -0.03783 | Turnover: 0.7287 | TC: 0.000701 | Net: -0.03853\n",
      "  MSR   Gross:  0.03198 | Turnover: 1.2038 | TC: 0.001242 | Net:  0.03073\n",
      "\n",
      "[33/59] Date: 2022-09-30 | Year: 2022\n",
      "  Window: 2007-09-30 to 2022-08-31\n",
      "  Assets with data: 262\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.14120 | Turnover: 1.1566 | TC: 0.001320 | Net:  0.13988\n",
      "  MV   Gross:  0.14552 | Turnover: 1.2773 | TC: 0.001463 | Net:  0.14406\n",
      "  MSR   Gross:  0.17033 | Turnover: 3.0833 | TC: 0.003608 | Net:  0.16672\n",
      "\n",
      "[34/59] Date: 2022-10-31 | Year: 2022\n",
      "  Window: 2007-10-31 to 2022-09-30\n",
      "  Assets with data: 265\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.02078 | Turnover: 1.3102 | TC: 0.001337 | Net:  0.01944\n",
      "  MV   Gross:  0.01684 | Turnover: 1.3854 | TC: 0.001409 | Net:  0.01543\n",
      "  MSR   Gross: -0.00951 | Turnover: 2.2455 | TC: 0.002224 | Net: -0.01173\n",
      "\n",
      "[35/59] Date: 2022-11-30 | Year: 2022\n",
      "  Window: 2007-11-30 to 2022-10-31\n",
      "  Assets with data: 266\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.00941 | Turnover: 1.2391 | TC: 0.001251 | Net:  0.00815\n",
      "  MV   Gross:  0.01002 | Turnover: 1.2799 | TC: 0.001293 | Net:  0.00873\n",
      "  MSR   Gross:  0.01371 | Turnover: 2.1980 | TC: 0.002228 | Net:  0.01148\n",
      "\n",
      "[36/59] Date: 2022-12-31 | Year: 2022\n",
      "  Window: 2007-12-31 to 2022-11-30\n",
      "  Assets with data: 266\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.09179 | Turnover: 0.7179 | TC: 0.000652 | Net: -0.09244\n",
      "  MV   Gross: -0.10721 | Turnover: 0.7374 | TC: 0.000658 | Net: -0.10787\n",
      "  MSR   Gross: -0.19938 | Turnover: 1.2960 | TC: 0.001038 | Net: -0.20041\n",
      "\n",
      "[37/59] Date: 2023-01-31 | Year: 2023\n",
      "  Window: 2008-01-31 to 2022-12-31\n",
      "  Assets with data: 265\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05355 | Turnover: 1.7825 | TC: 0.001687 | Net: -0.05524\n",
      "  MV   Gross: -0.04731 | Turnover: 1.9401 | TC: 0.001848 | Net: -0.04916\n",
      "  MSR   Gross: -0.01045 | Turnover: 3.4990 | TC: 0.003462 | Net: -0.01391\n",
      "\n",
      "[38/59] Date: 2023-02-28 | Year: 2023\n",
      "  Window: 2008-02-29 to 2023-01-31\n",
      "  Assets with data: 265\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.09156 | Turnover: 0.7636 | TC: 0.000834 | Net:  0.09072\n",
      "  MV   Gross:  0.09934 | Turnover: 0.8647 | TC: 0.000951 | Net:  0.09839\n",
      "  MSR   Gross:  0.14932 | Turnover: 1.8214 | TC: 0.002093 | Net:  0.14722\n",
      "\n",
      "[39/59] Date: 2023-03-31 | Year: 2023\n",
      "  Window: 2008-03-31 to 2023-02-28\n",
      "  Assets with data: 265\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.05198 | Turnover: 1.4858 | TC: 0.001563 | Net:  0.05042\n",
      "  MV   Gross:  0.05849 | Turnover: 1.5267 | TC: 0.001616 | Net:  0.05688\n",
      "  MSR   Gross:  0.11661 | Turnover: 2.7192 | TC: 0.003036 | Net:  0.11357\n",
      "\n",
      "[40/59] Date: 2023-04-30 | Year: 2023\n",
      "  Window: 2008-04-30 to 2023-03-31\n",
      "  Assets with data: 266\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.08648 | Turnover: 1.3706 | TC: 0.001252 | Net: -0.08773\n",
      "  MV   Gross: -0.07916 | Turnover: 1.4284 | TC: 0.001315 | Net: -0.08048\n",
      "  MSR   Gross: -0.01144 | Turnover: 2.0033 | TC: 0.001980 | Net: -0.01342\n",
      "\n",
      "[41/59] Date: 2023-05-31 | Year: 2023\n",
      "  Window: 2008-05-31 to 2023-04-30\n",
      "  Assets with data: 266\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.01935 | Turnover: 1.0897 | TC: 0.001111 | Net:  0.01824\n",
      "  MV   Gross:  0.01885 | Turnover: 1.1356 | TC: 0.001157 | Net:  0.01769\n",
      "  MSR   Gross:  0.01384 | Turnover: 1.6120 | TC: 0.001634 | Net:  0.01221\n",
      "\n",
      "[42/59] Date: 2023-06-30 | Year: 2023\n",
      "  Window: 2008-06-30 to 2023-05-31\n",
      "  Assets with data: 268\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.03698 | Turnover: 1.1717 | TC: 0.001128 | Net: -0.03811\n",
      "  MV   Gross: -0.04211 | Turnover: 1.1779 | TC: 0.001128 | Net: -0.04324\n",
      "  MSR   Gross: -0.09675 | Turnover: 1.7282 | TC: 0.001561 | Net: -0.09831\n",
      "\n",
      "[43/59] Date: 2023-07-31 | Year: 2023\n",
      "  Window: 2008-07-31 to 2023-06-30\n",
      "  Assets with data: 270\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.04455 | Turnover: 1.3955 | TC: 0.001458 | Net:  0.04309\n",
      "  MV   Gross:  0.04971 | Turnover: 1.3965 | TC: 0.001466 | Net:  0.04825\n",
      "  MSR   Gross:  0.11555 | Turnover: 2.3690 | TC: 0.002643 | Net:  0.11291\n",
      "\n",
      "[44/59] Date: 2023-08-31 | Year: 2023\n",
      "  Window: 2008-08-31 to 2023-07-31\n",
      "  Assets with data: 272\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.06485 | Turnover: 0.9164 | TC: 0.000857 | Net: -0.06571\n",
      "  MV   Gross: -0.06472 | Turnover: 0.9322 | TC: 0.000872 | Net: -0.06559\n",
      "  MSR   Gross: -0.06246 | Turnover: 1.1025 | TC: 0.001034 | Net: -0.06350\n",
      "\n",
      "[45/59] Date: 2023-09-30 | Year: 2023\n",
      "  Window: 2008-09-30 to 2023-08-31\n",
      "  Assets with data: 275\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.05685 | Turnover: 2.0007 | TC: 0.002114 | Net:  0.05474\n",
      "  MV   Gross:  0.06206 | Turnover: 1.9639 | TC: 0.002086 | Net:  0.05997\n",
      "  MSR   Gross:  0.11972 | Turnover: 2.6419 | TC: 0.002958 | Net:  0.11676\n",
      "\n",
      "[46/59] Date: 2023-10-31 | Year: 2023\n",
      "  Window: 2008-10-31 to 2023-09-30\n",
      "  Assets with data: 277\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00250 | Turnover: 4.5514 | TC: 0.004540 | Net: -0.00704\n",
      "  MV   Gross: -0.00536 | Turnover: 4.7927 | TC: 0.004767 | Net: -0.01012\n",
      "  MSR   Gross: -0.04133 | Turnover: 8.6697 | TC: 0.008311 | Net: -0.04964\n",
      "\n",
      "[47/59] Date: 2023-11-30 | Year: 2023\n",
      "  Window: 2008-11-30 to 2023-10-31\n",
      "  Assets with data: 280\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.03247 | Turnover: 2.8334 | TC: 0.002741 | Net: -0.03521\n",
      "  MV   Gross: -0.03380 | Turnover: 2.9976 | TC: 0.002896 | Net: -0.03669\n",
      "  MSR   Gross: -0.05921 | Turnover: 5.2254 | TC: 0.004916 | Net: -0.06412\n",
      "\n",
      "[48/59] Date: 2023-12-31 | Year: 2023\n",
      "  Window: 2008-12-31 to 2023-11-30\n",
      "  Assets with data: 280\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00647 | Turnover: 3.1032 | TC: 0.003083 | Net: -0.00956\n",
      "  MV   Gross: -0.00355 | Turnover: 3.1430 | TC: 0.003132 | Net: -0.00668\n",
      "  MSR   Gross:  0.04604 | Turnover: 4.6542 | TC: 0.004869 | Net:  0.04117\n",
      "\n",
      "[49/59] Date: 2024-01-31 | Year: 2024\n",
      "  Window: 2009-01-31 to 2023-12-31\n",
      "  Assets with data: 282\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.00778 | Turnover: 1.4925 | TC: 0.001504 | Net:  0.00627\n",
      "  MV   Gross:  0.01215 | Turnover: 1.5470 | TC: 0.001566 | Net:  0.01058\n",
      "  MSR   Gross:  0.08975 | Turnover: 2.7441 | TC: 0.002990 | Net:  0.08676\n",
      "\n",
      "[50/59] Date: 2024-02-29 | Year: 2024\n",
      "  Window: 2009-02-28 to 2024-01-31\n",
      "  Assets with data: 282\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.07020 | Turnover: 3.1820 | TC: 0.003405 | Net:  0.06679\n",
      "  MV   Gross:  0.06864 | Turnover: 3.4405 | TC: 0.003677 | Net:  0.06496\n",
      "  MSR   Gross:  0.04780 | Turnover: 7.2072 | TC: 0.007552 | Net:  0.04024\n",
      "\n",
      "[51/59] Date: 2024-03-31 | Year: 2024\n",
      "  Window: 2009-03-31 to 2024-02-29\n",
      "  Assets with data: 284\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.00344 | Turnover: 2.1900 | TC: 0.002198 | Net:  0.00124\n",
      "  MV   Gross:  0.00324 | Turnover: 2.3352 | TC: 0.002343 | Net:  0.00090\n",
      "  MSR   Gross: -0.00317 | Turnover: 4.3756 | TC: 0.004362 | Net: -0.00753\n",
      "\n",
      "[52/59] Date: 2024-04-30 | Year: 2024\n",
      "  Window: 2009-04-30 to 2024-03-31\n",
      "  Assets with data: 282\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.05037 | Turnover: 7.5995 | TC: 0.007217 | Net: -0.05759\n",
      "  MV   Gross: -0.05389 | Turnover: 8.1452 | TC: 0.007706 | Net: -0.06160\n",
      "  MSR   Gross: -0.10382 | Turnover: 17.5048 | TC: 0.015687 | Net: -0.11951\n",
      "\n",
      "[53/59] Date: 2024-05-31 | Year: 2024\n",
      "  Window: 2009-05-31 to 2024-04-30\n",
      "  Assets with data: 281\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.00233 | Turnover: 4.0333 | TC: 0.004043 | Net: -0.00172\n",
      "  MV   Gross:  0.00812 | Turnover: 4.2850 | TC: 0.004320 | Net:  0.00380\n",
      "  MSR   Gross:  0.09930 | Turnover: 10.0668 | TC: 0.011066 | Net:  0.08823\n",
      "\n",
      "[54/59] Date: 2024-06-30 | Year: 2024\n",
      "  Window: 2009-06-30 to 2024-05-31\n",
      "  Assets with data: 279\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.08550 | Turnover: 2.6842 | TC: 0.002914 | Net:  0.08259\n",
      "  MV   Gross:  0.08573 | Turnover: 2.8118 | TC: 0.003053 | Net:  0.08267\n",
      "  MSR   Gross:  0.09064 | Turnover: 4.1573 | TC: 0.004534 | Net:  0.08611\n",
      "\n",
      "[55/59] Date: 2024-07-31 | Year: 2024\n",
      "  Window: 2009-07-31 to 2024-06-30\n",
      "  Assets with data: 280\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.12861 | Turnover: 3.1103 | TC: 0.003510 | Net:  0.12510\n",
      "  MV   Gross:  0.13091 | Turnover: 3.1342 | TC: 0.003544 | Net:  0.12737\n",
      "  MSR   Gross:  0.18440 | Turnover: 4.1414 | TC: 0.004905 | Net:  0.17949\n",
      "\n",
      "[56/59] Date: 2024-08-31 | Year: 2024\n",
      "  Window: 2009-08-31 to 2024-07-31\n",
      "  Assets with data: 281\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.01459 | Turnover: 1.2428 | TC: 0.001225 | Net: -0.01582\n",
      "  MV   Gross: -0.01443 | Turnover: 1.2721 | TC: 0.001254 | Net: -0.01569\n",
      "  MSR   Gross: -0.07226 | Turnover: 1.7006 | TC: 0.001578 | Net: -0.07384\n",
      "\n",
      "[57/59] Date: 2024-09-30 | Year: 2024\n",
      "  Window: 2009-09-30 to 2024-08-31\n",
      "  Assets with data: 281\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.03333 | Turnover: 1.0247 | TC: 0.000991 | Net: -0.03432\n",
      "  MV   Gross: -0.03334 | Turnover: 1.0239 | TC: 0.000990 | Net: -0.03433\n",
      "  MSR   Gross: -0.01825 | Turnover: 2.2704 | TC: 0.002229 | Net: -0.02048\n",
      "\n",
      "[58/59] Date: 2024-10-31 | Year: 2024\n",
      "  Window: 2009-10-31 to 2024-09-30\n",
      "  Assets with data: 280\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross:  0.05941 | Turnover: 2.0774 | TC: 0.002201 | Net:  0.05721\n",
      "  MV   Gross:  0.06095 | Turnover: 2.1100 | TC: 0.002239 | Net:  0.05871\n",
      "  MSR   Gross:  0.15528 | Turnover: 3.3624 | TC: 0.003885 | Net:  0.15140\n",
      "\n",
      "[59/59] Date: 2024-11-30 | Year: 2024\n",
      "  Window: 2009-11-30 to 2024-10-31\n",
      "  Assets with data: 280\n",
      "  Running Deep Learning Regression...\n",
      "2.5\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV   Gross: -0.00906 | Turnover: 1.6228 | TC: 0.001608 | Net: -0.01067\n",
      "  MV   Gross: -0.01250 | Turnover: 1.6865 | TC: 0.001665 | Net: -0.01417\n",
      "  MSR   Gross: -0.06514 | Turnover: 5.0225 | TC: 0.004695 | Net: -0.06983\n",
      "\n",
      "============================================================\n",
      "BACKTEST COMPLETE\n",
      "============================================================\n",
      "\n",
      " GMV\n",
      "\n",
      "Sharpe Ratio: 0.0161\n",
      "Annualized Sharpe Ratio: 0.0558\n",
      "Total Return: -0.1353\n",
      "Average Turnover: 4.1853\n",
      "\n",
      " MV\n",
      "\n",
      "Sharpe Ratio: 0.0376\n",
      "Annualized Sharpe Ratio: 0.1304\n",
      "Total Return: -0.0474\n",
      "Average Turnover: 4.5880\n",
      "\n",
      " MSR\n",
      "\n",
      "Sharpe Ratio: -0.1139\n",
      "Annualized Sharpe Ratio: -0.3945\n",
      "Total Return: -2.6032\n",
      "Average Turnover: 8.7793\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('green_cleaned.csv', dtype={'ncusip': 'string'})\n",
    "df['ret_fwd_1'] = df.groupby('permno')['ret_excess'].shift(-1)\n",
    "\n",
    "data_f=pd.read_csv('F-F_Research_Data_Factors.csv',sep=',')\n",
    "data_f['Date']=pd.to_datetime(data_f['Date'], format=\"%Y%m\")\n",
    "data_f['Date']=data_f['Date']+pd.offsets.MonthEnd(0)\n",
    "data_f = data_f.set_index('Date')\n",
    "data_f = data_f[['Mkt-RF', 'SMB', 'HML', 'RF']].astype(float)\n",
    "\n",
    "# Run backtest with yearly signals\n",
    "results_df, metrics, results_df_2, metrics_2, results_df_3, metrics_3= backtest_dnn_yearly(\n",
    "    df,\n",
    "    test_start_date='2020-01-31',\n",
    "    test_end_date='2024-11-30',\n",
    "    lookback_window=180,\n",
    "    transaction_cost=0.001,\n",
    "    buys_path_template='buys_{}.csv',\n",
    "    sells_path_template='sells_{}.csv',\n",
    "    data_factor=data_f,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n GMV\")\n",
    "print(f\"\\nSharpe Ratio: {metrics['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MV\")\n",
    "print(f\"\\nSharpe Ratio: {metrics_2['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics_2['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics_2['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics_2['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MSR\")\n",
    "print(f\"\\nSharpe Ratio: {metrics_3['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics_3['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics_3['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics_3['avg_turnover']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb0ee69e-c4ba-4da2-b774-fb00a30784ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{34817: -0.402593100497543, 20482: 0.8091213419402481, 49154: -0.20820275354458814, 52230: -0.29324742641067636, 71175: 0.18209502341278883, 48653: -0.3117372606807958, 60943: -0.5120515435032617, 49680: 1.126107295359313, 34833: -0.24701978025830462, 59408: -0.02321867331464258, 87055: -0.584118445186186, 72726: -0.11083317700349198, 60442: -0.36968525198428104, 23579: 0.2808175868827555, 13856: 0.4869767306265857, 43553: -0.5816057669376756, 76841: -0.04336168871497144, 11308: 0.7815297855191278, 24109: 0.2589592252352374, 19502: -0.38553005942837315, 66093: -0.010011515766993075, 57904: -0.10934803358725118, 75825: 0.130369019163279, 75828: 0.07649326335650462, 86580: -0.09110541976769447, 44601: 0.524933211760182, 60986: -0.15414979231651768, 28222: 0.6973671316956556, 22592: 0.3630667406548882, 39490: 0.037842463485983004, 24643: -0.3253198079998979, 59459: -0.8016390323922797, 21573: 0.46545539647325646, 17478: -0.2861752770158962, 78916: 0.48977911294684157, 11850: 0.06288898539605689, 13901: 0.7390126492296473, 46674: 0.20399353335838333, 48725: 0.13674941020437825, 26710: 0.5828191672070414, 22103: 0.35038859520797294, 88661: -0.20258077976956615, 60506: 0.027389748029494537, 40539: 1.018878289009746, 89179: -0.1100601761692249, 22111: -2.027015665509881, 87137: -0.23639958479480144, 44644: -0.37704468916296907, 13928: -0.677878439541014, 19561: -0.858737978337706, 89195: 0.4950031956692331, 23660: 0.6448488639559494, 17005: -0.4998820757877433, 18542: -0.23417431547537446, 66157: -0.3270368616630058, 36468: -0.734532930613725, 86136: 0.3442262486650872, 41080: -0.5360855079393807, 85631: -0.13369349608759723, 78975: 0.06450378211798433, 59010: -0.4907260306449693, 81540: 0.09911583889350427, 66181: 0.376311549915329, 11404: 0.30963757317001755, 24205: 0.24380460596949036, 62092: -0.14307530217988804, 77462: -0.43985287621110636, 81055: -0.1728733455879998, 23712: 0.16907715513736427, 84129: -0.22121157694344037, 81061: 0.05253494746083103, 82598: -0.6013575314876044, 55976: 1.0495765659160263, 89258: 0.6379995886716642, 11955: -0.36438696888825617, 27828: -0.31502906527654384, 61621: 0.19168532566343413, 45751: -0.026526306682327583, 14008: 0.20100922575562155, 25785: -0.022297415051148926, 21178: -0.4857028869073801, 60599: 0.4342156969027353, 50876: 0.4249010210764314, 23229: 0.07455100822132961, 24766: -0.45745894048465024, 64186: -0.1858131508069272, 79547: -0.4511292400926371, 60097: -0.487116878249955, 21186: 0.3513989576908956, 62148: -0.4200451877145428, 26825: -0.17223101101547741, 12490: -0.09567549785120298, 14541: 0.2128002694950886, 37584: 0.41939241423083173, 80080: 0.22227853676265452, 60628: 0.4880827113537808, 21207: 0.19515506138957717, 16600: 0.48504322865364297, 42200: 0.07733104199933438, 39642: 0.49701745242301093, 15579: -0.188777389495856, 82651: -0.17175545673548245, 15069: -0.3676113080176311, 49373: 0.3057216773369659, 22752: 0.491713232896398, 57568: 0.4318255673739548, 35554: 0.040377770040726936, 35044: -0.020425209039727044, 80100: 0.1968856911795898, 25320: 0.418616983225162, 27887: 0.5981654378929687, 34032: 0.5111158601502795, 66800: -0.03155410330508061, 52978: -0.4303414923075332, 18163: 0.4817235249670158, 75510: 0.10147443991200086, 81655: 0.05723374252102865, 17144: 0.9125203796549973, 29946: -0.5125993638578733, 22779: -0.8324987993707098, 52476: -0.12660977419173974, 56573: 0.5241674949414373, 82686: -0.5534290713482759, 14593: 0.012915651482749304, 23819: -0.0986121940349899, 38156: -0.2819908064108626, 21776: 0.2693478703271204, 10516: 0.45163118677581043, 12052: 0.4721161856208759, 22293: -0.2940696815023572, 85269: -0.3425136239668093, 47896: -0.6064646300693811, 64282: -0.657803825440315, 75034: 0.03145786542202953, 12060: -0.7427454965828119, 12062: 0.026012118436673906, 21792: 0.11227514185531194, 87842: 0.778147368023191, 26403: 0.04310227674513783, 77605: -0.2681608637970826, 16678: 0.4912059857628121, 46886: 0.1925791753807742, 59176: 0.7202040389876625, 18729: -0.5281220370430654, 61735: -0.24759508577116193, 77606: 0.2340394603459393, 76076: -0.060399271381097776, 60206: 0.5133254768865151, 38703: -0.5077967040370328, 27959: 0.35941162919317904, 64311: -0.3748364962225252, 58683: -0.4052774437020726, 76605: 0.5805898664929274, 57665: -0.14829660372916773, 28484: 0.16947247900300788, 52038: -0.4251135919790401, 80711: -0.12404827072141342, 53065: 0.29515686205075325, 25419: -0.051419790601323116, 27983: -0.158428215760986, 40272: -1.0983550160994295, 65875: 0.7159944953653491, 86356: -0.6415050294257858, 86868: -0.06277919113669317, 17750: 0.722350196893815, 27991: 0.05957268903728927, 43350: 0.4259208246708717, 75607: -0.22733763764027687, 82775: 0.3101633035675627, 75100: -0.19483718146816195, 25953: 0.9713469775399787, 15202: -0.5989689628471474, 62308: 1.0341860016844593, 85348: -0.9801900599128193, 15720: 0.08140782608780432, 38762: 0.13494704176008207, 53613: 0.08704783025718435, 14702: -0.7777607545582615, 24942: -0.043337210127261445, 59248: 0.016794290350407547, 81774: -0.06756941688927291, 49015: 0.32829933255310567, 10104: 0.3718681793181648, 70519: 0.1890688759585653, 48506: -0.20062872372690196, 21371: -0.4512023958287103, 23931: 0.109142136528671, 10107: -0.3269157304935781, 52090: 0.5549082729058317, 77178: -0.6544332916286291, 54148: -0.3290630457624697, 58246: -0.5663926106962737, 64390: -1.0788251572279435, 77702: -0.948341062279212, 87432: 0.2739754588651489, 41355: 0.11132171695618653, 71563: -0.038485730343002816, 70033: -0.03986764726802936, 75154: -0.13940954736031438, 84373: -0.2617789013614758, 19350: -0.8281808392304186, 83862: 0.26782499735545706, 87447: 0.0981033677630117, 24985: -0.0954395028542303, 11674: -0.8449967298521818, 10138: 0.5902743803462311, 42906: -0.2710728959155252, 84381: -0.10333524522226566, 85913: 0.7015950916558661, 85914: -0.10333280332120484, 10145: 1.947303847857275, 17830: 1.4035214540979988, 85926: -0.3773506579046498, 64936: 0.360401641674007, 69032: -0.45831436360418526, 76201: -0.5394345383344912, 76712: -0.6867340654718144, 69550: -0.1219736613344928, 21936: -0.5802454486463544, 23473: -0.5368370476391431, 70578: 0.10528044921714437, 73139: -0.13413140844853083, 75186: -0.2333188549946509, 89525: -0.6023076150351622, 43449: 0.7720213875846911, 34746: -0.1228014975397437, 59328: -0.4164046646750576, 19393: 0.4467998703738945, 76226: -0.03683341604830098, 14277: 0.058122433808793394, 60871: -0.2428288656566846, 10696: 0.24511088926038763, 24010: 0.1632195114181774, 56274: -0.14287412432099367, 52695: -0.8055859729527998, 61399: -0.46386457480424076, 30681: 0.38782693193565326, 57817: 0.08090688224136229, 77274: 0.17217100429759857, 79323: 0.8244299560241324, 64995: 0.520143067105079, 18411: 0.5947559708258185, 22509: 1.09550445693585, 39917: 0.2462504276038069, 89070: -0.6893685202291936, 92655: -0.23604493284794698, 11762: -0.21289865861705967, 23026: 0.23335826229703482, 46578: 0.970396420716949, 22517: 0.4827583554888441, 49656: 0.22978164136299042, 25081: -0.3828688345039997}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    print(results_df['portfolio_weights'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bf25e0-0334-4df8-886c-e976651a4fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13525893920038667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['total_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f85f8bc-1075-4e8a-8119-2d19a30fc8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008632068888745495"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df58028-b81e-4987-92a9-52cb828f89f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.18531803247102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['portfolio_turnover'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

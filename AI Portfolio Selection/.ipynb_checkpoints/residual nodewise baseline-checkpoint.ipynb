{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247ce510-1800-493b-a43e-03914379a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def est_ndwcov_factor(Y, factors, ic, lambda_min=True):\n",
    "    \"\"\"\n",
    "    Estimate nodewise covariance with factor models using LASSO.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Y : numpy.ndarray\n",
    "        n x p matrix of observations\n",
    "    factors : numpy.ndarray\n",
    "        n x k matrix of factors\n",
    "    ic : str\n",
    "        Information criterion: 'WIC', 'BIC', 'GIC', 'AIC', or 'cv'\n",
    "    lambda_min : bool\n",
    "        If True and ic='cv', use lambda.min; otherwise use lambda.1se\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    TAU : numpy.ndarray\n",
    "        p x p precision matrix estimate\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    p = Y.shape[1]\n",
    "    n = Y.shape[0]\n",
    "    C = np.zeros((p, p))\n",
    "    np.fill_diagonal(C, 1)\n",
    "    tau = []\n",
    "    ns1 = np.ones((n, 1))\n",
    "    \n",
    "    # Fit factor model: Y = factors * beta + u\n",
    "    # Add intercept to factors\n",
    "    factors_with_intercept = np.column_stack([np.ones(n), factors])\n",
    "    \n",
    "    # Fit linear regression for each column of Y\n",
    "    factormodel = LinearRegression(fit_intercept=False)\n",
    "    factormodel.fit(factors_with_intercept, Y)\n",
    "    \n",
    "    # Get residuals and beta coefficients (excluding intercept)\n",
    "    u = Y - factormodel.predict(factors_with_intercept)\n",
    "    beta = factormodel.coef_[:, 1:]  # p x k matrix (excluding intercept)\n",
    "    \n",
    "    # Loop over the assets\n",
    "    for j in range(p):\n",
    "        # Create design matrix excluding column j\n",
    "        X_j = np.delete(u, j, axis=1)\n",
    "        y_j = u[:, j]\n",
    "        \n",
    "        if ic != 'cv':\n",
    "            # Fit LASSO path\n",
    "            alphas = np.logspace(-4, 1, 100)  # Create lambda sequence\n",
    "            df_list = []\n",
    "            sig_list = []\n",
    "            bic_list = []\n",
    "            coef_list = []\n",
    "            res_list = []\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                model = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n",
    "                model.fit(X_j, y_j)\n",
    "                \n",
    "                # Predictions and residuals\n",
    "                y_pred = model.predict(X_j)\n",
    "                res = y_j - y_pred\n",
    "                \n",
    "                # Degrees of freedom (number of non-zero coefficients)\n",
    "                df = np.sum(np.abs(model.coef_) > 1e-8)\n",
    "                \n",
    "                # Variance of residuals\n",
    "                sig = np.sum(res**2) / n\n",
    "                \n",
    "                # Compute information criterion\n",
    "                if ic == 'WIC':\n",
    "                    bic_val = np.log(sig) + df * np.log(n) / n * np.log(np.log(p))\n",
    "                elif ic == 'BIC':\n",
    "                    bic_val = np.log(sig) + df * np.log(n) / n\n",
    "                elif ic == 'GIC':\n",
    "                    bic_val = np.log(sig) + df * np.log(p) * np.log(np.log(n)) / n\n",
    "                elif ic == 'AIC':\n",
    "                    bic_val = np.log(sig) + 2 * df\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown IC: {ic}\")\n",
    "                \n",
    "                df_list.append(df)\n",
    "                sig_list.append(sig)\n",
    "                bic_list.append(bic_val)\n",
    "                coef_list.append(model.coef_.copy())\n",
    "                res_list.append(res)\n",
    "            \n",
    "            # Select model with minimum IC\n",
    "            jind = np.argmin(bic_list)\n",
    "            jpar = coef_list[jind]\n",
    "            jres = res_list[jind]\n",
    "            jtau = np.sum(y_j * jres) / n\n",
    "            \n",
    "        else:  # Cross-validation\n",
    "            lasso_cv = LassoCV(cv=5, fit_intercept=False, max_iter=10000, n_alphas=100)\n",
    "            lasso_cv.fit(X_j, y_j)\n",
    "            \n",
    "            if lambda_min:\n",
    "                # Use alpha that minimizes CV error (lambda.min equivalent)\n",
    "                jfit = lasso_cv.predict(X_j)\n",
    "                jpar = lasso_cv.coef_\n",
    "            else:\n",
    "                # Use alpha within 1 SE of minimum (lambda.1se equivalent)\n",
    "                cv_scores = lasso_cv.mse_path_.mean(axis=1)\n",
    "                cv_std = lasso_cv.mse_path_.std(axis=1)\n",
    "                min_idx = np.argmin(cv_scores)\n",
    "                threshold = cv_scores[min_idx] + cv_std[min_idx]\n",
    "                \n",
    "                # Find largest alpha with CV score below threshold\n",
    "                valid_indices = np.where(cv_scores <= threshold)[0]\n",
    "                se_idx = valid_indices[0] if len(valid_indices) > 0 else min_idx\n",
    "                \n",
    "                selected_alpha = lasso_cv.alphas_[se_idx]\n",
    "                model_1se = Lasso(alpha=selected_alpha, fit_intercept=False, max_iter=10000)\n",
    "                model_1se.fit(X_j, y_j)\n",
    "                jfit = model_1se.predict(X_j)\n",
    "                jpar = model_1se.coef_\n",
    "            \n",
    "            jres = y_j - jfit\n",
    "            jtau = np.sum(y_j * jres) / n\n",
    "        \n",
    "        # Fill in C matrix\n",
    "        # Insert coefficients back (accounting for missing j-th position)\n",
    "        C_row = np.insert(-jpar / jtau, j, 0)\n",
    "        C[j, :] = C_row\n",
    "        tau.append(jtau)\n",
    "    \n",
    "    # Set diagonal\n",
    "    np.fill_diagonal(C, 1 / np.array(tau))\n",
    "    omega = C.copy()\n",
    "    omegasym = (C + C.T) / 2\n",
    "    \n",
    "    # Compute factor covariance - ensure float64\n",
    "    covft = (1/n) * (factors.T @ factors) - (1/(n**2)) * (factors.T @ ns1 @ ns1.T @ factors)\n",
    "    covft = covft.astype(np.float64)\n",
    "    \n",
    "    # Ensure beta and omegasym are float64\n",
    "    beta = beta.astype(np.float64)\n",
    "    omegasym = omegasym.astype(np.float64)\n",
    "    \n",
    "    # Compute TAU\n",
    "    if factors.shape[1] == 1:\n",
    "        covft_inv = 1.0 / float(covft[0, 0])\n",
    "        p1 = 1.0 / (covft_inv + beta.T @ omegasym @ beta)\n",
    "        TAU = omega - omega @ beta @ p1 @ beta.T @ omega\n",
    "    else:\n",
    "        covft_inv = np.linalg.inv(covft)\n",
    "        p1 = np.linalg.inv(covft_inv + beta.T @ omegasym @ beta)\n",
    "        TAU = omega - omega @ beta @ p1 @ beta.T @ omega\n",
    "    \n",
    "    return TAU\n",
    "\n",
    "def gmv_weights(Theta_hat):\n",
    "    \"\"\"\n",
    "    Compute Global Minimum Variance (GMV) portfolio weights (Section 6.1).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # w* = (Θ 1_p) / (1_p' Θ 1_p)\n",
    "    numerator = Theta_hat @ ones_p\n",
    "    denominator = ones_p @ Theta_hat @ ones_p\n",
    "    \n",
    "    if np.abs(denominator) < 1e-10:\n",
    "        # Fallback to equal weights if precision matrix is near-singular\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = numerator / denominator\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "\n",
    "def compute_portfolio_metrics(returns, weights):\n",
    "    \"\"\"\n",
    "    Compute portfolio return, variance, and Sharpe ratio.\n",
    "    \"\"\"\n",
    "    portfolio_returns = returns @ weights\n",
    "    mean_return = np.mean(portfolio_returns)\n",
    "    variance = np.var(portfolio_returns, ddof=1)\n",
    "    sharpe_ratio = mean_return / np.sqrt(variance) if variance > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'return': mean_return,\n",
    "        'variance': variance,\n",
    "        'sharpe_ratio': sharpe_ratio\n",
    "    }\n",
    "\n",
    "def load_ff_factors(factors_path='factors_ff_monthly_raw.csv'):\n",
    "    \"\"\"\n",
    "    Load Fama-French factors from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    factors_path : str\n",
    "        Path to the factors CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    factors_df : pd.DataFrame\n",
    "        DataFrame with date index and factor columns\n",
    "    \"\"\"\n",
    "    factors_df = pd.read_csv(factors_path)\n",
    "    \n",
    "    # Convert month column (e.g., 192707) to datetime\n",
    "    # This gives us the first day of the month (1927-07-01)\n",
    "    factors_df['date'] = pd.to_datetime(factors_df.iloc[:, 0].astype(str), format='%Y%m')\n",
    "    \n",
    "    # Convert to end of month to match returns data\n",
    "    factors_df['date'] = factors_df['date'] + pd.offsets.MonthEnd(0)\n",
    "    \n",
    "    # Set date as index and keep only factor columns\n",
    "    factors_df = factors_df.set_index('date')[['Mkt-RF', 'SMB', 'HML']]\n",
    "    \n",
    "    # Convert to decimal form (assuming factors are in percentage points)\n",
    "    factors_df = factors_df / 100\n",
    "    \n",
    "    return factors_df\n",
    "\n",
    "\n",
    "def backtest_nodewise_gmv(df, \n",
    "                          factors_path='factors_ff_monthly_raw.csv',\n",
    "                          ic='GIC',\n",
    "                          test_start_date='2000-01-31', \n",
    "                          test_end_date='2003-12-31',\n",
    "                          lookback_window=180,\n",
    "                          transaction_cost=0.005,\n",
    "                          verbose=True):\n",
    "    \"\"\"\n",
    "    Backtest Nodewise + MSR strategy with monthly rebalancing,\n",
    "    180-month rolling window, and NaN filtering as per the paper.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: permno, datadate, ret_fwd_1\n",
    "    test_start_date : str\n",
    "        First date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    test_end_date : str\n",
    "        Last date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    lookback_window : int\n",
    "        Number of months in rolling training window (default: 180)\n",
    "    transaction_cost : float\n",
    "        Proportional transaction cost (default: 0.005 = 50 bps)\n",
    "    verbose : bool\n",
    "        If True, prints detailed log at each time step.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with columns: date, portfolio_return, cumulative_return\n",
    "    metrics : dict\n",
    "        Overall performance metrics\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    df = df.copy()\n",
    "    if 'datadate' not in df.columns or 'permno' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'datadate' and 'permno' columns\")\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'])\n",
    "\n",
    "    # Load Fama-French factors\n",
    "    factors_df = load_ff_factors(factors_path)\n",
    "    \n",
    "    # Get unique dates\n",
    "    all_dates = sorted(df['datadate'].unique())\n",
    "    \n",
    "    # Convert test dates to datetime\n",
    "    test_start_dt = pd.to_datetime(test_start_date)\n",
    "    test_end_dt = pd.to_datetime(test_end_date)\n",
    "    \n",
    "    # Find date indices\n",
    "    try:\n",
    "        test_start_idx = all_dates.index(test_start_dt)\n",
    "        test_end_idx = all_dates.index(test_end_dt)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Date not found in DataFrame: {e}\")\n",
    "    \n",
    "    if test_start_idx < lookback_window:\n",
    "        raise ValueError(f\"Not enough data for lookback. Test start date {test_start_date} \"\n",
    "                         f\"requires data back to {all_dates[test_start_idx - lookback_window]}, \"\n",
    "                         f\"but only {test_start_idx} periods are available.\")\n",
    "    \n",
    "    # Storage for results\n",
    "    portfolio_returns = []\n",
    "    portfolio_dates = []\n",
    "    portfolio_weights_list = []\n",
    "    portfolio_turnover_list = []\n",
    "    portfolio_gross_returns = []\n",
    "    \n",
    "    # Track weights by permno (handles entry/exit)\n",
    "    prev_weights_dict = {}\n",
    "    prev_oos_returns_dict = {}\n",
    "    prev_gross_return = 0.0\n",
    "    \n",
    "    # --- 2. Rolling Window Backtest ---\n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING BACKTEST\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    for t in range(test_start_idx, test_end_idx + 1):\n",
    "        current_date = all_dates[t]\n",
    "        \n",
    "        # Define the lookback window\n",
    "        window_start_date = all_dates[t - lookback_window]\n",
    "        window_end_date = all_dates[t - 1]\n",
    "        \n",
    "        # Get training data for this window\n",
    "        train_data = df[(df['datadate'] >= window_start_date) & \n",
    "                        (df['datadate'] <= window_end_date)]\n",
    "        \n",
    "        # Pivot to get returns matrix (time x assets)\n",
    "        returns_pivot = train_data.pivot(index='datadate', columns='permno', values='ret_fwd_1')\n",
    "        \n",
    "        # Reindex to ensure all dates are present\n",
    "        window_dates = all_dates[t - lookback_window : t]\n",
    "        returns_pivot = returns_pivot.reindex(index=window_dates)\n",
    "\n",
    "        # IMPORTANT: returns_pivot contains 1-month AHEAD returns\n",
    "        # So returns at date t are realized at t+1\n",
    "        # We need factors that correspond to when returns are realized\n",
    "        # Shift window_dates forward by 1 month to align with return realization\n",
    "        factor_dates = [(d + pd.DateOffset(months=1) + pd.offsets.MonthEnd(0)) for d in window_dates]\n",
    "        \n",
    "        # Get factors for the shifted window (when returns are realized)\n",
    "        # Since both dates are end-of-month, they should match exactly\n",
    "        try:\n",
    "            factors_window = factors_df.loc[factor_dates]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(f\"Factor dates not found in factors file. This should not happen \"\n",
    "                           f\"if both returns and factors are end-of-month. Missing dates: {e}\")\n",
    "        \n",
    "        # Check if we have all factors data\n",
    "        if factors_window.isna().any().any():\n",
    "            if verbose:\n",
    "                print(f\"\\n[{t - test_start_idx + 1}/{test_end_idx - test_start_idx + 1}] \"\n",
    "                      f\"Date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "                print(f\"  ⚠ Missing factor data in window, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter assets with any NaNs in this window\n",
    "        nan_assets = returns_pivot.columns[returns_pivot.isna().any()]\n",
    "        filtered_pivot = returns_pivot.drop(columns=nan_assets)\n",
    "        \n",
    "        current_assets = filtered_pivot.columns.tolist()\n",
    "        Y = filtered_pivot.values\n",
    "        factors = factors_window.values\n",
    "        n_train, p_current = Y.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[{t - test_start_idx + 1}/{test_end_idx - test_start_idx + 1}] \"\n",
    "                  f\"Date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Window: {window_start_date.strftime('%Y-%m-%d')} to \"\n",
    "                  f\"{window_end_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Assets: {p_current} with complete data\")\n",
    "\n",
    "        # Check for valid data\n",
    "        if n_train < lookback_window or p_current < 2:\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Insufficient data (n={n_train}, p={p_current}), using prev weights\")\n",
    "            new_weights_dict = prev_weights_dict.copy()\n",
    "        else:\n",
    "            try:\n",
    "                # Demean the returns\n",
    "                Y_bar = Y.mean(axis=0)\n",
    "                Y_star = Y - Y_bar\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Running Nodewise Regression...\")\n",
    "                Theta_hat = est_ndwcov_factor(Y, factors, ic=ic, lambda_min=True)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing GMV weights...\")\n",
    "                w_star = gmv_weights(Theta_hat)\n",
    "                \n",
    "                # Create weights dictionary\n",
    "                new_weights_dict = {asset: w_star[i] for i, asset in enumerate(current_assets)}\n",
    "                \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"  ✗ Error: {e}\")\n",
    "                    print(f\"  Using previous weights\")\n",
    "                new_weights_dict = prev_weights_dict.copy()\n",
    "\n",
    "        # Normalize weights to sum to 1\n",
    "        weight_sum = sum(new_weights_dict.values())\n",
    "        if weight_sum > 1e-10:\n",
    "            new_weights_dict = {k: v/weight_sum for k, v in new_weights_dict.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum, using previous weights\")\n",
    "            new_weights_dict = prev_weights_dict.copy()\n",
    "        \n",
    "        # --- 3. OOS Returns & Transaction Costs ---\n",
    "        \n",
    "        # Get out-of-sample returns for current month\n",
    "        oos_data = df[df['datadate'] == current_date]\n",
    "        oos_returns_series = oos_data.set_index('permno')['ret_fwd_1']\n",
    "        \n",
    "        # Filter out NaN returns\n",
    "        oos_returns_series = oos_returns_series.dropna()\n",
    "        oos_returns_dict = oos_returns_series.to_dict()\n",
    "        \n",
    "        # Find common assets between weights and returns\n",
    "        common_assets = set(new_weights_dict.keys()) & set(oos_returns_dict.keys())\n",
    "        \n",
    "        if len(common_assets) == 0:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ No common assets with valid returns, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to common assets and renormalize\n",
    "        common_weights = {a: new_weights_dict[a] for a in common_assets}\n",
    "        common_weight_sum = sum(common_weights.values())\n",
    "        if common_weight_sum > 1e-10:\n",
    "            common_weights = {k: v/common_weight_sum for k, v in common_weights.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Compute gross portfolio return\n",
    "        gross_return = sum(common_weights[a] * oos_returns_dict[a] for a in common_assets)\n",
    "        \n",
    "        # Sanity check\n",
    "        if np.isnan(gross_return) or np.isinf(gross_return):\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Invalid gross return: {gross_return}, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate transaction costs with proper weight adjustment\n",
    "        if len(prev_weights_dict) > 0:\n",
    "            # Step 1: Adjust ALL previous weights for returns\n",
    "            adjusted_prev = {}\n",
    "            \n",
    "            for asset, prev_w in prev_weights_dict.items():\n",
    "                if asset in prev_oos_returns_dict:\n",
    "                    prev_r = prev_oos_returns_dict[asset]\n",
    "                    if abs(1 + prev_gross_return) > 1e-6:\n",
    "                        adjusted_prev[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return)\n",
    "                    else:\n",
    "                        adjusted_prev[asset] = 0.0\n",
    "                else:\n",
    "                    # Asset had weight but no return data\n",
    "                    if abs(1 + prev_gross_return) > 1e-6:\n",
    "                        adjusted_prev[asset] = prev_w / (1 + prev_gross_return)\n",
    "                    else:\n",
    "                        adjusted_prev[asset] = 0.0\n",
    "            \n",
    "            # Step 2: Calculate turnover\n",
    "            all_assets = set(adjusted_prev.keys()) | set(common_weights.keys())\n",
    "            \n",
    "            turnover = 0.0\n",
    "            for asset in all_assets:\n",
    "                old_w = adjusted_prev.get(asset, 0.0)\n",
    "                new_w = common_weights.get(asset, 0.0)\n",
    "                turnover += abs(new_w - old_w)\n",
    "            \n",
    "            # Transaction cost on end-of-period portfolio value\n",
    "            tc = transaction_cost * (1 + gross_return) * turnover\n",
    "        \n",
    "        else:\n",
    "            # First period: no previous positions, buying into everything\n",
    "            turnover = sum(abs(w) for w in common_weights.values())\n",
    "            tc = transaction_cost * (1 + gross_return) * turnover\n",
    "        \n",
    "        # Net return\n",
    "        net_return = gross_return - tc\n",
    "        \n",
    "        # Store results\n",
    "        portfolio_returns.append(net_return)\n",
    "        portfolio_dates.append(current_date)\n",
    "        portfolio_weights_list.append(common_weights.copy())\n",
    "        portfolio_turnover_list.append(turnover)\n",
    "        portfolio_gross_returns.append(gross_return)\n",
    "        \n",
    "        # Update previous values for next iteration\n",
    "        prev_weights_dict = common_weights.copy()\n",
    "        prev_oos_returns_dict = {a: oos_returns_dict[a] for a in common_assets}\n",
    "        prev_gross_return = gross_return\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Gross: {gross_return:>8.5f} | Turnover: {turnover:>6.4f} | \"\n",
    "                  f\"TC: {tc:>8.6f} | Net: {net_return:>8.5f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BACKTEST COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # --- 4. Compile Results ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'date': portfolio_dates,\n",
    "        'portfolio_return': portfolio_returns,\n",
    "        'portfolio_gross_return': portfolio_gross_returns,\n",
    "        'portfolio_weights': portfolio_weights_list,\n",
    "        'portfolio_turnover': portfolio_turnover_list\n",
    "    })\n",
    "    results_df['cumulative_return'] = (1 + results_df['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    if len(portfolio_returns) > 0:\n",
    "        mean_return = np.mean(portfolio_returns)\n",
    "        variance = np.var(portfolio_returns, ddof=1)\n",
    "        sharpe_ratio = mean_return / np.sqrt(variance) if variance > 0 else 0\n",
    "        \n",
    "        # Annualized metrics (monthly data)\n",
    "        annual_return = mean_return * 12\n",
    "        annual_volatility = np.sqrt(variance * 12)\n",
    "        annual_sharpe = annual_return / annual_volatility if annual_volatility > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'mean_return': mean_return,\n",
    "            'variance': variance,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'annual_return': annual_return,\n",
    "            'annual_volatility': annual_volatility,\n",
    "            'annual_sharpe_ratio': annual_sharpe,\n",
    "            'total_return': results_df['cumulative_return'].iloc[-1],\n",
    "            'avg_turnover': np.mean(portfolio_turnover_list),\n",
    "            'n_periods': len(portfolio_returns)\n",
    "        }\n",
    "    else:\n",
    "        metrics = {\n",
    "            'mean_return': 0,\n",
    "            'variance': 0,\n",
    "            'sharpe_ratio': 0,\n",
    "            'annual_return': 0,\n",
    "            'annual_volatility': 0,\n",
    "            'annual_sharpe_ratio': 0,\n",
    "            'total_return': 0,\n",
    "            'avg_turnover': 0,\n",
    "            'n_periods': 0\n",
    "        }\n",
    "    \n",
    "    return results_df, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd9110d-d90e-4771-8cee-ce7de6ca110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your returns data\n",
    "df = pd.read_csv('../green cleaned.csv', dtype={'ncusip': 'string'})\n",
    "df['ret_fwd_1'] = df.groupby('permno')['ret_excess'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2805fe-384f-4b20-b8d0-726ba256403e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING BACKTEST\n",
      "============================================================\n",
      "\n",
      "[1/52] Date: 2020-01-31\n",
      "  Window: 2005-01-31 to 2019-12-31\n",
      "  Assets: 252 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.09000 | Turnover: 2.3240 | TC: 0.010574 | Net: -0.10058\n",
      "\n",
      "[2/52] Date: 2020-02-29\n",
      "  Window: 2005-02-28 to 2020-01-31\n",
      "  Assets: 252 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.07674 | Turnover: 0.2367 | TC: 0.001093 | Net: -0.07783\n",
      "\n",
      "[3/52] Date: 2020-03-31\n",
      "  Window: 2005-03-31 to 2020-02-29\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.08144 | Turnover: 0.3847 | TC: 0.002080 | Net:  0.07936\n",
      "\n",
      "[4/52] Date: 2020-04-30\n",
      "  Window: 2005-04-30 to 2020-03-31\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02277 | Turnover: 0.4074 | TC: 0.002084 | Net:  0.02069\n",
      "\n",
      "[5/52] Date: 2020-05-31\n",
      "  Window: 2005-05-31 to 2020-04-30\n",
      "  Assets: 250 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.04945 | Turnover: 0.2015 | TC: 0.000958 | Net: -0.05041\n",
      "\n",
      "[6/52] Date: 2020-06-30\n",
      "  Window: 2005-06-30 to 2020-05-31\n",
      "  Assets: 248 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.05536 | Turnover: 0.2713 | TC: 0.001432 | Net:  0.05393\n",
      "\n",
      "[7/52] Date: 2020-07-31\n",
      "  Window: 2005-07-31 to 2020-06-30\n",
      "  Assets: 250 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01509 | Turnover: 0.3761 | TC: 0.001852 | Net: -0.01694\n",
      "\n",
      "[8/52] Date: 2020-08-31\n",
      "  Window: 2005-08-31 to 2020-07-31\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00130 | Turnover: 0.3569 | TC: 0.001787 | Net: -0.00049\n",
      "\n",
      "[9/52] Date: 2020-09-30\n",
      "  Window: 2005-09-30 to 2020-08-31\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01400 | Turnover: 0.1684 | TC: 0.000830 | Net: -0.01483\n",
      "\n",
      "[10/52] Date: 2020-10-31\n",
      "  Window: 2005-10-31 to 2020-09-30\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.01078 | Turnover: 0.2999 | TC: 0.001516 | Net:  0.00926\n",
      "\n",
      "[11/52] Date: 2020-11-30\n",
      "  Window: 2005-11-30 to 2020-10-31\n",
      "  Assets: 252 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00793 | Turnover: 0.4107 | TC: 0.002037 | Net: -0.00997\n",
      "\n",
      "[12/52] Date: 2020-12-31\n",
      "  Window: 2005-12-31 to 2020-11-30\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01365 | Turnover: 0.1660 | TC: 0.000819 | Net: -0.01447\n",
      "\n",
      "[13/52] Date: 2021-01-31\n",
      "  Window: 2006-01-31 to 2020-12-31\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.04550 | Turnover: 0.3204 | TC: 0.001529 | Net: -0.04703\n",
      "\n",
      "[14/52] Date: 2021-02-28\n",
      "  Window: 2006-02-28 to 2021-01-31\n",
      "  Assets: 252 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.09069 | Turnover: 0.2858 | TC: 0.001559 | Net:  0.08914\n",
      "\n",
      "[15/52] Date: 2021-03-31\n",
      "  Window: 2006-03-31 to 2021-02-28\n",
      "  Assets: 251 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02981 | Turnover: 0.2744 | TC: 0.001413 | Net:  0.02840\n",
      "\n",
      "[16/52] Date: 2021-04-30\n",
      "  Window: 2006-04-30 to 2021-03-31\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00410 | Turnover: 0.2721 | TC: 0.001355 | Net: -0.00545\n",
      "\n",
      "[17/52] Date: 2021-05-31\n",
      "  Window: 2006-05-31 to 2021-04-30\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00182 | Turnover: 0.1556 | TC: 0.000776 | Net: -0.00260\n",
      "\n",
      "[18/52] Date: 2021-06-30\n",
      "  Window: 2006-06-30 to 2021-05-31\n",
      "  Assets: 254 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02807 | Turnover: 0.1507 | TC: 0.000775 | Net:  0.02729\n",
      "\n",
      "[19/52] Date: 2021-07-31\n",
      "  Window: 2006-07-31 to 2021-06-30\n",
      "  Assets: 254 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00529 | Turnover: 0.2707 | TC: 0.001361 | Net:  0.00393\n",
      "\n",
      "[20/52] Date: 2021-08-31\n",
      "  Window: 2006-08-31 to 2021-07-31\n",
      "  Assets: 255 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01411 | Turnover: 0.1574 | TC: 0.000776 | Net: -0.01489\n",
      "\n",
      "[21/52] Date: 2021-09-30\n",
      "  Window: 2006-09-30 to 2021-08-31\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02266 | Turnover: 0.2005 | TC: 0.001025 | Net:  0.02164\n",
      "\n",
      "[22/52] Date: 2021-10-31\n",
      "  Window: 2006-10-31 to 2021-09-30\n",
      "  Assets: 253 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00194 | Turnover: 0.3410 | TC: 0.001702 | Net: -0.00364\n",
      "\n",
      "[23/52] Date: 2021-11-30\n",
      "  Window: 2006-11-30 to 2021-10-31\n",
      "  Assets: 257 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.10983 | Turnover: 0.1916 | TC: 0.001063 | Net:  0.10877\n",
      "\n",
      "[24/52] Date: 2021-12-31\n",
      "  Window: 2006-12-31 to 2021-11-30\n",
      "  Assets: 255 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00501 | Turnover: 0.2291 | TC: 0.001151 | Net:  0.00386\n",
      "\n",
      "[25/52] Date: 2022-01-31\n",
      "  Window: 2007-01-31 to 2021-12-31\n",
      "  Assets: 256 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00301 | Turnover: 0.2220 | TC: 0.001107 | Net: -0.00412\n",
      "\n",
      "[26/52] Date: 2022-02-28\n",
      "  Window: 2007-02-28 to 2022-01-31\n",
      "  Assets: 255 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.06750 | Turnover: 0.2326 | TC: 0.001241 | Net:  0.06626\n",
      "\n",
      "[27/52] Date: 2022-03-31\n",
      "  Window: 2007-03-31 to 2022-02-28\n",
      "  Assets: 257 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02286 | Turnover: 0.3122 | TC: 0.001597 | Net:  0.02127\n",
      "\n",
      "[28/52] Date: 2022-04-30\n",
      "  Window: 2007-04-30 to 2022-03-31\n",
      "  Assets: 259 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02038 | Turnover: 0.3120 | TC: 0.001592 | Net:  0.01878\n",
      "\n",
      "[29/52] Date: 2022-05-31\n",
      "  Window: 2007-05-31 to 2022-04-30\n",
      "  Assets: 259 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01403 | Turnover: 0.1864 | TC: 0.000919 | Net: -0.01495\n",
      "\n",
      "[30/52] Date: 2022-06-30\n",
      "  Window: 2007-06-30 to 2022-05-31\n",
      "  Assets: 259 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00557 | Turnover: 0.2891 | TC: 0.001437 | Net: -0.00701\n",
      "\n",
      "[31/52] Date: 2022-07-31\n",
      "  Window: 2007-07-31 to 2022-06-30\n",
      "  Assets: 260 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01305 | Turnover: 0.3016 | TC: 0.001488 | Net: -0.01454\n",
      "\n",
      "[32/52] Date: 2022-08-31\n",
      "  Window: 2007-08-31 to 2022-07-31\n",
      "  Assets: 261 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.04458 | Turnover: 0.1638 | TC: 0.000782 | Net: -0.04536\n",
      "\n",
      "[33/52] Date: 2022-09-30\n",
      "  Window: 2007-09-30 to 2022-08-31\n",
      "  Assets: 262 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.06228 | Turnover: 0.2995 | TC: 0.001591 | Net:  0.06069\n",
      "\n",
      "[34/52] Date: 2022-10-31\n",
      "  Window: 2007-10-31 to 2022-09-30\n",
      "  Assets: 265 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.03002 | Turnover: 0.3796 | TC: 0.001955 | Net:  0.02806\n",
      "\n",
      "[35/52] Date: 2022-11-30\n",
      "  Window: 2007-11-30 to 2022-10-31\n",
      "  Assets: 266 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.01245 | Turnover: 0.3160 | TC: 0.001560 | Net: -0.01401\n",
      "\n",
      "[36/52] Date: 2022-12-31\n",
      "  Window: 2007-12-31 to 2022-11-30\n",
      "  Assets: 266 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.07009 | Turnover: 0.1713 | TC: 0.000796 | Net: -0.07089\n",
      "\n",
      "[37/52] Date: 2023-01-31\n",
      "  Window: 2008-01-31 to 2022-12-31\n",
      "  Assets: 265 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.03282 | Turnover: 0.4270 | TC: 0.002065 | Net: -0.03488\n",
      "\n",
      "[38/52] Date: 2023-02-28\n",
      "  Window: 2008-02-29 to 2023-01-31\n",
      "  Assets: 265 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.03149 | Turnover: 0.1842 | TC: 0.000950 | Net:  0.03054\n",
      "\n",
      "[39/52] Date: 2023-03-31\n",
      "  Window: 2008-03-31 to 2023-02-28\n",
      "  Assets: 265 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.04121 | Turnover: 0.3935 | TC: 0.002048 | Net:  0.03916\n",
      "\n",
      "[40/52] Date: 2023-04-30\n",
      "  Window: 2008-04-30 to 2023-03-31\n",
      "  Assets: 266 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.07724 | Turnover: 0.3334 | TC: 0.001538 | Net: -0.07878\n",
      "\n",
      "[41/52] Date: 2023-05-31\n",
      "  Window: 2008-05-31 to 2023-04-30\n",
      "  Assets: 266 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00259 | Turnover: 0.2316 | TC: 0.001161 | Net:  0.00143\n",
      "\n",
      "[42/52] Date: 2023-06-30\n",
      "  Window: 2008-06-30 to 2023-05-31\n",
      "  Assets: 268 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00330 | Turnover: 0.4830 | TC: 0.002423 | Net:  0.00088\n",
      "\n",
      "[43/52] Date: 2023-07-31\n",
      "  Window: 2008-07-31 to 2023-06-30\n",
      "  Assets: 270 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.03374 | Turnover: 0.3302 | TC: 0.001595 | Net: -0.03533\n",
      "\n",
      "[44/52] Date: 2023-08-31\n",
      "  Window: 2008-08-31 to 2023-07-31\n",
      "  Assets: 272 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.03950 | Turnover: 0.2791 | TC: 0.001340 | Net: -0.04084\n",
      "\n",
      "[45/52] Date: 2023-09-30\n",
      "  Window: 2008-09-30 to 2023-08-31\n",
      "  Assets: 275 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00187 | Turnover: 0.3415 | TC: 0.001704 | Net: -0.00357\n",
      "\n",
      "[46/52] Date: 2023-10-31\n",
      "  Window: 2008-10-31 to 2023-09-30\n",
      "  Assets: 277 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.02137 | Turnover: 0.5795 | TC: 0.002959 | Net:  0.01841\n",
      "\n",
      "[47/52] Date: 2023-11-30\n",
      "  Window: 2008-11-30 to 2023-10-31\n",
      "  Assets: 280 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.01263 | Turnover: 0.5235 | TC: 0.002650 | Net:  0.00998\n",
      "\n",
      "[48/52] Date: 2023-12-31\n",
      "  Window: 2008-12-31 to 2023-11-30\n",
      "  Assets: 280 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00037 | Turnover: 0.4129 | TC: 0.002065 | Net: -0.00170\n",
      "\n",
      "[49/52] Date: 2024-01-31\n",
      "  Window: 2009-01-31 to 2023-12-31\n",
      "  Assets: 282 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.00048 | Turnover: 0.5363 | TC: 0.002683 | Net: -0.00221\n",
      "\n",
      "[50/52] Date: 2024-02-29\n",
      "  Window: 2009-02-28 to 2024-01-31\n",
      "  Assets: 282 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross:  0.03409 | Turnover: 0.5921 | TC: 0.003061 | Net:  0.03103\n",
      "\n",
      "[51/52] Date: 2024-03-31\n",
      "  Window: 2009-03-31 to 2024-02-29\n",
      "  Assets: 284 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00501 | Turnover: 0.4786 | TC: 0.002381 | Net: -0.00739\n",
      "\n",
      "[52/52] Date: 2024-04-30\n",
      "  Window: 2009-04-30 to 2024-03-31\n",
      "  Assets: 282 with complete data\n",
      "  Running Nodewise Regression...\n",
      "  Computing GMV weights...\n",
      "  Gross: -0.00815 | Turnover: 0.2831 | TC: 0.001404 | Net: -0.00956\n",
      "\n",
      "============================================================\n",
      "BACKTEST COMPLETE\n",
      "============================================================\n",
      "Annual Sharpe Ratio: 0.0464\n",
      "Total Return: -1.40%\n"
     ]
    }
   ],
   "source": [
    "# Run backtest with factor-based nodewise regression\n",
    "results_df, metrics = backtest_nodewise_gmv(\n",
    "    df,\n",
    "    factors_path='factors_ff_monthly_raw.csv',\n",
    "    test_start_date='2020-01-31',\n",
    "    test_end_date='2024-04-30',\n",
    "    lookback_window=180,\n",
    "    ic='GIC',  # or 'BIC', 'WIC', 'AIC', 'cv'\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Annual Sharpe Ratio: {metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics['total_return']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a5c7cb-e89b-4058-aa5a-dd160fb35c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006573778421578418"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['mean_return']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6458d9fa-bb0a-4390-9e01-249514b626e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020086004212696726"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['variance']*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada2d3a4-2ed5-43a3-b894-55c2918e3c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3470817947119989"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics['avg_turnover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c13c20-43e7-4eba-aa16-6c310c1c53cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3d9953-584f-47c2-9da8-6a28149da1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_weights(Theta_hat, mu, target_return=0.01):\n",
    "    \"\"\"\n",
    "    Compute Mean-Variance portfolio weights with target return.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected returns\n",
    "    target_return : float\n",
    "        Target portfolio return (default: 0.01 = 1% monthly)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute key quantities\n",
    "    A = ones_p @ Theta_hat @ ones_p\n",
    "    B = ones_p @ Theta_hat @ mu\n",
    "    C = mu @ Theta_hat @ mu\n",
    "    D = A * C - B * B\n",
    "    \n",
    "    # Check for singularity\n",
    "    if np.abs(D) < 1e-10:\n",
    "        if np.abs(A) > 1e-10:\n",
    "            w_star = (Theta_hat @ ones_p) / A\n",
    "            return w_star\n",
    "        else:\n",
    "            return ones_p / p\n",
    "    \n",
    "    # Compute Lagrange multipliers\n",
    "    lambda1 = (C - B * target_return) / D\n",
    "    lambda2 = (A * target_return - B) / D\n",
    "    \n",
    "    # Compute weights\n",
    "    w_star = lambda1 * (Theta_hat @ ones_p) + lambda2 * (Theta_hat @ mu)\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "\n",
    "def msr_weights(Theta_hat, mu):\n",
    "    \"\"\"\n",
    "    Compute Maximum Sharpe Ratio portfolio weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected excess returns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights (sum to 1)\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute unnormalized weights\n",
    "    w_unnorm = Theta_hat @ mu\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    weight_sum = np.sum(w_unnorm)\n",
    "    \n",
    "    if np.abs(weight_sum) < 1e-10:\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = w_unnorm / weight_sum\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LassoCV, LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "# Import DNN-FM helper functions (assumed to be available)\n",
    "from DNNFM_functions import *\n",
    "from comp_m_functions import static_factor_obs, cov_e_poet\n",
    "\n",
    "def round(x):\n",
    "    return int(Decimal(x).to_integral_value(rounding=ROUND_HALF_UP))\n",
    "\n",
    "\n",
    "def DNN_FM_main(data, data_factor, architecture=1, const_err_cov=2.5, use_CV_err=False, eval_type='frob'):\n",
    "    \"\"\"\n",
    "    Main DNN-FM function. Simplified version focusing on core functionality.\n",
    "    \"\"\"\n",
    "    data_dm = data\n",
    "    data_factor_dm = data_factor\n",
    "    \n",
    "    # Obtain optimal tuning parameter\n",
    "    opt = opt_hyper_parameters(data=data, data_F=data_factor, architecture=architecture, \n",
    "                               const_err_cov=const_err_cov, use_CV_err=use_CV_err, eval_type=eval_type)\n",
    "    \n",
    "    # Compute DNN-FM based on optimal hyper-parameters\n",
    "    res_DNN_FM = DNN_FM_core(data=data_dm, data_factor=data_factor_dm, architecture=architecture, opt=opt)\n",
    "    \n",
    "    # Compute covariance matrix\n",
    "    res_DNN_FM_cov = DNN_FM_cov(data=data_dm, data_factor=data_factor_dm, \n",
    "                                DNN_model=res_DNN_FM['neural_net'], \n",
    "                                c_err_cov=opt['const_err_cov'], check_eig=False)\n",
    "    \n",
    "    res_DNN_FM.update(res_DNN_FM_cov)\n",
    "    \n",
    "    return res_DNN_FM\n",
    "\n",
    "\n",
    "def DNN_FM_core(data, data_factor, architecture, opt):\n",
    "    \"\"\"\n",
    "    Core function for creating Neural Network.\n",
    "    \"\"\"\n",
    "    num_n, num_s = data.shape\n",
    "    num_f = data_factor.shape[1]\n",
    "\n",
    "    # Create neural network specifications based on architecture\n",
    "    if architecture == 5:\n",
    "        inter_layer = False\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "        \n",
    "        hidden_layer_s = [256, 128, 64]\n",
    "        dropout_rates_tr = [d_rate] * (n_layers + 1)\n",
    "        activation_functions = ['relu'] * n_layers + [None]\n",
    "    else:\n",
    "        # Default architecture\n",
    "        inter_layer = False\n",
    "        n_layers = 1\n",
    "        d_rate = 0.2\n",
    "        \n",
    "        hidden_layer_s = [512]\n",
    "        dropout_rates_tr = [d_rate, d_rate]\n",
    "        activation_functions = ['relu', None]\n",
    "\n",
    "    # Optimization options\n",
    "    optimizer = 'Adam'\n",
    "    max_iter = 2000\n",
    "    max_iter_nc = 50\n",
    "    split_ratio = 0.3\n",
    "    batch_s = 256\n",
    "    use_bias = False\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=max_iter_nc, mode='min')\n",
    "    \n",
    "    learning_rate = opt['learning_rate']\n",
    "    reg_par_w = opt['reg_par_w']\n",
    "    reg_par_b = opt['reg_par_b']\n",
    "    el_r_pro = opt['el_r_pro']\n",
    "    \n",
    "    # Create sparse neural network\n",
    "    neural_net = sparse_nn(hidden_layer_s, activation_functions, dropout_rates_tr, num_s, num_f,\n",
    "                          reg_par_w, reg_par_b, el_r_pro, max_iter_nc, max_iter, optimizer, \n",
    "                          learning_rate, use_bias, inter_layer)\n",
    "    \n",
    "    neural_net.build_neural_network()\n",
    "    neural_net.compile_nn()\n",
    "    \n",
    "    fit_nn = neural_net.model.fit(data_factor, data, epochs=max_iter,\n",
    "                                  validation_split=split_ratio, shuffle=True, batch_size=batch_s,\n",
    "                                  callbacks=early_stopping, verbose=0)\n",
    "    \n",
    "    # Compute market sensitivity\n",
    "    with tf.GradientTape() as tape:\n",
    "        data_factor_ts = tf.convert_to_tensor(data_factor[-1,:].reshape(1,-1), dtype=tf.float32)\n",
    "        tape.watch(data_factor_ts)\n",
    "        y_pred = neural_net.model(data_factor_ts)\n",
    "    \n",
    "    market_sens = tape.jacobian(y_pred, data_factor_ts)[-1,:,-1,:].numpy()\n",
    "    market_return_t = data_factor_ts.numpy()[-1, :].reshape(1,-1)\n",
    "    market_sensitivity = (market_sens, market_return_t)\n",
    "    \n",
    "    res = {'neural_net': neural_net, 'opt': opt, 'market_sensitivity': market_sensitivity}\n",
    "    return res\n",
    "\n",
    "\n",
    "def DNN_FM_cov(data, data_factor, DNN_model, c_err_cov, check_eig=False):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix from DNN-FM model.\n",
    "    \"\"\"\n",
    "    num_n, num_s = data.shape\n",
    "    \n",
    "    y_hat_nn = DNN_model.model(data_factor).numpy()\n",
    "    resd_nn = data - y_hat_nn\n",
    "    \n",
    "    sig_hat_e = thres_resd_new(resd_nn, c_err_cov, num_s, num_n)\n",
    "    cov_f_nnet = np.cov(y_hat_nn.T)\n",
    "    \n",
    "    if not check_eig:\n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "    else:\n",
    "        cond = True\n",
    "        const_c = 0\n",
    "        while cond:\n",
    "            sig_hat_e = thres_resd_new(resd_nn, c_err_cov+const_c, num_s, num_n)\n",
    "            cond = (round(min(np.linalg.eig(sig_hat_e)[0]),2) < 0.01) or (np.linalg.cond(sig_hat_e) > num_s*10)\n",
    "            const_c+=0.01\n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "    \n",
    "    inv_sigma_y_nnet = sig_inv_f_nnet(cov_f_nnet, sig_hat_e)\n",
    "    \n",
    "    res = {'sigma_hat': sigma_y_nnet, 'sigma_f_hat': cov_f_nnet, 'sigma_e_hat': sig_hat_e, \n",
    "           'inv_sigma_hat': inv_sigma_y_nnet}\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def opt_hyper_parameters(data, data_F, architecture, const_err_cov, use_CV_err, eval_type):\n",
    "    \"\"\"\n",
    "    Determine optimal regularization and learning rate.\n",
    "    \"\"\"\n",
    "    if architecture == 1 or architecture == 5 or architecture == 7:\n",
    "        reg_par_w = 0.0005\n",
    "        reg_par_b = 0.0005\n",
    "    else:\n",
    "        reg_par_w = 0.0\n",
    "        reg_par_b = 0.0\n",
    "    \n",
    "    el_r_pro = 1\n",
    "    learning_rate = 0.0005\n",
    "    \n",
    "    if use_CV_err:\n",
    "        range_cov_err = np.arange(0, const_err_cov+0.6, 0.1)\n",
    "        opt = cv_split(data=data, data_F=data_F, architecture=architecture, \n",
    "                      reg_par=reg_par_w, lr=learning_rate, range_cov_err=range_cov_err, \n",
    "                      eval_type=eval_type)\n",
    "    else:\n",
    "        opt = {'learning_rate': learning_rate, 'reg_par_w': reg_par_w, \n",
    "               'reg_par_b': reg_par_b, 'el_r_pro': el_r_pro, 'const_err_cov': const_err_cov}\n",
    "    \n",
    "    return opt\n",
    "\n",
    "\n",
    "def cv_split(data, data_F, architecture, reg_par, lr, range_cov_err, eval_type):\n",
    "    \"\"\"\n",
    "    Cross-validation for hyperparameter selection.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    opt = {'learning_rate': lr, 'reg_par_w': reg_par, 'reg_par_b': reg_par, 'el_r_pro': 1}\n",
    "    \n",
    "    data_dm, _, _ = normalize_dat_sim(data)\n",
    "    data_F_dm, _, _ = normalize_dat_sim(data_F)\n",
    "    \n",
    "    res_DNN_FM = DNN_FM_core(data_dm, data_F_dm, architecture, opt)\n",
    "    neural_net = res_DNN_FM['neural_net']\n",
    "    \n",
    "    y_hat = neural_net.model(data_F_dm).numpy()\n",
    "    resd_nn = data_dm - y_hat\n",
    "    \n",
    "    n_folds = 10\n",
    "    res_mat = np.empty((n_folds, len(range_cov_err)))\n",
    "    res_mat[:] = np.nan\n",
    "    \n",
    "    split_sample, _ = ts_train_test_split(resd_nn, n_folds, train_size=0.5)\n",
    "    \n",
    "    for m_idx in range(n_folds):\n",
    "        resd_nn_s1 = split_sample[m_idx][0]\n",
    "        resd_nn_s2 = split_sample[m_idx][1]\n",
    "        \n",
    "        num_n, num_s = resd_nn_s1.shape\n",
    "        sigma_test = cov_sfm(resd_nn_s2)\n",
    "        sig_e_samp, thet_par = thres_cov_resd_aux(resd_nn_s1, num_s)\n",
    "        \n",
    "        for c_idx in range(len(range_cov_err)):\n",
    "            sig_hat_e = thres_cov_resd(sig_e_samp, thet_par, range_cov_err[c_idx], num_s, num_n)\n",
    "            \n",
    "            if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "                res_mat[m_idx, c_idx] = np.inf\n",
    "            else:\n",
    "                if eval_type == 'frob':\n",
    "                    res_mat[m_idx, c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')**2\n",
    "                elif eval_type == 'spec':\n",
    "                    res_mat[m_idx, c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord=2)**2\n",
    "    \n",
    "    idx_opt = np.where(res_mat.mean(axis=0) == np.nanmin(res_mat.mean(axis=0)))\n",
    "    opt.update({'const_err_cov': range_cov_err[idx_opt][0]})\n",
    "    \n",
    "    print(f\"CV took {time.time() - start:.2f} seconds\")\n",
    "    return opt\n",
    "\n",
    "\n",
    "def ts_train_test_split(X, n_folds=5, train_size=0.5):\n",
    "    \"\"\"\n",
    "    Time series train-test split.\n",
    "    \"\"\"\n",
    "    test_size = 1 - train_size\n",
    "    n_obs = X.shape[0]\n",
    "    size_split = n_obs - n_folds\n",
    "    \n",
    "    n_train = round(size_split * train_size)\n",
    "    n_test = round(size_split * test_size)\n",
    "    \n",
    "    split_t = (list(range(0, n_train)), list(range(n_train, n_train+n_test)))\n",
    "    \n",
    "    split_sample = []\n",
    "    split_index = []\n",
    "    \n",
    "    for jj in range(n_folds):\n",
    "        split_index.append(([el1 + jj for el1 in split_t[0]], [el2 + jj for el2 in split_t[1]]))\n",
    "        split_sample.append((X[split_index[jj][0],:], X[split_index[jj][1],:]))\n",
    "    \n",
    "    return split_sample, split_index\n",
    "\n",
    "\n",
    "def gmv_weights(Theta_hat):\n",
    "    \"\"\"\n",
    "    Compute Global Minimum Variance (GMV) portfolio weights.\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    numerator = Theta_hat @ ones_p\n",
    "    denominator = ones_p @ Theta_hat @ ones_p\n",
    "    \n",
    "    if np.abs(denominator) < 1e-10:\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = numerator / denominator\n",
    "    return w_star\n",
    "\n",
    "\n",
    "def load_recommendation_changes(rec_changes_path):\n",
    "    \"\"\"\n",
    "    Load recommendation changes from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rec_changes_path : str\n",
    "        Path to monthly_mean_recommendations_decay.csv file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    rec_changes_df : pd.DataFrame\n",
    "        DataFrame with columns: permno, date, ticker, weighted_mean_recommendation, \n",
    "        recommendation_change, num_recommendations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rec_changes_df = pd.read_csv(rec_changes_path)\n",
    "        rec_changes_df['date'] = pd.to_datetime(rec_changes_df['date'])\n",
    "        rec_changes_df['permno'] = rec_changes_df['permno'].astype(int)\n",
    "        return rec_changes_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ⚠ Warning: Could not load recommendation changes: {e}\")\n",
    "        return pd.DataFrame(columns=['permno', 'date', 'ticker', 'weighted_mean_recommendation', \n",
    "                                    'recommendation_change', 'num_recommendations'])\n",
    "\n",
    "\n",
    "def get_signal_permnos_for_date(rec_changes_df, date, buy_threshold=-0.5, sell_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Get sets of permnos with buy/sell signals based on recommendation changes.\n",
    "    \n",
    "    Note: Negative change = upgrade (moving toward Strong Buy) = BUY signal\n",
    "          Positive change = downgrade (moving toward Sell) = SELL signal\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rec_changes_df : pd.DataFrame\n",
    "        Recommendation changes dataframe\n",
    "    date : pd.Timestamp\n",
    "        Date to get signals for\n",
    "    buy_threshold : float\n",
    "        Threshold for buy signals (default: -0.5)\n",
    "    sell_threshold : float\n",
    "        Threshold for sell signals (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    buy_permnos : set\n",
    "        Set of permnos with buy signals\n",
    "    sell_permnos : set\n",
    "        Set of permnos with sell signals\n",
    "    \"\"\"\n",
    "    date_changes = rec_changes_df[rec_changes_df['date'] == date]\n",
    "    \n",
    "    # Buy signals: negative changes (recommendations getting better)\n",
    "    buys = date_changes[date_changes['recommendation_change'] <= buy_threshold]\n",
    "    buy_permnos = set(buys['permno'].values)\n",
    "    \n",
    "    # Sell signals: positive changes (recommendations getting worse)\n",
    "    sells = date_changes[date_changes['recommendation_change'] >= sell_threshold]\n",
    "    sell_permnos = set(sells['permno'].values)\n",
    "    \n",
    "    return buy_permnos, sell_permnos\n",
    "\n",
    "\n",
    "def load_ff_factors(factors_path='factors_ff_monthly_raw.csv'):\n",
    "    \"\"\"\n",
    "    Load Fama-French factors from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    factors_path : str\n",
    "        Path to the factors CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    factors_df : pd.DataFrame\n",
    "        DataFrame with date index and factor columns\n",
    "    \"\"\"\n",
    "    factors_df = pd.read_csv(factors_path)\n",
    "    \n",
    "    # Convert month column (e.g., 192707) to datetime\n",
    "    # This gives us the first day of the month (1927-07-01)\n",
    "    factors_df['date'] = pd.to_datetime(factors_df.iloc[:, 0].astype(str), format='%Y%m')\n",
    "    \n",
    "    # Convert to end of month to match returns data\n",
    "    factors_df['date'] = factors_df['date'] + pd.offsets.MonthEnd(0)\n",
    "    \n",
    "    # Set date as index and keep only factor columns\n",
    "    factors_df = factors_df.set_index('date')[['Mkt-RF', 'SMB', 'HML']]\n",
    "    \n",
    "    # Convert to decimal form (assuming factors are in percentage points)\n",
    "    factors_df = factors_df / 100\n",
    "    \n",
    "    return factors_df\n",
    "\n",
    "\n",
    "def backtest_dnn_yearly(df, \n",
    "                                test_start_date='2020-01-31', \n",
    "                                test_end_date='2024-11-30',\n",
    "                                lookback_window=180,\n",
    "                                transaction_cost=0.001,\n",
    "                                rec_changes_path=None,\n",
    "                                buy_threshold=-0.5,\n",
    "                                sell_threshold=0.5,\n",
    "                                data_factor=None,\n",
    "                                verbose=True):\n",
    "    \"\"\"\n",
    "    Backtest DNN-FM with analyst recommendation signals (GMV/MV/MSR strategies).\n",
    "    Only invests in stocks with buy/sell signals from analysts.\n",
    "    \n",
    "    Key improvements:\n",
    "    - Filters stocks based on analyst recommendation changes\n",
    "    - Better handling of adjusted weights for assets that exit\n",
    "    - More robust weight normalization\n",
    "    - Cleaner transaction cost calculation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: permno, datadate, ret_fwd_1\n",
    "    test_start_date : str\n",
    "        First date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    test_end_date : str\n",
    "        Last date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    lookback_window : int\n",
    "        Number of months in rolling training window (default: 180)\n",
    "    transaction_cost : float\n",
    "        Proportional transaction cost (default: 0.001 = 10 bps)\n",
    "    rec_changes_path : str\n",
    "        Path to monthly_mean_recommendations_decay.csv file (required)\n",
    "    buy_threshold : float\n",
    "        Threshold for buy signals (default: -0.5, negative = upgrade)\n",
    "    sell_threshold : float\n",
    "        Threshold for sell signals (default: 0.5, positive = downgrade)\n",
    "    data_factor : pd.DataFrame\n",
    "        Factor data for DNN-FM model\n",
    "    verbose : bool\n",
    "        If True, prints detailed log at each time step.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame (for GMV)\n",
    "    metrics : dict (for GMV)\n",
    "    results_df_2 : pd.DataFrame (for MV)\n",
    "    metrics_2 : dict (for MV)\n",
    "    results_df_3 : pd.DataFrame (for MSR)\n",
    "    metrics_3 : dict (for MSR)\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    df = df.copy()\n",
    "    if 'datadate' not in df.columns or 'permno' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'datadate' and 'permno' columns\")\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'])\n",
    "    \n",
    "    # Load recommendation changes (required)\n",
    "    if rec_changes_path is None:\n",
    "        raise ValueError(\"rec_changes_path is required for this strategy\")\n",
    "    \n",
    "    rec_changes_df = load_recommendation_changes(rec_changes_path)\n",
    "    if len(rec_changes_df) == 0:\n",
    "        raise ValueError(\"No recommendation changes loaded\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loaded recommendation changes: {len(rec_changes_df)} records\")\n",
    "        print(f\"Strategy: BUY threshold <= {buy_threshold}, SELL threshold >= {sell_threshold}\")\n",
    "    \n",
    "    # Get unique dates\n",
    "    all_dates = sorted(df['datadate'].unique())\n",
    "    \n",
    "    # Convert test dates to datetime\n",
    "    test_start_dt = pd.to_datetime(test_start_date)\n",
    "    test_end_dt = pd.to_datetime(test_end_date)\n",
    "    \n",
    "    # Find date indices\n",
    "    try:\n",
    "        test_start_idx = all_dates.index(test_start_dt)\n",
    "        test_end_idx = all_dates.index(test_end_dt)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Date not found in DataFrame: {e}\")\n",
    "    \n",
    "    if test_start_idx < lookback_window:\n",
    "        raise ValueError(f\"Not enough data for lookback. Test start date {test_start_date} \"\n",
    "                         f\"requires data back to {all_dates[test_start_idx - lookback_window]}, \"\n",
    "                         f\"but only {test_start_idx} periods are available.\")\n",
    "    \n",
    "    # Storage for results - GMV\n",
    "    portfolio_returns = []\n",
    "    portfolio_dates = []\n",
    "    portfolio_weights_list = []\n",
    "    portfolio_turnover_list = []\n",
    "    portfolio_gross_returns = []\n",
    "    \n",
    "    # Storage for results - MV\n",
    "    portfolio_returns_2 = []\n",
    "    portfolio_dates_2 = []\n",
    "    portfolio_weights_list_2 = []\n",
    "    portfolio_turnover_list_2 = []\n",
    "    portfolio_gross_returns_2 = []\n",
    "    \n",
    "    # Storage for results - MSR\n",
    "    portfolio_returns_3 = []\n",
    "    portfolio_dates_3 = []\n",
    "    portfolio_weights_list_3 = []\n",
    "    portfolio_turnover_list_3 = []\n",
    "    portfolio_gross_returns_3 = []\n",
    "    \n",
    "    # Track weights by permno - GMV\n",
    "    prev_weights_dict = {}\n",
    "    prev_oos_returns_dict = {}\n",
    "    prev_gross_return = 0.0\n",
    "    \n",
    "    # Track weights by permno - MV\n",
    "    prev_weights_dict_2 = {}\n",
    "    prev_oos_returns_dict_2 = {}\n",
    "    prev_gross_return_2 = 0.0\n",
    "    \n",
    "    # Track weights by permno - MSR\n",
    "    prev_weights_dict_3 = {}\n",
    "    prev_oos_returns_dict_3 = {}\n",
    "    prev_gross_return_3 = 0.0\n",
    "    \n",
    "    # --- 2. Rolling Window Backtest ---\n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING BACKTEST WITH DNN-FM + ANALYST SIGNALS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "    for t in range(test_start_idx, test_end_idx + 1):\n",
    "        current_date = all_dates[t]\n",
    "        \n",
    "        # Get buy and sell signals for current date\n",
    "        buy_permnos, sell_permnos = get_signal_permnos_for_date(\n",
    "            rec_changes_df, current_date, buy_threshold, sell_threshold\n",
    "        )\n",
    "        \n",
    "        # Combine all permnos with signals\n",
    "        allowed_permnos = buy_permnos | sell_permnos\n",
    "        \n",
    "        if len(allowed_permnos) == 0:\n",
    "            if verbose:\n",
    "                print(f\"\\n[{t - test_start_idx + 1}/{test_end_idx - test_start_idx + 1}] \"\n",
    "                      f\"Date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "                print(f\"  ⚠ No analyst signals for this date, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Define the lookback window\n",
    "        window_start_date = all_dates[t - lookback_window]\n",
    "        window_end_date = all_dates[t - 1]\n",
    "        \n",
    "        # Get training data for this window, filtered by allowed permnos\n",
    "        train_data = df[(df['datadate'] >= window_start_date) & \n",
    "                        (df['datadate'] <= window_end_date) &\n",
    "                        (df['permno'].isin(allowed_permnos))]\n",
    "        train_factor = data_factor.loc[window_start_date : window_end_date]\n",
    "        \n",
    "        # Pivot to get returns matrix (time x assets)\n",
    "        returns_pivot = train_data.pivot(index='datadate', columns='permno', values='ret_fwd_1')\n",
    "        \n",
    "        # Reindex to ensure all dates are present\n",
    "        window_dates = all_dates[t - lookback_window : t]\n",
    "        returns_pivot = returns_pivot.reindex(index=window_dates)\n",
    "        \n",
    "        # Filter assets with any NaNs in this window\n",
    "        nan_assets = returns_pivot.columns[returns_pivot.isna().any()]\n",
    "        filtered_pivot = returns_pivot.drop(columns=nan_assets)\n",
    "        \n",
    "        current_assets = filtered_pivot.columns.tolist()\n",
    "        Y = filtered_pivot.values\n",
    "        n_train, p_current = Y.shape\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[{t - test_start_idx + 1}/{test_end_idx - test_start_idx + 1}] \"\n",
    "                  f\"Date: {current_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Window: {window_start_date.strftime('%Y-%m-%d')} to \"\n",
    "                  f\"{window_end_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  Buys: {len(buy_permnos)} | Sells: {len(sell_permnos)} | \"\n",
    "                  f\"Assets w/ data: {p_current}\")\n",
    "\n",
    "        # Check for valid data\n",
    "        if n_train < lookback_window or p_current < 2:\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ Insufficient data (n={n_train}, p={p_current}), using prev weights\")\n",
    "            # Filter previous weights to only allowed permnos\n",
    "            new_weights_dict = {k: v for k, v in prev_weights_dict.items() if k in allowed_permnos}\n",
    "            new_weights_dict_2 = {k: v for k, v in prev_weights_dict_2.items() if k in allowed_permnos}\n",
    "            new_weights_dict_3 = {k: v for k, v in prev_weights_dict_3.items() if k in allowed_permnos}\n",
    "        else:\n",
    "            try:\n",
    "                # Demean the returns\n",
    "                Y_bar = Y.mean(axis=0)\n",
    "                Y_star = Y - Y_bar\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Running Deep Learning Regression...\")\n",
    "                F = train_factor.values.astype(float)\n",
    "                res_nnet_fm = DNN_FM_main(Y_star, F, architecture=5, const_err_cov=2.5, \n",
    "                                         use_CV_err=False, eval_type='frob')\n",
    "                Theta_hat = res_nnet_fm['inv_sigma_hat']\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing GMV weights...\")\n",
    "                w_star = gmv_weights(Theta_hat)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing MV weights...\")\n",
    "                w_star_2 = mv_weights(Theta_hat, Y_bar, target_return=0.01)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"  Computing MSR weights...\")\n",
    "                w_star_3 = msr_weights(Theta_hat, Y_bar)\n",
    "                \n",
    "                # Create weights dictionaries\n",
    "                new_weights_dict = {asset: w_star[i] for i, asset in enumerate(current_assets)}\n",
    "                new_weights_dict_2 = {asset: w_star_2[i] for i, asset in enumerate(current_assets)}\n",
    "                new_weights_dict_3 = {asset: w_star_3[i] for i, asset in enumerate(current_assets)}\n",
    "                \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"  ✗ Error: {e}\")\n",
    "                    print(f\"  Using previous weights\")\n",
    "                new_weights_dict = prev_weights_dict.copy()\n",
    "                new_weights_dict_2 = prev_weights_dict_2.copy()\n",
    "                new_weights_dict_3 = prev_weights_dict_3.copy()\n",
    "\n",
    "        # Normalize weights to sum to 1 - GMV\n",
    "        weight_sum = sum(new_weights_dict.values())\n",
    "        if weight_sum > 1e-10:\n",
    "            new_weights_dict = {k: v/weight_sum for k, v in new_weights_dict.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ GMV: Zero weight sum, using previous weights\")\n",
    "            new_weights_dict = prev_weights_dict.copy()\n",
    "            weight_sum = sum(new_weights_dict.values())\n",
    "            if weight_sum > 1e-10:\n",
    "                new_weights_dict = {k: v/weight_sum for k, v in new_weights_dict.items()}\n",
    "        \n",
    "        # Normalize weights to sum to 1 - MV\n",
    "        weight_sum_2 = sum(new_weights_dict_2.values())\n",
    "        if weight_sum_2 > 1e-10:\n",
    "            new_weights_dict_2 = {k: v/weight_sum_2 for k, v in new_weights_dict_2.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ MV: Zero weight sum, using previous weights\")\n",
    "            new_weights_dict_2 = prev_weights_dict_2.copy()\n",
    "            weight_sum_2 = sum(new_weights_dict_2.values())\n",
    "            if weight_sum_2 > 1e-10:\n",
    "                new_weights_dict_2 = {k: v/weight_sum_2 for k, v in new_weights_dict_2.items()}\n",
    "        \n",
    "        # Normalize weights to sum to 1 - MSR\n",
    "        weight_sum_3 = sum(new_weights_dict_3.values())\n",
    "        if weight_sum_3 > 1e-10:\n",
    "            new_weights_dict_3 = {k: v/weight_sum_3 for k, v in new_weights_dict_3.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ MSR: Zero weight sum, using previous weights\")\n",
    "            new_weights_dict_3 = prev_weights_dict_3.copy()\n",
    "            weight_sum_3 = sum(new_weights_dict_3.values())\n",
    "            if weight_sum_3 > 1e-10:\n",
    "                new_weights_dict_3 = {k: v/weight_sum_3 for k, v in new_weights_dict_3.items()}\n",
    "        \n",
    "        # --- 3. OOS Returns & Transaction Costs ---\n",
    "        \n",
    "        # Get out-of-sample returns for current month (only for allowed permnos)\n",
    "        oos_data = df[(df['datadate'] == current_date) & (df['permno'].isin(allowed_permnos))]\n",
    "        oos_returns_series = oos_data.set_index('permno')['ret_fwd_1']\n",
    "        \n",
    "        # Filter out NaN returns\n",
    "        oos_returns_series = oos_returns_series.dropna()\n",
    "        oos_returns_dict = oos_returns_series.to_dict()\n",
    "        \n",
    "        # Find common assets between weights and returns\n",
    "        common_assets = set(new_weights_dict.keys()) & set(oos_returns_dict.keys())\n",
    "        common_assets_2 = set(new_weights_dict_2.keys()) & set(oos_returns_dict.keys())\n",
    "        common_assets_3 = set(new_weights_dict_3.keys()) & set(oos_returns_dict.keys())\n",
    "        \n",
    "        if len(common_assets) == 0 or len(common_assets_2) == 0 or len(common_assets_3) == 0:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ No common assets with valid returns, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to common assets and renormalize - GMV\n",
    "        common_weights = {a: new_weights_dict[a] for a in common_assets}\n",
    "        common_weight_sum = sum(common_weights.values())\n",
    "        if common_weight_sum > 1e-10:\n",
    "            common_weights = {k: v/common_weight_sum for k, v in common_weights.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ GMV: Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to common assets and renormalize - MV\n",
    "        common_weights_2 = {a: new_weights_dict_2[a] for a in common_assets_2}\n",
    "        common_weight_sum_2 = sum(common_weights_2.values())\n",
    "        if common_weight_sum_2 > 1e-10:\n",
    "            common_weights_2 = {k: v/common_weight_sum_2 for k, v in common_weights_2.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ MV: Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to common assets and renormalize - MSR\n",
    "        common_weights_3 = {a: new_weights_dict_3[a] for a in common_assets_3}\n",
    "        common_weight_sum_3 = sum(common_weights_3.values())\n",
    "        if common_weight_sum_3 > 1e-10:\n",
    "            common_weights_3 = {k: v/common_weight_sum_3 for k, v in common_weights_3.items()}\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"  ⚠ MSR: Zero weight sum after filtering, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # Compute gross portfolio returns\n",
    "        gross_return = sum(common_weights[a] * oos_returns_dict[a] for a in common_assets)\n",
    "        gross_return_2 = sum(common_weights_2[a] * oos_returns_dict[a] for a in common_assets_2)\n",
    "        gross_return_3 = sum(common_weights_3[a] * oos_returns_dict[a] for a in common_assets_3)\n",
    "        \n",
    "        # Sanity checks\n",
    "        if np.isnan(gross_return) or np.isinf(gross_return):\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ GMV: Invalid gross return: {gross_return}, skipping period\")\n",
    "            continue\n",
    "        if np.isnan(gross_return_2) or np.isinf(gross_return_2):\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ MV: Invalid gross return: {gross_return_2}, skipping period\")\n",
    "            continue\n",
    "        if np.isnan(gross_return_3) or np.isinf(gross_return_3):\n",
    "            if verbose:\n",
    "                print(f\"  ⚠ MSR: Invalid gross return: {gross_return_3}, skipping period\")\n",
    "            continue\n",
    "        \n",
    "        # === IMPROVED TRANSACTION COST CALCULATION - GMV ===\n",
    "        if len(prev_weights_dict) > 0:\n",
    "            # Step 1: Adjust ALL previous weights for their returns\n",
    "            adjusted_prev = {}\n",
    "            \n",
    "            for asset, prev_w in prev_weights_dict.items():\n",
    "                if asset in prev_oos_returns_dict:\n",
    "                    prev_r = prev_oos_returns_dict[asset]\n",
    "                    if abs(1 + prev_gross_return) > 1e-6:\n",
    "                        adjusted_prev[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return)\n",
    "                    else:\n",
    "                        adjusted_prev[asset] = 0.0\n",
    "                else:\n",
    "                    # Asset had weight but no return data (exited)\n",
    "                    if abs(1 + prev_gross_return) > 1e-6:\n",
    "                        adjusted_prev[asset] = prev_w / (1 + prev_gross_return)\n",
    "                    else:\n",
    "                        adjusted_prev[asset] = 0.0\n",
    "            \n",
    "            # Step 2: Calculate turnover across all assets (old and new)\n",
    "            all_assets = set(adjusted_prev.keys()) | set(common_weights.keys())\n",
    "            \n",
    "            turnover = 0.0\n",
    "            for asset in all_assets:\n",
    "                old_w = adjusted_prev.get(asset, 0.0)\n",
    "                new_w = common_weights.get(asset, 0.0)\n",
    "                turnover += abs(new_w - old_w)\n",
    "            \n",
    "            # Transaction cost on end-of-period portfolio value\n",
    "            tc = transaction_cost * (1 + gross_return) * turnover\n",
    "        else:\n",
    "            # First period: buying into everything\n",
    "            turnover = sum(abs(w) for w in common_weights.values())\n",
    "            tc = transaction_cost * (1 + gross_return) * turnover\n",
    "        \n",
    "        # === IMPROVED TRANSACTION COST CALCULATION - MV ===\n",
    "        if len(prev_weights_dict_2) > 0:\n",
    "            adjusted_prev_2 = {}\n",
    "            \n",
    "            for asset, prev_w in prev_weights_dict_2.items():\n",
    "                if asset in prev_oos_returns_dict_2:\n",
    "                    prev_r = prev_oos_returns_dict_2[asset]\n",
    "                    if abs(1 + prev_gross_return_2) > 1e-6:\n",
    "                        adjusted_prev_2[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return_2)\n",
    "                    else:\n",
    "                        adjusted_prev_2[asset] = 0.0\n",
    "                else:\n",
    "                    if abs(1 + prev_gross_return_2) > 1e-6:\n",
    "                        adjusted_prev_2[asset] = prev_w / (1 + prev_gross_return_2)\n",
    "                    else:\n",
    "                        adjusted_prev_2[asset] = 0.0\n",
    "            \n",
    "            all_assets_2 = set(adjusted_prev_2.keys()) | set(common_weights_2.keys())\n",
    "            \n",
    "            turnover_2 = 0.0\n",
    "            for asset in all_assets_2:\n",
    "                old_w = adjusted_prev_2.get(asset, 0.0)\n",
    "                new_w = common_weights_2.get(asset, 0.0)\n",
    "                turnover_2 += abs(new_w - old_w)\n",
    "            \n",
    "            tc_2 = transaction_cost * (1 + gross_return_2) * turnover_2\n",
    "        else:\n",
    "            turnover_2 = sum(abs(w) for w in common_weights_2.values())\n",
    "            tc_2 = transaction_cost * (1 + gross_return_2) * turnover_2\n",
    "        \n",
    "        # === IMPROVED TRANSACTION COST CALCULATION - MSR ===\n",
    "        if len(prev_weights_dict_3) > 0:\n",
    "            adjusted_prev_3 = {}\n",
    "            \n",
    "            for asset, prev_w in prev_weights_dict_3.items():\n",
    "                if asset in prev_oos_returns_dict_3:\n",
    "                    prev_r = prev_oos_returns_dict_3[asset]\n",
    "                    if abs(1 + prev_gross_return_3) > 1e-6:\n",
    "                        adjusted_prev_3[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_return_3)\n",
    "                    else:\n",
    "                        adjusted_prev_3[asset] = 0.0\n",
    "                else:\n",
    "                    if abs(1 + prev_gross_return_3) > 1e-6:\n",
    "                        adjusted_prev_3[asset] = prev_w / (1 + prev_gross_return_3)\n",
    "                    else:\n",
    "                        adjusted_prev_3[asset] = 0.0\n",
    "            \n",
    "            all_assets_3 = set(adjusted_prev_3.keys()) | set(common_weights_3.keys())\n",
    "            \n",
    "            turnover_3 = 0.0\n",
    "            for asset in all_assets_3:\n",
    "                old_w = adjusted_prev_3.get(asset, 0.0)\n",
    "                new_w = common_weights_3.get(asset, 0.0)\n",
    "                turnover_3 += abs(new_w - old_w)\n",
    "            \n",
    "            tc_3 = transaction_cost * (1 + gross_return_3) * turnover_3\n",
    "        else:\n",
    "            turnover_3 = sum(abs(w) for w in common_weights_3.values())\n",
    "            tc_3 = transaction_cost * (1 + gross_return_3) * turnover_3\n",
    "        \n",
    "        # Net returns\n",
    "        net_return = gross_return - tc\n",
    "        net_return_2 = gross_return_2 - tc_2\n",
    "        net_return_3 = gross_return_3 - tc_3\n",
    "        \n",
    "        # Store results - GMV\n",
    "        portfolio_returns.append(net_return)\n",
    "        portfolio_dates.append(current_date)\n",
    "        portfolio_weights_list.append(common_weights.copy())\n",
    "        portfolio_turnover_list.append(turnover)\n",
    "        portfolio_gross_returns.append(gross_return)\n",
    "        \n",
    "        # Store results - MV\n",
    "        portfolio_returns_2.append(net_return_2)\n",
    "        portfolio_dates_2.append(current_date)\n",
    "        portfolio_weights_list_2.append(common_weights_2.copy())\n",
    "        portfolio_turnover_list_2.append(turnover_2)\n",
    "        portfolio_gross_returns_2.append(gross_return_2)\n",
    "        \n",
    "        # Store results - MSR\n",
    "        portfolio_returns_3.append(net_return_3)\n",
    "        portfolio_dates_3.append(current_date)\n",
    "        portfolio_weights_list_3.append(common_weights_3.copy())\n",
    "        portfolio_turnover_list_3.append(turnover_3)\n",
    "        portfolio_gross_returns_3.append(gross_return_3)\n",
    "        \n",
    "        # Update previous values for next iteration\n",
    "        prev_weights_dict = common_weights.copy()\n",
    "        prev_oos_returns_dict = {a: oos_returns_dict[a] for a in common_assets}\n",
    "        prev_gross_return = gross_return\n",
    "        \n",
    "        prev_weights_dict_2 = common_weights_2.copy()\n",
    "        prev_oos_returns_dict_2 = {a: oos_returns_dict[a] for a in common_assets_2}\n",
    "        prev_gross_return_2 = gross_return_2\n",
    "        \n",
    "        prev_weights_dict_3 = common_weights_3.copy()\n",
    "        prev_oos_returns_dict_3 = {a: oos_returns_dict[a] for a in common_assets_3}\n",
    "        prev_gross_return_3 = gross_return_3\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  GMV  - Gross: {gross_return:>8.5f} | Turnover: {turnover:>6.4f} | \"\n",
    "                  f\"TC: {tc:>8.6f} | Net: {net_return:>8.5f}\")\n",
    "            print(f\"  MV   - Gross: {gross_return_2:>8.5f} | Turnover: {turnover_2:>6.4f} | \"\n",
    "                  f\"TC: {tc_2:>8.6f} | Net: {net_return_2:>8.5f}\")\n",
    "            print(f\"  MSR  - Gross: {gross_return_3:>8.5f} | Turnover: {turnover_3:>6.4f} | \"\n",
    "                  f\"TC: {tc_3:>8.6f} | Net: {net_return_3:>8.5f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BACKTEST COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # --- 4. Compile Results ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'date': portfolio_dates,\n",
    "        'portfolio_return': portfolio_returns,\n",
    "        'portfolio_gross_return': portfolio_gross_returns,\n",
    "        'portfolio_weights': portfolio_weights_list,\n",
    "        'portfolio_turnover': portfolio_turnover_list\n",
    "    })\n",
    "    results_df['cumulative_return'] = (1 + results_df['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    results_df_2 = pd.DataFrame({\n",
    "        'date': portfolio_dates_2,\n",
    "        'portfolio_return': portfolio_returns_2,\n",
    "        'portfolio_gross_return': portfolio_gross_returns_2,\n",
    "        'portfolio_weights': portfolio_weights_list_2,\n",
    "        'portfolio_turnover': portfolio_turnover_list_2\n",
    "    })\n",
    "    results_df_2['cumulative_return'] = (1 + results_df_2['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    results_df_3 = pd.DataFrame({\n",
    "        'date': portfolio_dates_3,\n",
    "        'portfolio_return': portfolio_returns_3,\n",
    "        'portfolio_gross_return': portfolio_gross_returns_3,\n",
    "        'portfolio_weights': portfolio_weights_list_3,\n",
    "        'portfolio_turnover': portfolio_turnover_list_3\n",
    "    })\n",
    "    results_df_3['cumulative_return'] = (1 + results_df_3['portfolio_return']).cumprod() - 1\n",
    "    \n",
    "    # Helper function to compute metrics\n",
    "    def compute_metrics(returns_list, turnover_list, results_df):\n",
    "        if len(returns_list) > 0:\n",
    "            mean_return = np.mean(returns_list)\n",
    "            variance = np.var(returns_list, ddof=1)\n",
    "            sharpe_ratio = mean_return / np.sqrt(variance) if variance > 0 else 0\n",
    "            \n",
    "            # Annualized metrics (monthly data)\n",
    "            annual_return = mean_return * 12\n",
    "            annual_volatility = np.sqrt(variance * 12)\n",
    "            annual_sharpe = annual_return / annual_volatility if annual_volatility > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'mean_return': mean_return,\n",
    "                'variance': variance,\n",
    "                'sharpe_ratio': sharpe_ratio,\n",
    "                'annual_return': annual_return,\n",
    "                'annual_volatility': annual_volatility,\n",
    "                'annual_sharpe_ratio': annual_sharpe,\n",
    "                'total_return': results_df['cumulative_return'].iloc[-1],\n",
    "                'avg_turnover': np.mean(turnover_list),\n",
    "                'n_periods': len(returns_list)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'mean_return': 0, 'variance': 0, 'sharpe_ratio': 0,\n",
    "                'annual_return': 0, 'annual_volatility': 0, 'annual_sharpe_ratio': 0,\n",
    "                'total_return': 0, 'avg_turnover': 0, 'n_periods': 0\n",
    "            }\n",
    "    \n",
    "    # Compute metrics for all three strategies\n",
    "    metrics = compute_metrics(portfolio_returns, portfolio_turnover_list, results_df)\n",
    "    metrics_2 = compute_metrics(portfolio_returns_2, portfolio_turnover_list_2, results_df_2)\n",
    "    metrics_3 = compute_metrics(portfolio_returns_3, portfolio_turnover_list_3, results_df_3)\n",
    "    \n",
    "    return results_df, metrics, results_df_2, metrics_2, results_df_3, metrics_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3765c3c-1c95-490b-a755-dc7e34821677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded recommendation changes: 16960 records\n",
      "Strategy: BUY threshold <= -0.5, SELL threshold >= 0.5\n",
      "============================================================\n",
      "STARTING BACKTEST WITH DNN-FM + ANALYST SIGNALS\n",
      "============================================================\n",
      "\n",
      "[1/52] Date: 2020-01-31\n",
      "  Window: 2005-01-31 to 2019-12-31\n",
      "  Buys: 52 | Sells: 19 | Assets w/ data: 41\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross: -0.09676 | Turnover: 1.2218 | TC: 0.001104 | Net: -0.09786\n",
      "  MV   - Gross: -0.09454 | Turnover: 1.2328 | TC: 0.001116 | Net: -0.09566\n",
      "  MSR  - Gross: -0.08244 | Turnover: 1.3809 | TC: 0.001267 | Net: -0.08371\n",
      "\n",
      "[2/52] Date: 2020-02-29\n",
      "  Window: 2005-02-28 to 2020-01-31\n",
      "  Buys: 35 | Sells: 38 | Assets w/ data: 38\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross: -0.12752 | Turnover: 1.8583 | TC: 0.001621 | Net: -0.12914\n",
      "  MV   - Gross: -0.13473 | Turnover: 1.8639 | TC: 0.001613 | Net: -0.13634\n",
      "  MSR  - Gross: -0.07138 | Turnover: 2.1716 | TC: 0.002017 | Net: -0.07340\n",
      "\n",
      "[3/52] Date: 2020-03-31\n",
      "  Window: 2005-03-31 to 2020-02-29\n",
      "  Buys: 132 | Sells: 42 | Assets w/ data: 87\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.09035 | Turnover: 1.9281 | TC: 0.002102 | Net:  0.08824\n",
      "  MV   - Gross:  0.08893 | Turnover: 1.9329 | TC: 0.002105 | Net:  0.08683\n",
      "  MSR  - Gross:  0.08451 | Turnover: 2.0375 | TC: 0.002210 | Net:  0.08231\n",
      "\n",
      "[4/52] Date: 2020-04-30\n",
      "  Window: 2005-04-30 to 2020-03-31\n",
      "  Buys: 107 | Sells: 96 | Assets w/ data: 95\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.02283 | Turnover: 1.7299 | TC: 0.001769 | Net:  0.02106\n",
      "  MV   - Gross:  0.02830 | Turnover: 1.7437 | TC: 0.001793 | Net:  0.02651\n",
      "  MSR  - Gross:  0.03547 | Turnover: 2.0318 | TC: 0.002104 | Net:  0.03337\n",
      "\n",
      "[5/52] Date: 2020-05-31\n",
      "  Window: 2005-05-31 to 2020-04-30\n",
      "  Buys: 44 | Sells: 79 | Assets w/ data: 50\n",
      "  Running Deep Learning Regression...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x13e865da0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross: -0.02487 | Turnover: 2.4708 | TC: 0.002409 | Net: -0.02728\n",
      "  MV   - Gross: -0.02448 | Turnover: 2.4712 | TC: 0.002411 | Net: -0.02689\n",
      "  MSR  - Gross: -0.02307 | Turnover: 2.8160 | TC: 0.002751 | Net: -0.02582\n",
      "\n",
      "[6/52] Date: 2020-06-30\n",
      "  Window: 2005-06-30 to 2020-05-31\n",
      "  Buys: 38 | Sells: 72 | Assets w/ data: 46\n",
      "  Running Deep Learning Regression...\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x13e9d7740> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.05197 | Turnover: 2.5569 | TC: 0.002690 | Net:  0.04928\n",
      "  MV   - Gross:  0.05434 | Turnover: 2.5573 | TC: 0.002696 | Net:  0.05164\n",
      "  MSR  - Gross:  0.07353 | Turnover: 3.0184 | TC: 0.003240 | Net:  0.07029\n",
      "\n",
      "[7/52] Date: 2020-07-31\n",
      "  Window: 2005-07-31 to 2020-06-30\n",
      "  Buys: 75 | Sells: 63 | Assets w/ data: 72\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.03021 | Turnover: 2.0835 | TC: 0.002146 | Net:  0.02806\n",
      "  MV   - Gross:  0.03688 | Turnover: 2.1211 | TC: 0.002199 | Net:  0.03468\n",
      "  MSR  - Gross:  0.05039 | Turnover: 2.4475 | TC: 0.002571 | Net:  0.04782\n",
      "\n",
      "[8/52] Date: 2020-08-31\n",
      "  Window: 2005-08-31 to 2020-07-31\n",
      "  Buys: 27 | Sells: 40 | Assets w/ data: 20\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross: -0.00012 | Turnover: 2.2673 | TC: 0.002267 | Net: -0.00239\n",
      "  MV   - Gross:  0.00091 | Turnover: 2.2570 | TC: 0.002259 | Net: -0.00135\n",
      "  MSR  - Gross:  0.00560 | Turnover: 2.5479 | TC: 0.002562 | Net:  0.00304\n",
      "\n",
      "[9/52] Date: 2020-09-30\n",
      "  Window: 2005-09-30 to 2020-08-31\n",
      "  Buys: 61 | Sells: 50 | Assets w/ data: 46\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross: -0.00885 | Turnover: 1.9704 | TC: 0.001953 | Net: -0.01081\n",
      "  MV   - Gross: -0.00874 | Turnover: 1.9850 | TC: 0.001968 | Net: -0.01071\n",
      "  MSR  - Gross: -0.00815 | Turnover: 2.3815 | TC: 0.002362 | Net: -0.01051\n",
      "\n",
      "[10/52] Date: 2020-10-31\n",
      "  Window: 2005-10-31 to 2020-09-30\n",
      "  Buys: 45 | Sells: 49 | Assets w/ data: 38\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.09494 | Turnover: 2.3005 | TC: 0.002519 | Net:  0.09242\n",
      "  MV   - Gross:  0.08623 | Turnover: 2.3402 | TC: 0.002542 | Net:  0.08369\n",
      "  MSR  - Gross:  0.02076 | Turnover: 3.1706 | TC: 0.003236 | Net:  0.01753\n",
      "\n",
      "[11/52] Date: 2020-11-30\n",
      "  Window: 2005-11-30 to 2020-10-31\n",
      "  Buys: 49 | Sells: 61 | Assets w/ data: 55\n",
      "  Running Deep Learning Regression...\n",
      "  Computing GMV weights...\n",
      "  Computing MV weights...\n",
      "  Computing MSR weights...\n",
      "  GMV  - Gross:  0.01239 | Turnover: 2.3453 | TC: 0.002374 | Net:  0.01002\n",
      "  MV   - Gross:  0.01130 | Turnover: 2.3749 | TC: 0.002402 | Net:  0.00889\n",
      "  MSR  - Gross: -0.00444 | Turnover: 2.9810 | TC: 0.002968 | Net: -0.00741\n",
      "\n",
      "[12/52] Date: 2020-12-31\n",
      "  Window: 2005-12-31 to 2020-11-30\n",
      "  Buys: 22 | Sells: 53 | Assets w/ data: 33\n",
      "  Running Deep Learning Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3467\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3464\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3466\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:1185\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:1056\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1053\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1060\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1062\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1063\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1071\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1072\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:864\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    856\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    857\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    862\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    863\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    869\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:749\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[32m    748\u001b[39m head = \u001b[38;5;28mself\u001b[39m.prepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m.long_header)\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m records = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    751\u001b[39m frames = []\n\u001b[32m    752\u001b[39m skipped = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:851\u001b[39m, in \u001b[36mVerboseTB.get_records\u001b[39m\u001b[34m(self, etb, context, tb_offset)\u001b[39m\n\u001b[32m    845\u001b[39m         FIs.append(\n\u001b[32m    846\u001b[39m             FrameInfo(\n\u001b[32m    847\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mRaw frame\u001b[39m\u001b[33m\"\u001b[39m, filename, lineno, frame, code, context=context\n\u001b[32m    848\u001b[39m             )\n\u001b[32m    849\u001b[39m         )\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FIs\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m res = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFrameInfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[tb_offset:]\n\u001b[32m    852\u001b[39m res2 = [FrameInfo._from_stack_data_FrameInfo(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:597\u001b[39m, in \u001b[36mFrameInfo.stack_data\u001b[39m\u001b[34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[39m\n\u001b[32m    594\u001b[39m     frame, lineno = frame_and_lineno(x)\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frame.f_code, lineno\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m collapse_repeated(\n\u001b[32m    598\u001b[39m     stack,\n\u001b[32m    599\u001b[39m     mapper=mapper,\n\u001b[32m    600\u001b[39m     collapser=RepeatedFrames,\n\u001b[32m    601\u001b[39m     key=_frame_key,\n\u001b[32m    602\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:83\u001b[39m, in \u001b[36mcollapse_repeated\u001b[39m\u001b[34m(lst, collapser, mapper, key)\u001b[39m\n\u001b[32m     81\u001b[39m original_group, highlighted_group = \u001b[38;5;28mzip\u001b[39m(*group)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_highlighted:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(mapper, original_group)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     85\u001b[39m     keyed_group, _ = \u001b[38;5;28mzip\u001b[39m(*highlighted_group)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:587\u001b[39m, in \u001b[36mFrameInfo.stack_data.<locals>.mapper\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmapper\u001b[39m(f):\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:551\u001b[39m, in \u001b[36mFrameInfo.__init__\u001b[39m\u001b[34m(self, frame_or_tb, options)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    548\u001b[39m         frame_or_tb: Union[FrameType, TracebackType],\n\u001b[32m    549\u001b[39m         options: Optional[Options] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    550\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[38;5;28mself\u001b[39m.executing = \u001b[43mSource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecuting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_or_tb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m     frame, \u001b[38;5;28mself\u001b[39m.lineno = frame_and_lineno(frame_or_tb)\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/executing/executing.py:224\u001b[39m, in \u001b[36mSource.executing\u001b[39m\u001b[34m(cls, frame_or_tb)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    223\u001b[39m     node = stmts = decorator = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     source = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfor_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m     tree = source.tree\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tree:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/executing/executing.py:143\u001b[39m, in \u001b[36mSource.for_frame\u001b[39m\u001b[34m(cls, frame, use_cache)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfor_frame\u001b[39m(\u001b[38;5;28mcls\u001b[39m, frame, use_cache=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# type: (types.FrameType, bool) -> \"Source\"\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    Returns the `Source` object corresponding to the file the frame is executing in.\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfor_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_code\u001b[49m\u001b[43m.\u001b[49m\u001b[43mco_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf_globals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/executing/executing.py:172\u001b[39m, in \u001b[36mSource.for_filename\u001b[39m\u001b[34m(cls, filename, module_globals, use_cache)\u001b[39m\n\u001b[32m    169\u001b[39m     linecache.cache[filename] = entry \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    170\u001b[39m     lines = get_lines()\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_for_filename_and_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/executing/executing.py:183\u001b[39m, in \u001b[36mSource._for_filename_and_lines\u001b[39m\u001b[34m(cls, filename, lines)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m result = source_cache[(filename, lines)] = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/executing/executing.py:128\u001b[39m, in \u001b[36mSource.__init__\u001b[39m\u001b[34m(self, filename, lines)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m ast.walk(\u001b[38;5;28mself\u001b[39m.tree):\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_child_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnhancedAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEnhancedAST\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m lineno \u001b[38;5;129;01min\u001b[39;00m node_linenos(node):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/ast.py:278\u001b[39m, in \u001b[36miter_child_nodes\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m field\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(field, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m field:\n\u001b[32m    279\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, AST):\n\u001b[32m    280\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../green cleaned.csv', dtype={'ncusip': 'string'})\n",
    "df['ret_fwd_1'] = df.groupby('permno')['ret_excess'].shift(-1)\n",
    "\n",
    "data_f=pd.read_csv('F-F_Research_Data_Factors.csv',sep=',')\n",
    "data_f['Date']=pd.to_datetime(data_f['Date'], format=\"%Y%m\")\n",
    "data_f['Date']=data_f['Date']+pd.offsets.MonthEnd(0)\n",
    "data_f = data_f.set_index('Date')\n",
    "data_f = data_f[['Mkt-RF', 'SMB', 'HML', 'RF']].astype(float)\n",
    "\n",
    "# Run backtest with yearly signals\n",
    "results_df, metrics, results_df_2, metrics_2, results_df_3, metrics_3= backtest_dnn_yearly(\n",
    "    df,\n",
    "    test_start_date='2020-01-31',\n",
    "    test_end_date='2024-04-30',\n",
    "    lookback_window=180,\n",
    "    transaction_cost=0.001,\n",
    "    rec_changes_path='../examples/monthly_mean_recommendations_decay.csv',\n",
    "    data_factor=data_f,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n GMV\")\n",
    "print(f\"\\nSharpe Ratio: {metrics['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MV\")\n",
    "print(f\"\\nSharpe Ratio: {metrics_2['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics_2['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics_2['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics_2['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MSR\")\n",
    "print(f\"\\nSharpe Ratio: {metrics_3['sharpe_ratio']:.4f}\")\n",
    "print(f\"Annualized Sharpe Ratio: {metrics_3['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Total Return: {metrics_3['total_return']:.4f}\")\n",
    "print(f\"Average Turnover: {metrics_3['avg_turnover']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954585e-e8f6-4347-b6a3-78f6a1acd570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

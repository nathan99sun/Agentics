{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed0744e-864b-4b29-8373-479caf0033e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 15:17:17.330031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import get_context\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "from DNNFM_functions import *\n",
    "from comp_m_functions import static_factor_obs, cov_e_poet\n",
    "\n",
    "def round(x):\n",
    "    return int(Decimal(x).to_integral_value(rounding=ROUND_HALF_UP))\n",
    "\n",
    "#---------------- Main ----------------\n",
    "\n",
    "def DNN_FM_main(data, data_factor, architecture=1, const_err_cov=2.5, use_CV_err=False, eval_type='frob'):\n",
    "\n",
    "    data_dm = data\n",
    "    data_factor_dm = data_factor\n",
    "    \n",
    "    # Obtain optimal tuning parameter based on cross-validation or pre-specified values\n",
    "    opt = opt_hyper_parameters(data=data, data_F=data_factor, architecture=architecture, const_err_cov=const_err_cov, \n",
    "                               use_CV_err=use_CV_err, eval_type=eval_type)\n",
    "    # Compute DNN-FM based on optimal hyper-parameters\n",
    "    res_DNN_FM = DNN_FM_core(data=data_dm, data_factor=data_factor_dm, architecture=architecture, opt=opt)\n",
    "\n",
    "    # c_err_min = DNN_FM_cov_e_cmin(data=data_dm, data_factor=data_factor_dm, DNN_model=res_DNN_FM['neural_net'])\n",
    "\n",
    "    print(opt['const_err_cov'])\n",
    "    res_DNN_FM_cov = DNN_FM_cov(data=data_dm, data_factor=data_factor_dm, DNN_model=res_DNN_FM['neural_net'], \n",
    "                                c_err_cov=opt['const_err_cov'], check_eig=False)\n",
    "    \n",
    "    res_DNN_FM.update(res_DNN_FM_cov)\n",
    "\n",
    "    return res_DNN_FM\n",
    "\n",
    "\n",
    "#---------------- Functions ----------------\n",
    "\n",
    "# Core function for creating Neural Network\n",
    "\n",
    "def DNN_FM_core(data, data_factor, architecture, opt):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "    num_f = data_factor.shape[1]\n",
    "\n",
    "    # Create neural network specifications\n",
    "\n",
    "    if architecture == 1:\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 1\n",
    "        d_rate = 0.2\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 512\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 2:\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.1\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 256\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "        \n",
    "    elif architecture == 3:\n",
    "        \n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 1\n",
    "        d_rate = 0.0\n",
    "\n",
    "        if inter_layer:\n",
    "            c_temp = 2\n",
    "        else:\n",
    "            c_temp = 1\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+c_temp)\n",
    "        activation_functions = [np.nan] * (n_layers+c_temp)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            hidden_layer_s[mm] = 512\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        if inter_layer:\n",
    "            dropout_rates_tr[len(dropout_rates_tr)-1] = d_rate\n",
    "            activation_functions[len(activation_functions)-2] = 'relu'\n",
    "            activation_functions[len(activation_functions)-1] = 'relu'\n",
    "        else:\n",
    "            activation_functions[len(activation_functions)-1] = None\n",
    "    \n",
    "    elif architecture == 4:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.0\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "    \n",
    "    elif architecture == 5:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "\n",
    "    elif architecture == 6:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.0\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 7:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 8:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 9:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 32\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 16\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 8\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "    elif architecture == 10:\n",
    "\n",
    "        inter_layer = False\n",
    "\n",
    "        n_layers = 3\n",
    "        d_rate = 0.2\n",
    "\n",
    "        hidden_layer_s = [np.nan] * n_layers\n",
    "        dropout_rates_tr = [np.nan] * (n_layers+1)\n",
    "        activation_functions = [np.nan] * (n_layers+1)\n",
    "        dropout_rates_tr[0] = d_rate\n",
    "        for mm in range(0, n_layers):\n",
    "            if mm == 0:\n",
    "                hidden_layer_s[mm] = 256\n",
    "            elif mm == 1:\n",
    "                hidden_layer_s[mm] = 128\n",
    "            elif mm == 2:\n",
    "                hidden_layer_s[mm] = 64\n",
    "            dropout_rates_tr[mm+1] = d_rate\n",
    "            activation_functions[mm] = 'relu'\n",
    "        \n",
    "        activation_functions[len(activation_functions)-1] = None\n",
    "\n",
    "\n",
    "    # Optimization options\n",
    "    optimizer = 'Adam'\n",
    "    max_iter = 2000                 # maximum number of iterations\n",
    "    max_iter_nc = 50                # maximum number of iterations early stopping\n",
    "    \n",
    "    split_ratio = 0.3               # split ratio for training and validation set\n",
    "    batch_s = 256                   # batch size\n",
    "    use_bias = False                # include bias in neural net, if true\n",
    "\n",
    "    # Define early stopping criterion\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=max_iter_nc, mode='min')\n",
    "\n",
    "    learning_rate = opt['learning_rate']\n",
    "    reg_par_w = opt['reg_par_w']          # regularization parameter for weights\n",
    "    reg_par_b = opt['reg_par_b']          # regularization parameter for bias\n",
    "    el_r_pro = opt['el_r_pro']            # split between elastic net and lasso: 1 - only l1 norm\n",
    "\n",
    "    \"\"\"\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    learning_rate,\n",
    "    decay_steps=1,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sparse neural network and compile it\n",
    "    neural_net = sparse_nn(hidden_layer_s, activation_functions, dropout_rates_tr, num_s, num_f,\n",
    "                reg_par_w, reg_par_b, el_r_pro, max_iter_nc, max_iter, optimizer, learning_rate, use_bias, inter_layer)\n",
    "\n",
    "    neural_net.build_neural_network()\n",
    "    # Compile and fit model\n",
    "    neural_net.compile_nn()\n",
    "\n",
    "    fit_nn = neural_net.model.fit(data_factor, data, epochs=max_iter,\n",
    "                        validation_split=split_ratio, shuffle=True, batch_size=batch_s,\n",
    "                        callbacks=early_stopping, verbose=0)\n",
    "    \n",
    "    # Compute market sensitivity\n",
    "    with tf.GradientTape() as tape:\n",
    "        data_factor_ts = tf.convert_to_tensor(data_factor[-1,:].reshape(1,-1), dtype=tf.float32)\n",
    "        tape.watch(data_factor_ts)\n",
    "        y_pred = neural_net.model(data_factor_ts)\n",
    "\n",
    "    market_sens = tape.jacobian(y_pred, data_factor_ts)[-1,:,-1,:].numpy() \n",
    "\n",
    "    # Store for analysis\n",
    "    market_return_t = data_factor_ts.numpy()[-1, :].reshape(1,-1)\n",
    "    market_sensitivity = (market_sens, market_return_t)\n",
    "\n",
    "    res = {'neural_net': neural_net, 'opt': opt, 'market_sensitivity': market_sensitivity}\n",
    "    return res\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "def DNN_FM_cov(data, data_factor, DNN_model, c_err_cov, check_eig=False):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "\n",
    "    y_hat_nn = DNN_model.model(data_factor).numpy()\n",
    "    resd_nn = data - y_hat_nn\n",
    "\n",
    "    sig_hat_e = thres_resd_new(resd_nn, c_err_cov, num_s, num_n)\n",
    "    cov_f_nnet = np.cov(y_hat_nn.T)\n",
    "\n",
    "    if not check_eig:\n",
    "                \n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "    else:\n",
    "\n",
    "        cond = True\n",
    "        const_c = 0\n",
    "        while cond:\n",
    "\n",
    "            sig_hat_e = thres_resd_new(resd_nn, c_err_cov+const_c, num_s, num_n)\n",
    "\n",
    "            cond = (round(min(np.linalg.eig(sig_hat_e)[0]),2) < 0.01) or (np.linalg.cond(sig_hat_e) > num_s*10)\n",
    "            const_c+=0.01\n",
    "        \n",
    "        sigma_y_nnet = cov_f_nnet + sig_hat_e\n",
    "\n",
    "    inv_sigma_y_nnet = sig_inv_f_nnet(cov_f_nnet, sig_hat_e)\n",
    "\n",
    "    res = {'sigma_hat': sigma_y_nnet, 'sigma_f_hat': cov_f_nnet, 'sigma_e_hat': sig_hat_e, \n",
    "           'inv_sigma_hat': inv_sigma_y_nnet}\n",
    "\n",
    "    return res\n",
    "\n",
    "def DNN_FM_cov_e_cmin(data, data_factor, DNN_model):\n",
    "\n",
    "    num_n, num_s = data.shape\n",
    "\n",
    "    y_hat_nn = DNN_model.model(data_factor).numpy()\n",
    "    resd_nn = data - y_hat_nn\n",
    "\n",
    "    num_n, num_s = resd_nn.shape\n",
    "\n",
    "    f = lambda c: mineig_cov_e(resd=resd_nn, c_err_cov=c, num_s=num_s, num_n=num_n)\n",
    "\n",
    "    if (f(50) * f(-50) < 0):\n",
    "        r = brentq(f, -50, 50)\n",
    "        return max(0, r)\n",
    "    else:\n",
    "        c = 0\n",
    "        return c\n",
    "\n",
    "def mineig_cov_e(resd, c_err_cov, num_s, num_n):\n",
    "\n",
    "    return min(np.linalg.eig(thres_resd_new(resd, c_err_cov, num_s, num_n))[0])\n",
    "\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "# Function for determining optimal regularization and learning rate based on cross-validation or fixed\n",
    "def opt_hyper_parameters(data, data_F, architecture, const_err_cov, use_CV_err, eval_type):\n",
    "    \n",
    "    parallel = False\n",
    "\n",
    "    if architecture == 1 or architecture == 5 or architecture == 7:\n",
    "        reg_par_w = 0.0005           # regularization parameter for weights\n",
    "        reg_par_b = 0.0005           # regularization parameter for bias\n",
    "    elif architecture == 9 or architecture == 10:\n",
    "        reg_par_w = 0.005           # regularization parameter for weights\n",
    "        reg_par_b = 0.005           # regularization parameter for bias\n",
    "    else:\n",
    "        reg_par_w = 0.0             # regularization parameter for weights\n",
    "        reg_par_b = 0.0             # regularization parameter for bias\n",
    "    el_r_pro = 1                    # split between elastic net and lasso: 1 - only l1 norm\n",
    "    # Alternative specification for the learning rate\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    if use_CV_err:\n",
    "\n",
    "        range_cov_err = np.arange(0, const_err_cov+0.6, 0.1)\n",
    "\n",
    "        \"\"\"\n",
    "        opt = block_cv(data=data, data_F=data_F, architecture=architecture, \n",
    "                       reg_par=reg_par_w, lr=learning_rate, range_cov_err=range_cov_err, \n",
    "                       eval_type=eval_type, test_size=10, parallel=parallel)\n",
    "        \"\"\"\n",
    "        \n",
    "        opt = cv_split(data=data, data_F=data_F, architecture=architecture, \n",
    "                       reg_par=reg_par_w, lr=learning_rate, range_cov_err=range_cov_err, \n",
    "                       eval_type=eval_type)\n",
    "\n",
    "    else:\n",
    "\n",
    "        opt = {'learning_rate': learning_rate, 'reg_par_w': reg_par_w, \n",
    "            'reg_par_b': reg_par_b, 'el_r_pro': el_r_pro, 'const_err_cov': const_err_cov}\n",
    "    \n",
    "    return opt\n",
    "        \n",
    "#-------------------------------------------\n",
    "\n",
    "# Function that determines hyperparameters based on block cross-validation\n",
    "\n",
    "def block_cv(data, data_F, architecture, reg_par, lr, range_cov_err, eval_type, min_train_ratio=0.8, test_size=5, parallel=False):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    T, p = data.shape\n",
    "    train_size = int(np.floor(min_train_ratio * T)) # Get training size\n",
    "    numBlocks = int(np.floor((T - train_size) / test_size)) # Compute number of blocks\n",
    "    \n",
    "    # Show error message if test size is too large\n",
    "    assert numBlocks > 1, f\"Test size is too large\"\n",
    "\n",
    "    # Predefine dictionary for options\n",
    "    opt = {'learning_rate': lr, 'reg_par_w': reg_par, 'reg_par_b': reg_par, 'el_r_pro': 1}\n",
    "    \n",
    "    res_mat = np.empty((numBlocks,len(range_cov_err)))\n",
    "    res_mat[:] = np.nan\n",
    "\n",
    "    for bl in range(1, numBlocks+1):\n",
    "        idxTrainX = list(range((bl-1)*test_size,train_size+(bl-1)*test_size))\n",
    "\n",
    "        if bl == numBlocks:\n",
    "            idxTestX = list(range(train_size+(bl-1)*test_size,T))\n",
    "        else:\n",
    "            idxTestX = list(range(train_size+(bl-1)*test_size,train_size+bl*test_size))\n",
    "\n",
    "        \n",
    "        data_ntrain, data_mean, data_std = normalize_dat(data[idxTrainX,:])\n",
    "        data_F_ntrain, data_F_mean, data_F_std = normalize_dat(data_F[idxTrainX,:])    \n",
    "\n",
    "        test_res = np.empty((len(idxTestX),len(range_cov_err)))\n",
    "        test_res[:] = np.nan\n",
    "        \n",
    "        # if bl == 1:\n",
    "        res_DNN_FM = DNN_FM_core(data_ntrain, data_F_ntrain, architecture, opt)\n",
    "        neural_net = res_DNN_FM['neural_net']\n",
    "\n",
    "\n",
    "        for idx_t in range(1, len(idxTestX)+1):\n",
    "\n",
    "            ind_test_temp = idxTrainX[idx_t:]+idxTestX[:idx_t]\n",
    "\n",
    "            data_ntest, data_mean, data_std = normalize_dat(data[ind_test_temp,:])\n",
    "            data_F_ntest, data_F_mean, data_F_std = normalize_dat(data_F[ind_test_temp,:])\n",
    "\n",
    "            y_hat = neural_net.model(data_F_ntrain).numpy()\n",
    "            resd_nn = data_ntrain - y_hat\n",
    "\n",
    "            num_n, num_s = resd_nn.shape\n",
    "\n",
    "            sigma_test = cov_sfm(data_ntest - neural_net.model(data_F_ntest).numpy())\n",
    "\n",
    "            \"\"\"\n",
    "            # define the number of worker processes to use\n",
    "            num_processes = 2\n",
    "\n",
    "            combined_list = [(resd_nn, sigma_test, eval_type, const_err_item) for const_err_item in range_cov_err]\n",
    "\n",
    "            if parallel:\n",
    "                # create a pool of worker processes\n",
    "                with get_context(\"spawn\").Pool(processes=num_processes) as pool:\n",
    "                    ret = pool.imap(err_cv_core, combined_list)\n",
    "                    pool.close()\n",
    "                    pool.join()\n",
    "            else:\n",
    "                ret = map(err_cv_core, combined_list)\n",
    "\n",
    "            for indx, err_measure in enumerate(ret):\n",
    "                test_res[idx_t-1,indx] = err_measure\n",
    "            \"\"\"\n",
    "\n",
    "            # sigma_test = cov_sfm(data_ntest - neural_net.model(data_F_ntest).numpy())\n",
    "\n",
    "            rate_thres = np.sqrt((np.log(num_s))/num_n)\n",
    "            sig_e_samp = np.cov(resd_nn.T)\n",
    "            thet_par = np.empty((num_s, num_s))\n",
    "            thet_par[:] = np.nan\n",
    "\n",
    "            for ii in range(0, num_s):\n",
    "                for jj in range(0, num_s):\n",
    "                    thet_par[ii, jj] = np.mean(np.abs(resd_nn[:, ii] * resd_nn[:, jj] - sig_e_samp[ii, jj]))\n",
    "\n",
    "            sig_e_diag = np.diag(sig_e_samp)\n",
    "            \n",
    "            \"\"\"\n",
    "            sig_e_diag = np.diag(np.diag(sig_e_samp)**(0.5))\n",
    "            R = np.linalg.inv(sig_e_diag) @ sig_e_samp @ np.linalg.inv(sig_e_diag)\n",
    "            \"\"\"\n",
    "            \n",
    "            for c_idx in range(0,len(range_cov_err)):\n",
    "                lam = rate_thres * range_cov_err[c_idx] * thet_par\n",
    "                \n",
    "                \"\"\"\n",
    "                M = soft_t(R, lam)\n",
    "                M = M - np.diag(np.diag(M)) + np.eye(num_s)\n",
    "                sig_hat_e = sig_e_diag @ M @ sig_e_diag\n",
    "                \"\"\"\n",
    "                \n",
    "                sig_hat_e = soft_t(sig_e_samp, lam)\n",
    "                np.fill_diagonal(sig_hat_e, sig_e_diag)\n",
    "                \n",
    "                # sig_hat_e = thres_resd_new(resd_nn, range_cov_err[c_idx], num_s, num_n)\n",
    "\n",
    "                if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "                    test_res[idx_t-1,c_idx] = np.inf\n",
    "                else:\n",
    "                    if eval_type == 'frob':\n",
    "                        test_res[idx_t-1,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')**2\n",
    "                    elif eval_type == 'spec':\n",
    "                        test_res[idx_t-1,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord=2)**2\n",
    "\n",
    "            \n",
    "        res_mat[bl-1,:] = np.mean(test_res, axis=0)\n",
    "\n",
    "\n",
    "     \n",
    "    idx_opt = np.where(res_mat.mean(axis=0) == np.nanmin(res_mat.mean(axis=0)))\n",
    "\n",
    "    opt.update({'const_err_cov': range_cov_err[idx_opt][0]})\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "    return opt\n",
    "\n",
    "def err_cv_core(tuple_in):\n",
    "\n",
    "    resd_nn, sigma_test, eval_type, const_err_cov = tuple_in[0], tuple_in[1], tuple_in[2], tuple_in[3]\n",
    "\n",
    "    num_n, num_s = resd_nn.shape\n",
    "\n",
    "    st = time.time()\n",
    "    sig_hat_e = thres_resd_new(resd_nn, const_err_cov, num_s, num_n)\n",
    "    print(time.time()-st)\n",
    "\n",
    "    if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "        test_res = np.inf\n",
    "    else:\n",
    "        if eval_type == 'frob':\n",
    "            test_res = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')\n",
    "        elif eval_type == 'spec':\n",
    "            st1 = time.time()\n",
    "            test_res = np.linalg.norm(sig_hat_e - sigma_test, ord=2)\n",
    "            print(time.time()-st1)\n",
    "\n",
    "    return test_res\n",
    "\n",
    "def cv_split(data, data_F, architecture, reg_par, lr, range_cov_err, eval_type):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Predefine dictionary for options\n",
    "    opt = {'learning_rate': lr, 'reg_par_w': reg_par, 'reg_par_b': reg_par, 'el_r_pro': 1}\n",
    "\n",
    "    data_dm, _, _ = normalize_dat_sim(data)\n",
    "    data_F_dm, _, _ = normalize_dat_sim(data_F)    \n",
    "\n",
    "    res_DNN_FM = DNN_FM_core(data_dm, data_F_dm, architecture, opt)\n",
    "    neural_net = res_DNN_FM['neural_net']\n",
    "\n",
    "    y_hat = neural_net.model(data_F_dm).numpy()\n",
    "    resd_nn = data_dm - y_hat\n",
    "\n",
    "    n_folds = 10\n",
    "\n",
    "    res_mat = np.empty((n_folds,len(range_cov_err)))\n",
    "    res_mat[:] = np.nan\n",
    "\n",
    "    split_sample, _ = ts_train_test_split(resd_nn, n_folds, train_size=0.5)\n",
    "\n",
    "    for m_idx in range(0, n_folds):\n",
    "\n",
    "\n",
    "            resd_nn_s1 = split_sample[m_idx][0]\n",
    "            resd_nn_s2 = split_sample[m_idx][1]\n",
    "\n",
    "            num_n, num_s = resd_nn_s1.shape\n",
    "\n",
    "            sigma_test = cov_sfm(resd_nn_s2) # np.cov(resd_nn_s2.T) # \n",
    "\n",
    "            sig_e_samp, thet_par = thres_cov_resd_aux(resd_nn_s1, num_s)\n",
    "\n",
    "            for c_idx in range(0,len(range_cov_err)):\n",
    "                \n",
    "                sig_hat_e = thres_cov_resd(sig_e_samp, thet_par, range_cov_err[c_idx], num_s, num_n)\n",
    "\n",
    "                if min(np.linalg.eig(sig_hat_e)[0]) < 0:\n",
    "                    res_mat[m_idx,c_idx] = np.inf\n",
    "                else:\n",
    "                    if eval_type == 'frob':\n",
    "                        res_mat[m_idx,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord='fro')**2\n",
    "                    elif eval_type == 'spec':\n",
    "                        res_mat[m_idx,c_idx] = np.linalg.norm(sig_hat_e - sigma_test, ord=2)**2\n",
    "\n",
    "    idx_opt = np.where(res_mat.mean(axis=0) == np.nanmin(res_mat.mean(axis=0)))\n",
    "\n",
    "    opt.update({'const_err_cov': range_cov_err[idx_opt][0]})\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "\n",
    "    return opt\n",
    "\n",
    "def ts_train_test_split(X, n_folds = 5, train_size=0.5):\n",
    "\n",
    "    test_size = 1 - train_size\n",
    "    n_obs = X.shape[0]\n",
    "\n",
    "    size_split = n_obs - n_folds\n",
    "\n",
    "    n_train = round(size_split * train_size)\n",
    "    n_test = round(size_split * test_size)\n",
    "\n",
    "    split_t = (list(range(0, n_train)), list(range(n_train, n_train+n_test)))\n",
    "\n",
    "    split_sample = []\n",
    "    split_index = []\n",
    "\n",
    "    for jj in range(0, n_folds):\n",
    "        split_index.append(([el1 + jj for el1 in split_t[0]], [el2 + jj for el2 in split_t[1]]))\n",
    "        split_sample.append((X[split_index[jj][0],:], X[split_index[jj][1],:]))\n",
    "\n",
    "    return split_sample, split_index\n",
    "\n",
    "\n",
    "def gmv_weights(Theta_hat):\n",
    "    \"\"\"\n",
    "    Compute Global Minimum Variance (GMV) portfolio weights (Section 6.1).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # w* = (Θ 1_p) / (1_p' Θ 1_p)\n",
    "    numerator = Theta_hat @ ones_p\n",
    "    denominator = ones_p @ Theta_hat @ ones_p\n",
    "    \n",
    "    if np.abs(denominator) < 1e-10:\n",
    "        # Fallback to equal weights if precision matrix is near-singular\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = numerator / denominator\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "def mv_weights(Theta_hat, mu, target_return=0.01):\n",
    "    \"\"\"\n",
    "    Compute Mean-Variance portfolio weights with target return.\n",
    "    \n",
    "    Solves the constrained optimization:\n",
    "    min w' Sigma w  subject to  w' mu = target_return  and  w' 1 = 1\n",
    "    \n",
    "    Solution uses Lagrange multipliers with two constraints.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected returns\n",
    "    target_return : float\n",
    "        Target portfolio return (default: 0.01 = 1% monthly)\n",
    "    long_only : bool\n",
    "        If True, falls back to GMV if MV produces negative weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute key quantities\n",
    "    A = ones_p @ Theta_hat @ ones_p  # 1' Theta 1\n",
    "    B = ones_p @ Theta_hat @ mu       # 1' Theta mu  \n",
    "    C = mu @ Theta_hat @ mu           # mu' Theta mu\n",
    "    D = A * C - B * B                  # Determinant\n",
    "    \n",
    "    # Check for singularity\n",
    "    if np.abs(D) < 1e-10:\n",
    "        print('SINGULARITY')\n",
    "        # System is singular, use GMV instead\n",
    "        if np.abs(A) > 1e-10:\n",
    "            w_star = (Theta_hat @ ones_p) / A\n",
    "            return w_star\n",
    "        else:\n",
    "            return ones_p / p\n",
    "    \n",
    "    \n",
    "    # Compute Lagrange multipliers\n",
    "    lambda1 = (C - B * target_return) / D\n",
    "    lambda2 = (A * target_return - B) / D\n",
    "    \n",
    "    # Compute weights: w = lambda1 * Theta^{-1} 1 + lambda2 * Theta^{-1} mu\n",
    "    w_star = lambda1 * (Theta_hat @ ones_p) + lambda2 * (Theta_hat @ mu)\n",
    "    \n",
    "    return w_star\n",
    "\n",
    "def msr_weights(Theta_hat, mu):\n",
    "    \"\"\"\n",
    "    Compute Maximum Sharpe Ratio portfolio weights.\n",
    "    \n",
    "    The maximum Sharpe ratio portfolio solves:\n",
    "    max (w' mu) / sqrt(w' Sigma w)\n",
    "    \n",
    "    Solution (when mu represents excess returns):\n",
    "    w ∝ Sigma^{-1} mu = Theta mu\n",
    "    \n",
    "    Then normalize so that sum(w) = 1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Theta_hat : np.ndarray, shape (p, p)\n",
    "        Precision matrix (Sigma^{-1})\n",
    "    mu : np.ndarray, shape (p,)\n",
    "        Expected excess returns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_star : np.ndarray, shape (p,)\n",
    "        Portfolio weights (sum to 1)\n",
    "    \"\"\"\n",
    "    p = Theta_hat.shape[0]\n",
    "    ones_p = np.ones(p)\n",
    "    \n",
    "    # Compute unnormalized weights: w ∝ Theta mu\n",
    "    w_unnorm = Theta_hat @ mu\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    weight_sum = np.sum(w_unnorm)\n",
    "    \n",
    "    if np.abs(weight_sum) < 1e-10:\n",
    "        print('WARNING: Weight sum near zero, returning equal weights')\n",
    "        return ones_p / p\n",
    "    \n",
    "    w_star = w_unnorm / weight_sum\n",
    "    \n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7b474f-d8c0-4704-89a1-525bfc76f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Assuming DNN_FM_main, gmv_weights, mv_weights, msr_weights are imported\n",
    "# from your existing modules\n",
    "\n",
    "def train_logistic_regression(df, train_start, train_end, features=['mom12m', 'mve', 'bm']):\n",
    "    \"\"\"\n",
    "    Train logistic regression on historical data to predict positive returns.\n",
    "    \"\"\"\n",
    "    train_df = df[(df['datadate'] >= train_start) & (df['datadate'] <= train_end)].copy()\n",
    "    \n",
    "    # Create binary target: 1 if positive return, 0 otherwise\n",
    "    train_df['target'] = (train_df['ret_fwd_1'] > 0).astype(int)\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    train_df = train_df.dropna(subset=features + ['target'])\n",
    "    \n",
    "    # Prepare features\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df['target']\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Train logistic regression\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    log_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"  Training samples: {len(train_df)}\")\n",
    "    print(f\"  Training accuracy: {log_reg.score(X_train_scaled, y_train):.4f}\")\n",
    "    \n",
    "    return log_reg, scaler\n",
    "\n",
    "\n",
    "def select_stocks_with_logistic(df, predict_date, log_reg, scaler, \n",
    "                                 features=['mom12m', 'mve', 'bm'],\n",
    "                                 method='top_n', n_stocks=100, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Use trained logistic regression to select stocks for a given date.\n",
    "    \"\"\"\n",
    "    predict_df = df[df['datadate'] == predict_date].copy()\n",
    "    predict_df = predict_df.dropna(subset=features)\n",
    "    \n",
    "    if len(predict_df) == 0:\n",
    "        print(f\"  ⚠ No stocks with complete data on {predict_date}\")\n",
    "        return []\n",
    "    \n",
    "    # Prepare features for prediction\n",
    "    X_predict = predict_df[features]\n",
    "    X_predict_scaled = scaler.transform(X_predict)\n",
    "    \n",
    "    # Generate buy probabilities\n",
    "    predict_df['buy_probability'] = log_reg.predict_proba(X_predict_scaled)[:, 1]\n",
    "    \n",
    "    # Select stocks based on method\n",
    "    if method == 'top_n':\n",
    "        selected_df = predict_df.nlargest(n_stocks, 'buy_probability')\n",
    "    elif method == 'top_and_bottom':\n",
    "        top_n = predict_df.nlargest(n_stocks, 'buy_probability')\n",
    "        bottom_n = predict_df.nsmallest(n_stocks, 'buy_probability')\n",
    "        selected_df = pd.concat([top_n, bottom_n])\n",
    "    elif method == 'threshold':\n",
    "        selected_df = predict_df[predict_df['buy_probability'] >= threshold]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    selected_permnos = selected_df['permno'].tolist()\n",
    "    \n",
    "    print(f\"  Stocks evaluated: {len(predict_df)}\")\n",
    "    print(f\"  Stocks selected: {len(selected_permnos)}\")\n",
    "    print(f\"  Buy probability range: [{predict_df['buy_probability'].min():.4f}, \"\n",
    "          f\"{predict_df['buy_probability'].max():.4f}]\")\n",
    "    \n",
    "    return selected_permnos\n",
    "\n",
    "\n",
    "def integrated_backtest(df, \n",
    "                        test_start_date='2020-01-31', \n",
    "                        test_end_date='2024-11-30',\n",
    "                        logistic_train_years=15,\n",
    "                        logistic_features=['mom12m', 'mve', 'bm'],\n",
    "                        stock_selection_method='top_n',\n",
    "                        n_stocks=100,\n",
    "                        lookback_window=180,\n",
    "                        transaction_cost=0.001,\n",
    "                        data_factor=None,\n",
    "                        dnn_architecture=5,\n",
    "                        dnn_const_err_cov=2.5,\n",
    "                        mv_target_return=0.01,\n",
    "                        verbose=True):\n",
    "    \"\"\"\n",
    "    Integrated backtest combining logistic regression stock selection with DNN-FM optimization.\n",
    "    \n",
    "    Annual workflow:\n",
    "    1. Train logistic regression on past N years (every January)\n",
    "    2. Select stocks based on buy probability\n",
    "    3. Run DNN-FM on selected stocks monthly until next retrain\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with columns: permno, datadate, ret_fwd_1, mom12m, mve, bm\n",
    "    test_start_date : str\n",
    "        First date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    test_end_date : str\n",
    "        Last date for out-of-sample returns (format: 'YYYY-MM-DD')\n",
    "    logistic_train_years : int\n",
    "        Number of years to use for training logistic regression (default: 15)\n",
    "    logistic_features : list\n",
    "        Features for logistic regression (default: ['mom12m', 'mve', 'bm'])\n",
    "    stock_selection_method : str\n",
    "        'top_n', 'threshold', or 'top_and_bottom' (default: 'top_n')\n",
    "    n_stocks : int\n",
    "        Number of stocks to select (default: 100)\n",
    "    lookback_window : int\n",
    "        Number of months in rolling training window for DNN-FM (default: 180)\n",
    "    transaction_cost : float\n",
    "        Proportional transaction cost (default: 0.001 = 10 bps)\n",
    "    data_factor : pd.DataFrame\n",
    "        Factor data for DNN-FM model\n",
    "    dnn_architecture : int\n",
    "        Architecture choice for DNN-FM (default: 5)\n",
    "    dnn_const_err_cov : float\n",
    "        Error covariance constant for DNN-FM (default: 2.5)\n",
    "    mv_target_return : float\n",
    "        Target return for MV portfolio (default: 0.01 = 1% monthly)\n",
    "    verbose : bool\n",
    "        If True, prints detailed log at each time step\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_dict : dict\n",
    "        Dictionary with keys 'gmv', 'mv', 'msr', each containing:\n",
    "        {'results_df': DataFrame, 'metrics': dict}\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    df = df.copy()\n",
    "    if 'datadate' not in df.columns or 'permno' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'datadate' and 'permno' columns\")\n",
    "    df['datadate'] = pd.to_datetime(df['datadate'])\n",
    "    \n",
    "    # Get unique dates\n",
    "    all_dates = sorted(df['datadate'].unique())\n",
    "    \n",
    "    # Convert test dates to datetime\n",
    "    test_start_dt = pd.to_datetime(test_start_date)\n",
    "    test_end_dt = pd.to_datetime(test_end_date)\n",
    "    \n",
    "    # Find date indices\n",
    "    try:\n",
    "        test_start_idx = all_dates.index(test_start_dt)\n",
    "        test_end_idx = all_dates.index(test_end_dt)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(f\"Date not found in DataFrame: {e}\")\n",
    "    \n",
    "    if test_start_idx < lookback_window:\n",
    "        raise ValueError(f\"Not enough data for lookback window\")\n",
    "    \n",
    "    # Extract years that need logistic regression retraining\n",
    "    start_year = test_start_dt.year\n",
    "    end_year = test_end_dt.year\n",
    "    test_years = list(range(start_year, end_year + 1))\n",
    "    \n",
    "    # Portfolio types to compute\n",
    "    portfolio_types = ['gmv', 'mv', 'msr']\n",
    "    \n",
    "    # Storage for each portfolio type\n",
    "    results_storage = {ptype: {\n",
    "        'returns': [],\n",
    "        'dates': [],\n",
    "        'weights_list': [],\n",
    "        'turnover_list': [],\n",
    "        'gross_returns': [],\n",
    "        'prev_weights_dict': {},\n",
    "        'prev_oos_returns_dict': {},\n",
    "        'prev_gross_return': 0.0\n",
    "    } for ptype in portfolio_types}\n",
    "    \n",
    "    # Track current stock universe\n",
    "    current_permnos = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"INTEGRATED BACKTEST: LOGISTIC REGRESSION + DNN-FM\")\n",
    "        print(f\"Test Period: {test_start_date} to {test_end_date}\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # --- 2. Annual Logistic Retraining Loop ---\n",
    "    for year in test_years:\n",
    "        # Retrain logistic regression in January\n",
    "        retrain_date = pd.to_datetime(f'{year}-01-31')\n",
    "        \n",
    "        if retrain_date not in all_dates:\n",
    "            print(f\"\\n⚠ Warning: {retrain_date} not in dataset, skipping year {year}\")\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"YEAR {year}: RETRAINING LOGISTIC REGRESSION\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        # Define training window for logistic regression\n",
    "        train_end = pd.to_datetime(f'{year-1}-12-31')\n",
    "        train_start = pd.to_datetime(f'{year - logistic_train_years}-01-31')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Logistic training period: {train_start.strftime('%Y-%m-%d')} to \"\n",
    "                  f\"{train_end.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Train logistic regression\n",
    "        log_reg, scaler = train_logistic_regression(\n",
    "            df, train_start, train_end, features=logistic_features\n",
    "        )\n",
    "        \n",
    "        # Select stocks for this year\n",
    "        current_permnos = select_stocks_with_logistic(\n",
    "            df, retrain_date, log_reg, scaler,\n",
    "            features=logistic_features,\n",
    "            method=stock_selection_method,\n",
    "            n_stocks=n_stocks\n",
    "        )\n",
    "        \n",
    "        if len(current_permnos) == 0:\n",
    "            print(f\"  ⚠ No stocks selected for {year}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Determine date range for this year's strategy\n",
    "        year_start_date = retrain_date\n",
    "        \n",
    "        if year == end_year:\n",
    "            year_end_date = test_end_dt\n",
    "        else:\n",
    "            year_end_date = pd.to_datetime(f'{year}-12-31')\n",
    "            if year_end_date not in all_dates:\n",
    "                year_dates = [d for d in all_dates if d.year == year]\n",
    "                year_end_date = max(year_dates) if year_dates else year_start_date\n",
    "        \n",
    "        # For the first year, respect test_start_date\n",
    "        if year == start_year and test_start_dt > year_start_date:\n",
    "            year_start_date = test_start_dt\n",
    "        \n",
    "        try:\n",
    "            year_start_idx = all_dates.index(year_start_date)\n",
    "            year_end_idx = all_dates.index(year_end_date)\n",
    "        except ValueError as e:\n",
    "            print(f\"  ⚠ Date error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRunning monthly rebalancing from {year_start_date.strftime('%Y-%m-%d')} \"\n",
    "                  f\"to {year_end_date.strftime('%Y-%m-%d')}\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        # --- 3. Monthly Loop for DNN-FM ---\n",
    "        for t in range(year_start_idx, year_end_idx + 1):\n",
    "            current_date = all_dates[t]\n",
    "            \n",
    "            if t < lookback_window:\n",
    "                if verbose:\n",
    "                    print(f\"\\n[{current_date.strftime('%Y-%m-%d')}] \"\n",
    "                          f\"Skipping: insufficient lookback\")\n",
    "                continue\n",
    "            \n",
    "            # Define training window\n",
    "            window_start_date = all_dates[t - lookback_window]\n",
    "            window_end_date = all_dates[t - 1]\n",
    "            \n",
    "            # Filter data to selected stocks only\n",
    "            train_data = df[\n",
    "                (df['datadate'] >= window_start_date) & \n",
    "                (df['datadate'] <= window_end_date) &\n",
    "                (df['permno'].isin(current_permnos))\n",
    "            ]\n",
    "            \n",
    "            # Pivot returns\n",
    "            returns_pivot = train_data.pivot(\n",
    "                index='datadate', columns='permno', values='ret_fwd_1'\n",
    "            )\n",
    "            \n",
    "            # Reindex to ensure all dates\n",
    "            window_dates = all_dates[t - lookback_window : t]\n",
    "            returns_pivot = returns_pivot.reindex(index=window_dates)\n",
    "            \n",
    "            # Get factor data for this window\n",
    "            train_factor = data_factor.loc[window_start_date : window_end_date]\n",
    "            \n",
    "            # Filter assets with any NaNs\n",
    "            nan_assets = returns_pivot.columns[returns_pivot.isna().any()]\n",
    "            filtered_pivot = returns_pivot.drop(columns=nan_assets)\n",
    "            \n",
    "            current_assets = filtered_pivot.columns.tolist()\n",
    "            Y = filtered_pivot.values\n",
    "            n_train, p_current = Y.shape\n",
    "            \n",
    "            if verbose:\n",
    "                month_num = t - year_start_idx + 1\n",
    "                print(f\"\\n[Month {month_num}] {current_date.strftime('%Y-%m-%d')}\")\n",
    "                print(f\"  Assets: {p_current}/{len(current_permnos)} with complete data\")\n",
    "            \n",
    "            # Check validity\n",
    "            if n_train < lookback_window or p_current < 2:\n",
    "                if verbose:\n",
    "                    print(f\"  ⚠ Insufficient data, using previous weights\")\n",
    "                \n",
    "                new_weights_dict = {ptype: results_storage[ptype]['prev_weights_dict'].copy() \n",
    "                                   for ptype in portfolio_types}\n",
    "            else:\n",
    "                try:\n",
    "                    # Demean\n",
    "                    Y_bar = Y.mean(axis=0)\n",
    "                    Y_star = Y - Y_bar\n",
    "                    \n",
    "                    # Run DNN-FM\n",
    "                    if verbose:\n",
    "                        print(f\"  Running DNN-FM...\")\n",
    "                    F = train_factor.values.astype(float)\n",
    "                    res_nnet_fm = DNN_FM_main(Y_star, F, architecture=dnn_architecture, \n",
    "                                             const_err_cov=dnn_const_err_cov, \n",
    "                                             use_CV_err=False, eval_type='frob')\n",
    "                    Theta_hat = res_nnet_fm['inv_sigma_hat']\n",
    "                    \n",
    "                    # Compute weights for each portfolio type\n",
    "                    new_weights_dict = {}\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"  Computing portfolio weights...\")\n",
    "                    \n",
    "                    w_gmv = gmv_weights(Theta_hat)\n",
    "                    new_weights_dict['gmv'] = {\n",
    "                        asset: w_gmv[i] for i, asset in enumerate(current_assets)\n",
    "                    }\n",
    "                    \n",
    "                    w_mv = mv_weights(Theta_hat, Y_bar, target_return=mv_target_return)\n",
    "                    new_weights_dict['mv'] = {\n",
    "                        asset: w_mv[i] for i, asset in enumerate(current_assets)\n",
    "                    }\n",
    "                    \n",
    "                    w_msr = msr_weights(Theta_hat, Y_bar)\n",
    "                    new_weights_dict['msr'] = {\n",
    "                        asset: w_msr[i] for i, asset in enumerate(current_assets)\n",
    "                    }\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"  ✓ DNN-FM completed for GMV, MV, MSR\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"  ✗ Error: {e}\")\n",
    "                    new_weights_dict = {ptype: results_storage[ptype]['prev_weights_dict'].copy() \n",
    "                                       for ptype in portfolio_types}\n",
    "            \n",
    "            # Get OOS returns (common for all portfolio types)\n",
    "            oos_data = df[df['datadate'] == current_date]\n",
    "            oos_returns_series = oos_data.set_index('permno')['ret_fwd_1'].dropna()\n",
    "            oos_returns_dict = oos_returns_series.to_dict()\n",
    "            \n",
    "            # --- 4. Process Each Portfolio Type ---\n",
    "            for ptype in portfolio_types:\n",
    "                # Normalize weights\n",
    "                weights = new_weights_dict[ptype]\n",
    "                weight_sum = sum(weights.values())\n",
    "                if weight_sum > 1e-10:\n",
    "                    weights = {k: v/weight_sum for k, v in weights.items()}\n",
    "                else:\n",
    "                    weights = results_storage[ptype]['prev_weights_dict'].copy()\n",
    "                \n",
    "                # Common assets\n",
    "                common_assets = set(weights.keys()) & set(oos_returns_dict.keys())\n",
    "                \n",
    "                if len(common_assets) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Filter and renormalize\n",
    "                common_weights = {a: weights[a] for a in common_assets}\n",
    "                common_weight_sum = sum(common_weights.values())\n",
    "                if common_weight_sum > 1e-10:\n",
    "                    common_weights = {k: v/common_weight_sum for k, v in common_weights.items()}\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Gross return\n",
    "                gross_return = sum(\n",
    "                    common_weights[a] * oos_returns_dict[a] for a in common_assets\n",
    "                )\n",
    "                \n",
    "                if np.isnan(gross_return) or np.isinf(gross_return):\n",
    "                    continue\n",
    "                \n",
    "                # Transaction costs\n",
    "                prev_weights = results_storage[ptype]['prev_weights_dict']\n",
    "                prev_oos_returns = results_storage[ptype]['prev_oos_returns_dict']\n",
    "                prev_gross_ret = results_storage[ptype]['prev_gross_return']\n",
    "                \n",
    "                if len(prev_weights) > 0:\n",
    "                    adjusted_prev = {}\n",
    "                    for asset, prev_w in prev_weights.items():\n",
    "                        if asset in prev_oos_returns:\n",
    "                            prev_r = prev_oos_returns[asset]\n",
    "                            if abs(1 + prev_gross_ret) > 1e-6:\n",
    "                                adjusted_prev[asset] = prev_w * (1 + prev_r) / (1 + prev_gross_ret)\n",
    "                            else:\n",
    "                                adjusted_prev[asset] = 0.0\n",
    "                        else:\n",
    "                            if abs(1 + prev_gross_ret) > 1e-6:\n",
    "                                adjusted_prev[asset] = prev_w / (1 + prev_gross_ret)\n",
    "                            else:\n",
    "                                adjusted_prev[asset] = 0.0\n",
    "                    \n",
    "                    all_assets = set(adjusted_prev.keys()) | set(common_weights.keys())\n",
    "                    turnover = sum(\n",
    "                        abs(common_weights.get(a, 0.0) - adjusted_prev.get(a, 0.0))\n",
    "                        for a in all_assets\n",
    "                    )\n",
    "                    tc = transaction_cost * (1 + gross_return) * turnover\n",
    "                else:\n",
    "                    turnover = sum(abs(w) for w in common_weights.values())\n",
    "                    tc = transaction_cost * (1 + gross_return) * turnover\n",
    "                \n",
    "                net_return = gross_return - tc\n",
    "                \n",
    "                # Store results\n",
    "                results_storage[ptype]['returns'].append(net_return)\n",
    "                results_storage[ptype]['dates'].append(current_date)\n",
    "                results_storage[ptype]['weights_list'].append(common_weights.copy())\n",
    "                results_storage[ptype]['turnover_list'].append(turnover)\n",
    "                results_storage[ptype]['gross_returns'].append(gross_return)\n",
    "                \n",
    "                # Update state\n",
    "                results_storage[ptype]['prev_weights_dict'] = common_weights.copy()\n",
    "                results_storage[ptype]['prev_oos_returns_dict'] = {a: oos_returns_dict[a] for a in common_assets}\n",
    "                results_storage[ptype]['prev_gross_return'] = gross_return\n",
    "            \n",
    "            if verbose:\n",
    "                # Print summary for all portfolios\n",
    "                print(f\"  Portfolio Returns:\")\n",
    "                for ptype in portfolio_types:\n",
    "                    if len(results_storage[ptype]['returns']) > 0:\n",
    "                        last_idx = len(results_storage[ptype]['returns']) - 1\n",
    "                        gross_ret = results_storage[ptype]['gross_returns'][last_idx]\n",
    "                        net_ret = results_storage[ptype]['returns'][last_idx]\n",
    "                        to = results_storage[ptype]['turnover_list'][last_idx]\n",
    "                        tc = gross_ret - net_ret\n",
    "                        print(f\"    {ptype.upper()}: Gross={gross_ret:>7.4f} | TO={to:>5.3f} | \"\n",
    "                              f\"TC={tc:>7.5f} | Net={net_ret:>7.4f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"BACKTEST COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    # --- 5. Compile Results ---\n",
    "    results_dict = {}\n",
    "    \n",
    "    for ptype in portfolio_types:\n",
    "        portfolio_returns = results_storage[ptype]['returns']\n",
    "        portfolio_dates = results_storage[ptype]['dates']\n",
    "        portfolio_turnover_list = results_storage[ptype]['turnover_list']\n",
    "        portfolio_gross_returns = results_storage[ptype]['gross_returns']\n",
    "        \n",
    "        if len(portfolio_returns) == 0:\n",
    "            results_dict[ptype] = {\n",
    "                'results_df': pd.DataFrame(),\n",
    "                'metrics': {}\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'date': portfolio_dates,\n",
    "            'portfolio_return': portfolio_returns,\n",
    "            'portfolio_gross_return': portfolio_gross_returns,\n",
    "            'portfolio_turnover': portfolio_turnover_list\n",
    "        })\n",
    "        results_df['cumulative_return'] = (1 + results_df['portfolio_return']).cumprod() - 1\n",
    "        \n",
    "        # Compute metrics\n",
    "        mean_return = np.mean(portfolio_returns)\n",
    "        variance = np.var(portfolio_returns, ddof=1)\n",
    "        sharpe_ratio = mean_return / np.sqrt(variance) if variance > 0 else 0\n",
    "        \n",
    "        annual_return = mean_return * 12\n",
    "        annual_volatility = np.sqrt(variance * 12)\n",
    "        annual_sharpe = annual_return / annual_volatility if annual_volatility > 0 else 0\n",
    "        \n",
    "        overall_metrics = {\n",
    "            'mean_return': mean_return,\n",
    "            'variance': variance,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'annual_return': annual_return,\n",
    "            'annual_volatility': annual_volatility,\n",
    "            'annual_sharpe_ratio': annual_sharpe,\n",
    "            'total_return': results_df['cumulative_return'].iloc[-1],\n",
    "            'avg_turnover': np.mean(portfolio_turnover_list),\n",
    "            'n_periods': len(portfolio_returns)\n",
    "        }\n",
    "        \n",
    "        results_dict[ptype] = {\n",
    "            'results_df': results_df,\n",
    "            'metrics': overall_metrics\n",
    "        }\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0173714-8360-4647-b99f-b4ab978f90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../green cleaned.csv', dtype={'ncusip': 'string'})\n",
    "df['ret_fwd_1'] = (df.groupby('permno')['ret_excess'].shift(-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3214ecb5-84c1-4220-8bd7-88d7be06b616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INTEGRATED BACKTEST: LOGISTIC REGRESSION + DNN-FM\n",
      "Test Period: 2020-01-31 to 2024-04-30\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "YEAR 2020: RETRAINING LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Logistic training period: 2005-01-31 to 2019-12-31\n",
      "  Training samples: 89793\n",
      "  Training accuracy: 0.5590\n",
      "  Stocks evaluated: 497\n",
      "  Stocks selected: 100\n",
      "  Buy probability range: [0.5322, 0.5826]\n",
      "\n",
      "Running monthly rebalancing from 2020-01-31 to 2020-12-31\n",
      "======================================================================\n",
      "\n",
      "[Month 1] 2020-01-31\n",
      "  Assets: 64/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0991 | TO=1.380 | TC=0.00124 | Net=-0.1004\n",
      "    MV: Gross=-0.0976 | TO=1.387 | TC=0.00125 | Net=-0.0989\n",
      "    MSR: Gross=-0.0861 | TO=1.562 | TC=0.00143 | Net=-0.0876\n",
      "\n",
      "[Month 2] 2020-02-29\n",
      "  Assets: 64/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.1204 | TO=0.093 | TC=0.00008 | Net=-0.1205\n",
      "    MV: Gross=-0.1097 | TO=0.155 | TC=0.00014 | Net=-0.1098\n",
      "    MSR: Gross=-0.0728 | TO=0.205 | TC=0.00019 | Net=-0.0730\n",
      "\n",
      "[Month 3] 2020-03-31\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.1168 | TO=0.207 | TC=0.00023 | Net= 0.1166\n",
      "    MV: Gross= 0.1125 | TO=0.215 | TC=0.00024 | Net= 0.1123\n",
      "    MSR: Gross= 0.0977 | TO=0.330 | TC=0.00036 | Net= 0.0973\n",
      "\n",
      "[Month 4] 2020-04-30\n",
      "  Assets: 64/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0382 | TO=0.205 | TC=0.00021 | Net= 0.0380\n",
      "    MV: Gross= 0.0420 | TO=0.242 | TC=0.00025 | Net= 0.0418\n",
      "    MSR: Gross= 0.0740 | TO=0.247 | TC=0.00026 | Net= 0.0738\n",
      "\n",
      "[Month 5] 2020-05-31\n",
      "  Assets: 64/100 with complete data\n",
      "  Running DNN-FM...\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x13b8f77e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0122 | TO=0.113 | TC=0.00011 | Net=-0.0123\n",
      "    MV: Gross=-0.0126 | TO=0.107 | TC=0.00011 | Net=-0.0127\n",
      "    MSR: Gross=-0.0158 | TO=0.132 | TC=0.00013 | Net=-0.0159\n",
      "\n",
      "[Month 6] 2020-06-30\n",
      "  Assets: 62/100 with complete data\n",
      "  Running DNN-FM...\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x13c7f4e00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0623 | TO=0.079 | TC=0.00008 | Net= 0.0622\n",
      "    MV: Gross= 0.0667 | TO=0.081 | TC=0.00009 | Net= 0.0666\n",
      "    MSR: Gross= 0.0989 | TO=0.205 | TC=0.00023 | Net= 0.0987\n",
      "\n",
      "[Month 7] 2020-07-31\n",
      "  Assets: 62/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0641 | TO=0.130 | TC=0.00014 | Net= 0.0640\n",
      "    MV: Gross= 0.0671 | TO=0.126 | TC=0.00013 | Net= 0.0670\n",
      "    MSR: Gross= 0.0897 | TO=0.161 | TC=0.00018 | Net= 0.0895\n",
      "\n",
      "[Month 8] 2020-08-31\n",
      "  Assets: 62/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0014 | TO=0.187 | TC=0.00019 | Net= 0.0012\n",
      "    MV: Gross= 0.0018 | TO=0.200 | TC=0.00020 | Net= 0.0016\n",
      "    MSR: Gross= 0.0090 | TO=0.181 | TC=0.00018 | Net= 0.0088\n",
      "\n",
      "[Month 9] 2020-09-30\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0171 | TO=0.107 | TC=0.00011 | Net=-0.0172\n",
      "    MV: Gross=-0.0176 | TO=0.121 | TC=0.00012 | Net=-0.0177\n",
      "    MSR: Gross=-0.0377 | TO=0.154 | TC=0.00015 | Net=-0.0378\n",
      "\n",
      "[Month 10] 2020-10-31\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0825 | TO=0.148 | TC=0.00016 | Net= 0.0824\n",
      "    MV: Gross= 0.0805 | TO=0.153 | TC=0.00017 | Net= 0.0804\n",
      "    MSR: Gross= 0.0368 | TO=0.291 | TC=0.00030 | Net= 0.0365\n",
      "\n",
      "[Month 11] 2020-11-30\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0035 | TO=0.258 | TC=0.00026 | Net= 0.0033\n",
      "    MV: Gross= 0.0039 | TO=0.273 | TC=0.00027 | Net= 0.0036\n",
      "    MSR: Gross= 0.0005 | TO=0.339 | TC=0.00034 | Net= 0.0001\n",
      "\n",
      "[Month 12] 2020-12-31\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0062 | TO=0.073 | TC=0.00007 | Net= 0.0061\n",
      "    MV: Gross= 0.0068 | TO=0.074 | TC=0.00007 | Net= 0.0067\n",
      "    MSR: Gross= 0.0017 | TO=0.176 | TC=0.00018 | Net= 0.0015\n",
      "\n",
      "======================================================================\n",
      "YEAR 2021: RETRAINING LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Logistic training period: 2006-01-31 to 2020-12-31\n",
      "  Training samples: 89754\n",
      "  Training accuracy: 0.5589\n",
      "  Stocks evaluated: 497\n",
      "  Stocks selected: 100\n",
      "  Buy probability range: [0.5277, 0.5799]\n",
      "\n",
      "Running monthly rebalancing from 2021-01-31 to 2021-12-31\n",
      "======================================================================\n",
      "\n",
      "[Month 1] 2021-01-31\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0149 | TO=0.828 | TC=0.00082 | Net=-0.0157\n",
      "    MV: Gross=-0.0132 | TO=0.832 | TC=0.00082 | Net=-0.0140\n",
      "    MSR: Gross=-0.0282 | TO=1.018 | TC=0.00099 | Net=-0.0292\n",
      "\n",
      "[Month 2] 2021-02-28\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0583 | TO=0.139 | TC=0.00015 | Net= 0.0582\n",
      "    MV: Gross= 0.0614 | TO=0.139 | TC=0.00015 | Net= 0.0612\n",
      "    MSR: Gross= 0.0331 | TO=0.213 | TC=0.00022 | Net= 0.0329\n",
      "\n",
      "[Month 3] 2021-03-31\n",
      "  Assets: 66/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0245 | TO=0.124 | TC=0.00013 | Net= 0.0243\n",
      "    MV: Gross= 0.0229 | TO=0.138 | TC=0.00014 | Net= 0.0227\n",
      "    MSR: Gross= 0.0316 | TO=0.173 | TC=0.00018 | Net= 0.0314\n",
      "\n",
      "[Month 4] 2021-04-30\n",
      "  Assets: 66/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0082 | TO=0.078 | TC=0.00008 | Net= 0.0081\n",
      "    MV: Gross= 0.0133 | TO=0.093 | TC=0.00009 | Net= 0.0132\n",
      "    MSR: Gross=-0.0113 | TO=0.114 | TC=0.00011 | Net=-0.0114\n",
      "\n",
      "[Month 5] 2021-05-31\n",
      "  Assets: 66/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0256 | TO=0.102 | TC=0.00010 | Net= 0.0255\n",
      "    MV: Gross= 0.0172 | TO=0.106 | TC=0.00011 | Net= 0.0171\n",
      "    MSR: Gross= 0.0500 | TO=0.204 | TC=0.00021 | Net= 0.0498\n",
      "\n",
      "[Month 6] 2021-06-30\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0249 | TO=0.170 | TC=0.00017 | Net= 0.0247\n",
      "    MV: Gross= 0.0216 | TO=0.178 | TC=0.00018 | Net= 0.0215\n",
      "    MSR: Gross= 0.0354 | TO=0.163 | TC=0.00017 | Net= 0.0353\n",
      "\n",
      "[Month 7] 2021-07-31\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0070 | TO=0.096 | TC=0.00010 | Net= 0.0069\n",
      "    MV: Gross= 0.0051 | TO=0.110 | TC=0.00011 | Net= 0.0050\n",
      "    MSR: Gross= 0.0133 | TO=0.145 | TC=0.00015 | Net= 0.0132\n",
      "\n",
      "[Month 8] 2021-08-31\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0323 | TO=0.079 | TC=0.00008 | Net=-0.0323\n",
      "    MV: Gross=-0.0306 | TO=0.100 | TC=0.00010 | Net=-0.0306\n",
      "    MSR: Gross=-0.0388 | TO=0.109 | TC=0.00011 | Net=-0.0389\n",
      "\n",
      "[Month 9] 2021-09-30\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0553 | TO=0.080 | TC=0.00008 | Net= 0.0552\n",
      "    MV: Gross= 0.0496 | TO=0.106 | TC=0.00011 | Net= 0.0495\n",
      "    MSR: Gross= 0.0833 | TO=0.117 | TC=0.00013 | Net= 0.0832\n",
      "\n",
      "[Month 10] 2021-10-31\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0052 | TO=0.156 | TC=0.00016 | Net= 0.0051\n",
      "    MV: Gross= 0.0016 | TO=0.165 | TC=0.00017 | Net= 0.0014\n",
      "    MSR: Gross= 0.0225 | TO=0.141 | TC=0.00014 | Net= 0.0224\n",
      "\n",
      "[Month 11] 2021-11-30\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0675 | TO=0.084 | TC=0.00009 | Net= 0.0674\n",
      "    MV: Gross= 0.0699 | TO=0.088 | TC=0.00009 | Net= 0.0698\n",
      "    MSR: Gross= 0.0549 | TO=0.131 | TC=0.00014 | Net= 0.0548\n",
      "\n",
      "[Month 12] 2021-12-31\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0448 | TO=0.119 | TC=0.00011 | Net=-0.0449\n",
      "    MV: Gross=-0.0354 | TO=0.164 | TC=0.00016 | Net=-0.0356\n",
      "    MSR: Gross=-0.0755 | TO=0.109 | TC=0.00010 | Net=-0.0756\n",
      "\n",
      "======================================================================\n",
      "YEAR 2022: RETRAINING LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Logistic training period: 2007-01-31 to 2021-12-31\n",
      "  Training samples: 89722\n",
      "  Training accuracy: 0.5607\n",
      "  Stocks evaluated: 497\n",
      "  Stocks selected: 100\n",
      "  Buy probability range: [0.5367, 0.5843]\n",
      "\n",
      "Running monthly rebalancing from 2022-01-31 to 2022-12-31\n",
      "======================================================================\n",
      "\n",
      "[Month 1] 2022-01-31\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0195 | TO=0.781 | TC=0.00077 | Net=-0.0203\n",
      "    MV: Gross=-0.0185 | TO=0.795 | TC=0.00078 | Net=-0.0193\n",
      "    MSR: Gross=-0.0238 | TO=0.849 | TC=0.00083 | Net=-0.0247\n",
      "\n",
      "[Month 2] 2022-02-28\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0534 | TO=0.113 | TC=0.00012 | Net= 0.0533\n",
      "    MV: Gross= 0.0528 | TO=0.114 | TC=0.00012 | Net= 0.0526\n",
      "    MSR: Gross= 0.0565 | TO=0.161 | TC=0.00017 | Net= 0.0563\n",
      "\n",
      "[Month 3] 2022-03-31\n",
      "  Assets: 59/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0364 | TO=0.125 | TC=0.00012 | Net=-0.0365\n",
      "    MV: Gross=-0.0307 | TO=0.150 | TC=0.00015 | Net=-0.0308\n",
      "    MSR: Gross=-0.0598 | TO=0.113 | TC=0.00011 | Net=-0.0599\n",
      "\n",
      "[Month 4] 2022-04-30\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0063 | TO=0.169 | TC=0.00017 | Net= 0.0061\n",
      "    MV: Gross= 0.0079 | TO=0.187 | TC=0.00019 | Net= 0.0077\n",
      "    MSR: Gross=-0.0031 | TO=0.240 | TC=0.00024 | Net=-0.0033\n",
      "\n",
      "[Month 5] 2022-05-31\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0445 | TO=0.085 | TC=0.00008 | Net=-0.0446\n",
      "    MV: Gross=-0.0448 | TO=0.098 | TC=0.00009 | Net=-0.0448\n",
      "    MSR: Gross=-0.0431 | TO=0.120 | TC=0.00011 | Net=-0.0432\n",
      "\n",
      "[Month 6] 2022-06-30\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0602 | TO=0.097 | TC=0.00010 | Net= 0.0601\n",
      "    MV: Gross= 0.0560 | TO=0.087 | TC=0.00009 | Net= 0.0559\n",
      "    MSR: Gross= 0.0881 | TO=0.190 | TC=0.00021 | Net= 0.0879\n",
      "\n",
      "[Month 7] 2022-07-31\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0378 | TO=0.129 | TC=0.00012 | Net=-0.0380\n",
      "    MV: Gross=-0.0380 | TO=0.172 | TC=0.00017 | Net=-0.0381\n",
      "    MSR: Gross=-0.0371 | TO=0.176 | TC=0.00017 | Net=-0.0373\n",
      "\n",
      "[Month 8] 2022-08-31\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0864 | TO=0.063 | TC=0.00006 | Net=-0.0865\n",
      "    MV: Gross=-0.0880 | TO=0.100 | TC=0.00009 | Net=-0.0881\n",
      "    MSR: Gross=-0.0759 | TO=0.158 | TC=0.00015 | Net=-0.0761\n",
      "\n",
      "[Month 9] 2022-09-30\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0604 | TO=0.141 | TC=0.00015 | Net= 0.0603\n",
      "    MV: Gross= 0.0612 | TO=0.195 | TC=0.00021 | Net= 0.0610\n",
      "    MSR: Gross= 0.0381 | TO=0.271 | TC=0.00028 | Net= 0.0379\n",
      "\n",
      "[Month 10] 2022-10-31\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0644 | TO=0.155 | TC=0.00016 | Net= 0.0642\n",
      "    MV: Gross= 0.0652 | TO=0.163 | TC=0.00017 | Net= 0.0650\n",
      "    MSR: Gross= 0.0538 | TO=0.175 | TC=0.00018 | Net= 0.0536\n",
      "\n",
      "[Month 11] 2022-11-30\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0343 | TO=0.210 | TC=0.00020 | Net=-0.0345\n",
      "    MV: Gross=-0.0333 | TO=0.235 | TC=0.00023 | Net=-0.0335\n",
      "    MSR: Gross=-0.0405 | TO=0.212 | TC=0.00020 | Net=-0.0407\n",
      "\n",
      "[Month 12] 2022-12-31\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0059 | TO=0.095 | TC=0.00010 | Net= 0.0058\n",
      "    MV: Gross= 0.0054 | TO=0.086 | TC=0.00009 | Net= 0.0053\n",
      "    MSR: Gross= 0.0093 | TO=0.228 | TC=0.00023 | Net= 0.0091\n",
      "\n",
      "======================================================================\n",
      "YEAR 2023: RETRAINING LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Logistic training period: 2008-01-31 to 2022-12-31\n",
      "  Training samples: 89704\n",
      "  Training accuracy: 0.5621\n",
      "  Stocks evaluated: 498\n",
      "  Stocks selected: 100\n",
      "  Buy probability range: [0.5409, 0.5783]\n",
      "\n",
      "Running monthly rebalancing from 2023-01-31 to 2023-12-31\n",
      "======================================================================\n",
      "\n",
      "[Month 1] 2023-01-31\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0361 | TO=1.644 | TC=0.00158 | Net=-0.0377\n",
      "    MV: Gross=-0.0389 | TO=1.594 | TC=0.00153 | Net=-0.0405\n",
      "    MSR: Gross=-0.0279 | TO=2.034 | TC=0.00198 | Net=-0.0299\n",
      "\n",
      "[Month 2] 2023-02-28\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0245 | TO=0.118 | TC=0.00012 | Net= 0.0244\n",
      "    MV: Gross= 0.0219 | TO=0.131 | TC=0.00013 | Net= 0.0218\n",
      "    MSR: Gross= 0.0322 | TO=0.148 | TC=0.00015 | Net= 0.0320\n",
      "\n",
      "[Month 3] 2023-03-31\n",
      "  Assets: 60/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0213 | TO=0.128 | TC=0.00013 | Net= 0.0212\n",
      "    MV: Gross= 0.0240 | TO=0.210 | TC=0.00021 | Net= 0.0238\n",
      "    MSR: Gross= 0.0157 | TO=0.201 | TC=0.00020 | Net= 0.0155\n",
      "\n",
      "[Month 4] 2023-04-30\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0297 | TO=0.173 | TC=0.00017 | Net=-0.0298\n",
      "    MV: Gross=-0.0360 | TO=0.184 | TC=0.00018 | Net=-0.0362\n",
      "    MSR: Gross=-0.0172 | TO=0.228 | TC=0.00022 | Net=-0.0174\n",
      "\n",
      "[Month 5] 2023-05-31\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0482 | TO=0.078 | TC=0.00008 | Net= 0.0481\n",
      "    MV: Gross= 0.0475 | TO=0.123 | TC=0.00013 | Net= 0.0474\n",
      "    MSR: Gross= 0.0499 | TO=0.136 | TC=0.00014 | Net= 0.0498\n",
      "\n",
      "[Month 6] 2023-06-30\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0335 | TO=0.146 | TC=0.00015 | Net= 0.0333\n",
      "    MV: Gross= 0.0342 | TO=0.201 | TC=0.00021 | Net= 0.0340\n",
      "    MSR: Gross= 0.0323 | TO=0.217 | TC=0.00022 | Net= 0.0321\n",
      "\n",
      "[Month 7] 2023-07-31\n",
      "  Assets: 63/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0017 | TO=0.216 | TC=0.00022 | Net= 0.0015\n",
      "    MV: Gross= 0.0011 | TO=0.240 | TC=0.00024 | Net= 0.0009\n",
      "    MSR: Gross= 0.0027 | TO=0.260 | TC=0.00026 | Net= 0.0025\n",
      "\n",
      "[Month 8] 2023-08-31\n",
      "  Assets: 63/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0460 | TO=0.121 | TC=0.00012 | Net=-0.0461\n",
      "    MV: Gross=-0.0444 | TO=0.157 | TC=0.00015 | Net=-0.0445\n",
      "    MSR: Gross=-0.0486 | TO=0.122 | TC=0.00012 | Net=-0.0487\n",
      "\n",
      "[Month 9] 2023-09-30\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0217 | TO=0.162 | TC=0.00016 | Net=-0.0219\n",
      "    MV: Gross=-0.0248 | TO=0.129 | TC=0.00013 | Net=-0.0250\n",
      "    MSR: Gross=-0.0163 | TO=0.301 | TC=0.00030 | Net=-0.0166\n",
      "\n",
      "[Month 10] 2023-10-31\n",
      "  Assets: 61/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0368 | TO=0.298 | TC=0.00031 | Net= 0.0364\n",
      "    MV: Gross= 0.0397 | TO=0.261 | TC=0.00027 | Net= 0.0394\n",
      "    MSR: Gross= 0.0328 | TO=0.435 | TC=0.00045 | Net= 0.0324\n",
      "\n",
      "[Month 11] 2023-11-30\n",
      "  Assets: 62/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0234 | TO=0.154 | TC=0.00016 | Net= 0.0233\n",
      "    MV: Gross= 0.0210 | TO=0.213 | TC=0.00022 | Net= 0.0208\n",
      "    MSR: Gross= 0.0264 | TO=0.270 | TC=0.00028 | Net= 0.0262\n",
      "\n",
      "[Month 12] 2023-12-31\n",
      "  Assets: 62/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0240 | TO=0.155 | TC=0.00016 | Net= 0.0238\n",
      "    MV: Gross= 0.0265 | TO=0.189 | TC=0.00019 | Net= 0.0263\n",
      "    MSR: Gross= 0.0209 | TO=0.151 | TC=0.00015 | Net= 0.0208\n",
      "\n",
      "======================================================================\n",
      "YEAR 2024: RETRAINING LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "Logistic training period: 2009-01-31 to 2023-12-31\n",
      "  Training samples: 89703\n",
      "  Training accuracy: 0.5689\n",
      "  Stocks evaluated: 500\n",
      "  Stocks selected: 100\n",
      "  Buy probability range: [0.5485, 0.5899]\n",
      "\n",
      "Running monthly rebalancing from 2024-01-31 to 2024-04-30\n",
      "======================================================================\n",
      "\n",
      "[Month 1] 2024-01-31\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0217 | TO=1.611 | TC=0.00165 | Net= 0.0201\n",
      "    MV: Gross= 0.0074 | TO=1.596 | TC=0.00161 | Net= 0.0058\n",
      "    MSR: Gross= 0.0523 | TO=1.886 | TC=0.00198 | Net= 0.0503\n",
      "\n",
      "[Month 2] 2024-02-29\n",
      "  Assets: 67/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0266 | TO=0.253 | TC=0.00026 | Net= 0.0263\n",
      "    MV: Gross= 0.0301 | TO=0.240 | TC=0.00025 | Net= 0.0298\n",
      "    MSR: Gross= 0.0205 | TO=0.331 | TC=0.00034 | Net= 0.0202\n",
      "\n",
      "[Month 3] 2024-03-31\n",
      "  Assets: 65/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross=-0.0366 | TO=0.143 | TC=0.00014 | Net=-0.0368\n",
      "    MV: Gross=-0.0428 | TO=0.162 | TC=0.00015 | Net=-0.0430\n",
      "    MSR: Gross=-0.0265 | TO=0.255 | TC=0.00025 | Net=-0.0267\n",
      "\n",
      "[Month 4] 2024-04-30\n",
      "  Assets: 63/100 with complete data\n",
      "  Running DNN-FM...\n",
      "2.5\n",
      "  Computing portfolio weights...\n",
      "  ✓ DNN-FM completed for GMV, MV, MSR\n",
      "  Portfolio Returns:\n",
      "    GMV: Gross= 0.0159 | TO=0.151 | TC=0.00015 | Net= 0.0158\n",
      "    MV: Gross= 0.0091 | TO=0.163 | TC=0.00016 | Net= 0.0090\n",
      "    MSR: Gross= 0.0309 | TO=0.315 | TC=0.00033 | Net= 0.0306\n",
      "\n",
      "======================================================================\n",
      "BACKTEST COMPLETE\n",
      "======================================================================\n",
      "GMV Sharpe: 0.5677\n",
      "MV Sharpe:  0.5592\n",
      "MSR Sharpe: 0.6924\n"
     ]
    }
   ],
   "source": [
    "data_f=pd.read_csv('F-F_Research_Data_Factors.csv',sep=',')\n",
    "data_f['Date']=pd.to_datetime(data_f['Date'], format=\"%Y%m\")\n",
    "data_f['Date']=data_f['Date']+pd.offsets.MonthEnd(0)\n",
    "data_f = data_f.set_index('Date')\n",
    "data_f = data_f[['Mkt-RF', 'SMB', 'HML', 'RF']].astype(float)\n",
    "\n",
    "results = integrated_backtest(\n",
    "    df,\n",
    "    test_start_date='2020-01-31',\n",
    "    test_end_date='2024-04-30',\n",
    "    logistic_train_years=15,\n",
    "    stock_selection_method='top_and_bottom',\n",
    "    n_stocks=50,\n",
    "    lookback_window=180,\n",
    "    transaction_cost=0.001,\n",
    "    data_factor=data_f,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Access results for each portfolio\n",
    "gmv_results = results['gmv']['results_df']\n",
    "gmv_metrics = results['gmv']['metrics']\n",
    "\n",
    "mv_results = results['mv']['results_df']\n",
    "mv_metrics = results['mv']['metrics']\n",
    "\n",
    "msr_results = results['msr']['results_df']\n",
    "msr_metrics = results['msr']['metrics']\n",
    "\n",
    "# Compare Sharpe ratios\n",
    "print(f\"GMV Sharpe: {gmv_metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"MV Sharpe:  {mv_metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"MSR Sharpe: {msr_metrics['annual_sharpe_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc475f9-c242-4893-b468-67d950d625c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GMV\n",
      "Annualized Sharpe Ratio: 0.5677\n",
      "Mean Return: 0.0916\n",
      "Variance: 0.0261\n",
      "Avg Turnover: 0.2451\n",
      "\n",
      " MV\n",
      "Annualized Sharpe Ratio: 0.5592\n",
      "Mean Return: 0.0889\n",
      "Variance: 0.0253\n",
      "Avg Turnover: 0.2602\n",
      "\n",
      " MSR\n",
      "Annualized Sharpe Ratio: 0.6924\n",
      "Mean Return: 0.1135\n",
      "Variance: 0.0269\n",
      "Avg Turnover: 0.3231\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n GMV\")\n",
    "print(f\"Annualized Sharpe Ratio: {gmv_metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Mean Return: {gmv_metrics['mean_return']*12:.4f}\")\n",
    "print(f\"Variance: {gmv_metrics['variance']*12:.4f}\")\n",
    "print(f\"Avg Turnover: {gmv_metrics['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MV\")\n",
    "print(f\"Annualized Sharpe Ratio: {mv_metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Mean Return: {mv_metrics['mean_return']*12:.4f}\")\n",
    "print(f\"Variance: {mv_metrics['variance']*12:.4f}\")\n",
    "print(f\"Avg Turnover: {mv_metrics['avg_turnover']:.4f}\")\n",
    "\n",
    "print(f\"\\n MSR\")\n",
    "print(f\"Annualized Sharpe Ratio: {msr_metrics['annual_sharpe_ratio']:.4f}\")\n",
    "print(f\"Mean Return: {msr_metrics['mean_return']*12:.4f}\")\n",
    "print(f\"Variance: {msr_metrics['variance']*12:.4f}\")\n",
    "print(f\"Avg Turnover: {msr_metrics['avg_turnover']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f64c85-3d40-4586-ba04-55f2266b3a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

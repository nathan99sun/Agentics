{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ca1131-7a77-40b2-ac12-7b0b921c4df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 389\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Load JSONL file into a pandas DataFrame\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} articles\")\n",
    "    return df\n",
    "\n",
    "def load_finbert_model():\n",
    "    \"\"\"Load the FinBERT model\"\"\"\n",
    "    print(\"Loading FinBERT model...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "        return tokenizer, model, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def analyze_sentiment(text, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"Analyze sentiment using FinBERT\"\"\"\n",
    "    try:\n",
    "        # Truncate text if too long\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          max_length=max_length, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # FinBERT outputs: [positive, negative, neutral]\n",
    "        probs = predictions[0].cpu().numpy()\n",
    "        sentiment_map = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "        sentiment = sentiment_map[np.argmax(probs)]\n",
    "        \n",
    "        return sentiment, probs\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return 'neutral', np.array([0.33, 0.33, 0.34])\n",
    "\n",
    "def generate_monthly_signals(df, tokenizer, model, device, start_date='2020-01', end_date='2024-12'):\n",
    "    \"\"\"Generate monthly buy/hold/sell signals for each company\"\"\"\n",
    "    # Convert publishdate to datetime\n",
    "    print(\"\\nProcessing dates...\")\n",
    "    df['Publishdate'] = pd.to_datetime(df['Publishdate'], errors='coerce')\n",
    "    \n",
    "    # Remove rows with invalid dates\n",
    "    df = df.dropna(subset=['Publishdate'])\n",
    "    \n",
    "    # Filter date range\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date) + pd.offsets.MonthEnd(0)\n",
    "    df = df[(df['Publishdate'] >= start) & (df['Publishdate'] <= end)].copy()\n",
    "    \n",
    "    print(f\"Filtered to {len(df)} articles between {start_date} and {end_date}\")\n",
    "    \n",
    "    # Add year-month column\n",
    "    df['year_month'] = df['Publishdate'].dt.to_period('M')\n",
    "    \n",
    "    # Analyze sentiment for each article\n",
    "    print(\"\\nAnalyzing sentiment for articles...\")\n",
    "    sentiments = []\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    total = len(df)\n",
    "    for idx, row in df.iterrows():\n",
    "        if len(sentiments) % 100 == 0:\n",
    "            print(f\"Progress: {len(sentiments)}/{total} articles processed\")\n",
    "        \n",
    "        # Combine title and text\n",
    "        text = str(row.get('Title', ''))\n",
    "        if pd.notna(row.get('Text')):\n",
    "            text = text + \" \" + str(row['Text'])\n",
    "        \n",
    "        if not text.strip():\n",
    "            sentiments.append('neutral')\n",
    "            sentiment_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        sentiment, probs = analyze_sentiment(text, tokenizer, model, device)\n",
    "        sentiments.append(sentiment)\n",
    "        \n",
    "        # Calculate sentiment score: positive - negative\n",
    "        score = float(probs[0] - probs[1])\n",
    "        sentiment_scores.append(score)\n",
    "    \n",
    "    df['sentiment'] = sentiments\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    \n",
    "    # Generate monthly signals per company\n",
    "    print(\"\\nGenerating monthly signals...\")\n",
    "    monthly_signals = []\n",
    "    \n",
    "    grouped = df.groupby(['symbol', 'year_month'])\n",
    "    for (symbol, year_month), group in grouped:\n",
    "        avg_score = group['sentiment_score'].mean()\n",
    "        article_count = len(group)\n",
    "        \n",
    "        # Generate signal based on average sentiment score\n",
    "        # Positive threshold: > 0.1, Negative threshold: < -0.1\n",
    "        if avg_score > 0.1:\n",
    "            signal = 'buy'\n",
    "        elif avg_score < -0.1:\n",
    "            signal = 'sell'\n",
    "        else:\n",
    "            signal = 'hold'\n",
    "        \n",
    "        company_name = group['company'].iloc[0] if 'company' in group.columns else ''\n",
    "        \n",
    "        monthly_signals.append({\n",
    "            'symbol': symbol,\n",
    "            'company': company_name,\n",
    "            'year_month': str(year_month),\n",
    "            'signal': signal,\n",
    "            'avg_sentiment_score': round(avg_score, 4),\n",
    "            'article_count': article_count,\n",
    "            'positive_count': int((group['sentiment'] == 'positive').sum()),\n",
    "            'negative_count': int((group['sentiment'] == 'negative').sum()),\n",
    "            'neutral_count': int((group['sentiment'] == 'neutral').sum())\n",
    "        })\n",
    "    \n",
    "    signals_df = pd.DataFrame(monthly_signals)\n",
    "    return signals_df, df\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    file_path = \"stock_data_articles.jsonl\"  # Update with your file path\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = load_jsonl(file_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"Error: No data loaded\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    if 'publishdate' in df.columns:\n",
    "        print(f\"  Date range: {df['publishdate'].min()} to {df['publishdate'].max()}\")\n",
    "    if 'symbol' in df.columns:\n",
    "        print(f\"  Number of companies: {df['symbol'].nunique()}\")\n",
    "    \n",
    "    # Load model\n",
    "    tokenizer, model, device = load_finbert_model()\n",
    "    \n",
    "    # Generate signals\n",
    "    signals_df, analyzed_df = generate_monthly_signals(df, tokenizer, model, device)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    signals_df.to_csv('monthly_signals.csv', index=False)\n",
    "    analyzed_df.to_csv('analyzed_articles.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total monthly signals generated: {len(signals_df)}\")\n",
    "    print(f\"\\nSignal distribution:\")\n",
    "    print(signals_df['signal'].value_counts().to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE SIGNALS\")\n",
    "    print(\"=\"*50)\n",
    "    print(signals_df.head(10).to_string())\n",
    "    \n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"  - monthly_signals.csv\")\n",
    "    print(f\"  - analyzed_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b4654e-5f42-4df1-ad48-3584ebefee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "# USE os.environ TO SET TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dede029-3939-40fd-ab38-bbab73cc20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"hf://datasets/KrossKinetic/SP500-Financial-News-Articles-Time-Series/stock_data_articles.jsonl\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300f5ac0-cf0a-4eeb-8519-e0965ae8702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      "  Shape: (4589, 7)\n",
      "  Columns: ['id_', 'links', 'symbol', 'company', 'Title', 'Text', 'Publishdate']\n",
      "Loading FinBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3467\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3464\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3466\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:1185\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:1056\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1053\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1060\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1062\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1063\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1071\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1072\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:864\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    856\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    857\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    862\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    863\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    869\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:776\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    766\u001b[39m         frames.append(\n\u001b[32m    767\u001b[39m             theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    768\u001b[39m                 [\n\u001b[32m   (...)\u001b[39m\u001b[32m    773\u001b[39m             )\n\u001b[32m    774\u001b[39m         )\n\u001b[32m    775\u001b[39m         skipped = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m     frames.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m skipped:\n\u001b[32m    778\u001b[39m     frames.append(\n\u001b[32m    779\u001b[39m         theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    780\u001b[39m             [\n\u001b[32m   (...)\u001b[39m\u001b[32m    785\u001b[39m         )\n\u001b[32m    786\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/ultratb.py:652\u001b[39m, in \u001b[36mVerboseTB.format_record\u001b[39m\u001b[34m(self, frame_info)\u001b[39m\n\u001b[32m    648\u001b[39m result += \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m call \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m result += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m result += theme_table[\u001b[38;5;28mself\u001b[39m._theme_name].format(\n\u001b[32m    651\u001b[39m     _format_traceback_lines(\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m         \u001b[43mframe_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlines\u001b[49m,\n\u001b[32m    653\u001b[39m         theme_table[\u001b[38;5;28mself\u001b[39m._theme_name],\n\u001b[32m    654\u001b[39m         \u001b[38;5;28mself\u001b[39m.has_colors,\n\u001b[32m    655\u001b[39m         lvals_toks,\n\u001b[32m    656\u001b[39m     )\n\u001b[32m    657\u001b[39m )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/IPython/core/tbtools.py:355\u001b[39m, in \u001b[36mFrameInfo.lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlines\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotOneValueFound:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDummy\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:734\u001b[39m, in \u001b[36mFrameInfo.lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlines\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Union[Line, LineGap, BlankLineRange]]:\n\u001b[32m    720\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    721\u001b[39m \u001b[33;03m    A list of lines to display, determined by options.\u001b[39;00m\n\u001b[32m    722\u001b[39m \u001b[33;03m    The objects yielded either have type Line, BlankLineRange\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    732\u001b[39m \u001b[33;03m    The Line objects are all within the ranges from .included_pieces.\u001b[39;00m\n\u001b[32m    733\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m     pieces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mincluded_pieces\u001b[49m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pieces:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:677\u001b[39m, in \u001b[36mFrameInfo.included_pieces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mincluded_pieces\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[\u001b[38;5;28mrange\u001b[39m]:\n\u001b[32m    668\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[33;03m    The list of pieces (ranges of lines) to display for this frame.\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[33;03m    Consists of .executing_piece, surrounding context pieces\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    675\u001b[39m \u001b[33;03m    Always a subset of .scope_pieces.\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     scope_pieces = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscope_pieces\u001b[49m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope_pieces:\n\u001b[32m    679\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:617\u001b[39m, in \u001b[36mFrameInfo.scope_pieces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    612\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source.pieces\n\u001b[32m    614\u001b[39m scope_start, scope_end = \u001b[38;5;28mself\u001b[39m.source.line_range(\u001b[38;5;28mself\u001b[39m.scope)\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    616\u001b[39m     piece\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m piece \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpieces\u001b[49m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scope_start <= piece.start \u001b[38;5;129;01mand\u001b[39;00m piece.stop <= scope_end\n\u001b[32m    619\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:145\u001b[39m, in \u001b[36mcached_property.cached_property_wrapper\u001b[39m\u001b[34m(self, obj, _cls)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:101\u001b[39m, in \u001b[36mSource.pieces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tree:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     98\u001b[39m         \u001b[38;5;28mrange\u001b[39m(i, i + \u001b[32m1\u001b[39m)\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.lines) + \u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_pieces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:116\u001b[39m, in \u001b[36mSource._clean_pieces\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_clean_pieces\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[\u001b[38;5;28mrange\u001b[39m]:\n\u001b[32m    113\u001b[39m     pieces = \u001b[38;5;28mself\u001b[39m._raw_split_into_pieces(\u001b[38;5;28mself\u001b[39m.tree, \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.lines) + \u001b[32m1\u001b[39m)\n\u001b[32m    114\u001b[39m     pieces = \u001b[43m[\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpieces\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# Combine overlapping pieces, i.e. consecutive pieces where the end of the first\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# is greater than the start of the second.\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# This can happen when two statements are on the same line separated by a semicolon.\u001b[39;00m\n\u001b[32m    123\u001b[39m     new_pieces = pieces[:\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:168\u001b[39m, in \u001b[36mSource._raw_split_into_pieces\u001b[39m\u001b[34m(self, stmt, start, end)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rang, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(group_by_key_func(body, \u001b[38;5;28mself\u001b[39m.line_range).items()):\n\u001b[32m    167\u001b[39m     sub_stmt = group[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_split_into_pieces\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:168\u001b[39m, in \u001b[36mSource._raw_split_into_pieces\u001b[39m\u001b[34m(self, stmt, start, end)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rang, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(group_by_key_func(body, \u001b[38;5;28mself\u001b[39m.line_range).items()):\n\u001b[32m    167\u001b[39m     sub_stmt = group[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_split_into_pieces\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:168\u001b[39m, in \u001b[36mSource._raw_split_into_pieces\u001b[39m\u001b[34m(self, stmt, start, end)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rang, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(group_by_key_func(body, \u001b[38;5;28mself\u001b[39m.line_range).items()):\n\u001b[32m    167\u001b[39m     sub_stmt = group[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_split_into_pieces\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_stmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_start\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:166\u001b[39m, in \u001b[36mSource._raw_split_into_pieces\u001b[39m\u001b[34m(self, stmt, start, end)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, body \u001b[38;5;129;01min\u001b[39;00m ast.iter_fields(stmt):\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    163\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m body \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    164\u001b[39m             \u001b[38;5;28misinstance\u001b[39m(body[\u001b[32m0\u001b[39m], (ast.stmt, ast.ExceptHandler, \u001b[38;5;28mgetattr\u001b[39m(ast, \u001b[33m'\u001b[39m\u001b[33mmatch_case\u001b[39m\u001b[33m'\u001b[39m, ())))\n\u001b[32m    165\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m rang, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mgroup_by_key_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mline_range\u001b[49m\u001b[43m)\u001b[49m.items()):\n\u001b[32m    167\u001b[39m             sub_stmt = group[\u001b[32m0\u001b[39m]\n\u001b[32m    168\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m inner_start, inner_end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raw_split_into_pieces(sub_stmt, *rang):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:125\u001b[39m, in \u001b[36mgroup_by_key_func\u001b[39m\u001b[34m(iterable, key_func)\u001b[39m\n\u001b[32m    123\u001b[39m result = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result[\u001b[43mkey_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m].append(item)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/core.py:178\u001b[39m, in \u001b[36mSource.line_range\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mline_range\u001b[39m(\u001b[38;5;28mself\u001b[39m, node: ast.AST) -> Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mline_range\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masttext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/stack_data/utils.py:40\u001b[39m, in \u001b[36mline_range\u001b[39m\u001b[34m(atok, node)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m start, end\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     (start, _), (end, _) = \u001b[43matok\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m start, end + \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/asttokens/asttokens.py:401\u001b[39m, in \u001b[36mASTText.get_text_positions\u001b[39m\u001b[34m(self, node, padded)\u001b[39m\n\u001b[32m    398\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), (\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m supports_tokenless(node):\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_text_positions_tokenless\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.asttokens.get_text_positions(node, padded)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/agentics/lib/python3.12/site-packages/asttokens/asttokens.py:319\u001b[39m, in \u001b[36mASTText._get_text_positions_tokenless\u001b[39m\u001b[34m(self, node, padded)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m._asttokens = ASTTokens(\n\u001b[32m    313\u001b[39m         \u001b[38;5;28mself\u001b[39m._text,\n\u001b[32m    314\u001b[39m         tree=\u001b[38;5;28mself\u001b[39m.tree,\n\u001b[32m    315\u001b[39m         filename=\u001b[38;5;28mself\u001b[39m._filename,\n\u001b[32m    316\u001b[39m     )\n\u001b[32m    317\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._asttokens\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_text_positions_tokenless\u001b[39m(\u001b[38;5;28mself\u001b[39m, node, padded):\n\u001b[32m    320\u001b[39m   \u001b[38;5;66;03m# type: (AstNode, bool) -> Tuple[Tuple[int, int], Tuple[int, int]]\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[33;03m  Version of ``get_text_positions()`` that doesn't use tokens.\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m    324\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m is_module(node):\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# Modules don't have position info, so just return the range of the whole text.\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;66;03m# The token-using method does something different, but its behavior seems weird and inconsistent.\u001b[39;00m\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# For example, in a file with only comments, it only returns the first line.\u001b[39;00m\n\u001b[32m    328\u001b[39m     \u001b[38;5;66;03m# It's hard to imagine a case when this matters.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Disable transformers progress bars to avoid context errors\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "\n",
    "def load_finbert_model():\n",
    "    \"\"\"Load the FinBERT model\"\"\"\n",
    "    print(\"Loading FinBERT model...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully on {device}\")\n",
    "        return tokenizer, model, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def analyze_sentiment(text, tokenizer, model, device, max_length=512):\n",
    "    \"\"\"Analyze sentiment using FinBERT\"\"\"\n",
    "    try:\n",
    "        # Truncate text if too long\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                          max_length=max_length, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # FinBERT outputs: [positive, negative, neutral]\n",
    "        probs = predictions[0].cpu().numpy()\n",
    "        sentiment_map = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "        sentiment = sentiment_map[np.argmax(probs)]\n",
    "        \n",
    "        return sentiment, probs\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return 'neutral', np.array([0.33, 0.33, 0.34])\n",
    "\n",
    "def generate_monthly_signals(df, tokenizer, model, device, start_date='2020-01', end_date='2024-12'):\n",
    "    \"\"\"Generate monthly buy/hold/sell signals for each company\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert publishdate to datetime - handle different column name cases\n",
    "    date_col = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() == 'publishdate':\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    if date_col is None:\n",
    "        raise ValueError(\"Could not find 'publishdate' column in dataframe\")\n",
    "    \n",
    "    print(f\"\\nProcessing dates from column '{date_col}'...\")\n",
    "    df['publishdate'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    \n",
    "    # Remove rows with invalid dates\n",
    "    df = df.dropna(subset=['publishdate'])\n",
    "    \n",
    "    # Filter date range\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date) + pd.offsets.MonthEnd(0)\n",
    "    df = df[(df['publishdate'] >= start) & (df['publishdate'] <= end)].copy()\n",
    "    \n",
    "    print(f\"Filtered to {len(df)} articles between {start_date} and {end_date}\")\n",
    "    \n",
    "    # Add year-month column\n",
    "    df['year_month'] = df['publishdate'].dt.to_period('M')\n",
    "    \n",
    "    # Find title and text columns (case-insensitive)\n",
    "    title_col = next((col for col in df.columns if col.lower() == 'title'), None)\n",
    "    text_col = next((col for col in df.columns if col.lower() == 'text'), None)\n",
    "    symbol_col = next((col for col in df.columns if col.lower() == 'symbol'), None)\n",
    "    company_col = next((col for col in df.columns if col.lower() == 'company'), None)\n",
    "    \n",
    "    if symbol_col is None:\n",
    "        raise ValueError(\"Could not find 'symbol' column in dataframe\")\n",
    "    \n",
    "    # Analyze sentiment for each article\n",
    "    print(\"\\nAnalyzing sentiment for articles...\")\n",
    "    sentiments = []\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    total = len(df)\n",
    "    for idx, row in df.iterrows():\n",
    "        if len(sentiments) % 100 == 0:\n",
    "            print(f\"Progress: {len(sentiments)}/{total} articles processed\")\n",
    "        \n",
    "        # Combine title and text\n",
    "        text = str(row[title_col]) if title_col and pd.notna(row.get(title_col)) else ''\n",
    "        if text_col and pd.notna(row.get(text_col)):\n",
    "            text = text + \" \" + str(row[text_col])\n",
    "        \n",
    "        if not text.strip():\n",
    "            sentiments.append('neutral')\n",
    "            sentiment_scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        sentiment, probs = analyze_sentiment(text, tokenizer, model, device)\n",
    "        sentiments.append(sentiment)\n",
    "        \n",
    "        # Calculate sentiment score: positive - negative\n",
    "        score = float(probs[0] - probs[1])\n",
    "        sentiment_scores.append(score)\n",
    "    \n",
    "    df['sentiment'] = sentiments\n",
    "    df['sentiment_score'] = sentiment_scores\n",
    "    \n",
    "    # Generate monthly signals per company\n",
    "    print(\"\\nGenerating monthly signals...\")\n",
    "    monthly_signals = []\n",
    "    \n",
    "    grouped = df.groupby([symbol_col, 'year_month'])\n",
    "    for (symbol, year_month), group in grouped:\n",
    "        avg_score = group['sentiment_score'].mean()\n",
    "        article_count = len(group)\n",
    "        \n",
    "        # Generate signal based on average sentiment score\n",
    "        # Positive threshold: > 0.1, Negative threshold: < -0.1\n",
    "        if avg_score > 0.2:\n",
    "            signal = 'buy'\n",
    "        elif avg_score < -0.2:\n",
    "            signal = 'sell'\n",
    "        else:\n",
    "            signal = 'hold'\n",
    "        \n",
    "        company_name = group[company_col].iloc[0] if company_col and company_col in group.columns else ''\n",
    "        \n",
    "        monthly_signals.append({\n",
    "            'symbol': symbol,\n",
    "            'company': company_name,\n",
    "            'year_month': str(year_month),\n",
    "            'signal': signal,\n",
    "            'avg_sentiment_score': round(avg_score, 4),\n",
    "            'article_count': article_count,\n",
    "            'positive_count': int((group['sentiment'] == 'positive').sum()),\n",
    "            'negative_count': int((group['sentiment'] == 'negative').sum()),\n",
    "            'neutral_count': int((group['sentiment'] == 'neutral').sum())\n",
    "        })\n",
    "    \n",
    "    signals_df = pd.DataFrame(monthly_signals)\n",
    "    return signals_df, df\n",
    "\n",
    "# Main execution - use your existing df\n",
    "print(f\"Dataset info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Load model\n",
    "tokenizer, model, device = load_finbert_model()\n",
    "\n",
    "# Generate signals\n",
    "signals_df, analyzed_df = generate_monthly_signals(df, tokenizer, model, device)\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "signals_df.to_csv('monthly_signals.csv', index=False)\n",
    "analyzed_df.to_csv('analyzed_articles.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total monthly signals generated: {len(signals_df)}\")\n",
    "print(f\"\\nSignal distribution:\")\n",
    "print(signals_df['signal'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE SIGNALS\")\n",
    "print(\"=\"*50)\n",
    "print(signals_df.head(10))\n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - monthly_signals.csv\")\n",
    "print(f\"  - analyzed_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128033b-60c0-4964-9d67-823d63775b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
